"""
å› å­ç®¡ç†å™¨æ¨¡å—

æ•´åˆå› å­æ³¨å†Œè¡¨ã€åˆ†ç±»å™¨å’Œå­˜å‚¨åŠŸèƒ½ï¼Œæä¾›ç»Ÿä¸€çš„å› å­ç®¡ç†æ¥å£ã€‚
"""

import json
import os
from pathlib import Path
from typing import Dict, List, Optional, Any, Union, Tuple

import numpy as np
import pandas as pd
from numpyencoder import NumpyEncoder

from quant_lib import setup_logger
from quant_lib.config.logger_config import log_warning, log_notice, log_error
from .classifier.factor_classifier import FactorClassifier
from .factor_calculator.factor_calculator import FactorCalculator
# å¯¼å…¥å­æ¨¡å—
from .registry.factor_registry import FactorRegistry, FactorCategory, FactorMetadata
from .storage.single_storage import add_single_factor_test_result
from ..config_manager.base_config import workspaces_result_dir
from ..config_manager.factor_direction_config import FACTOR_DIRECTIONS
from ..data_manager.data_manager import DataManager, fill_and_align_by_stock_pool, my_align
from ..utils.data.check_data import check_data_quality_detail

logger = setup_logger(__name__)


class FactorResultsManager:
    """å› å­æµ‹è¯•ç»“æœç±»"""

    def __init__(self,
                 **kwargs):

        self.results_dir =  workspaces_result_dir

        for key, value in kwargs.items():
            setattr(self, key, value)

    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {k: v for k, v in self.__dict__.items()}

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'FactorResultsManager':
        """ä»å­—å…¸åˆ›å»º"""
        return cls(**data)

    def _save_factor_results(self,
                             factor_name: str,
                             stock_index: str,  # æ¯”å¦‚ä¸­è¯800
                             start_date: str, end_date: str,
                             returns_calculator_func_name: str,  # æ–°å¢å‚æ•°ï¼Œç”¨äºåŒºåˆ† 'c2c' æˆ– 'o2o'
                             results: Dict):
        """
        å°†å•æ¬¡å› å­æµ‹è¯•çš„æ‰€æœ‰æˆæœï¼Œä¿å­˜åˆ°ç»“æ„åŒ–çš„ç›®å½•ä¸­ã€‚
        """
        # 1. åˆ›å»ºä¸€ä¸ªä»¥æ—¥æœŸèŒƒå›´å‘½åçš„ã€å”¯ä¸€çš„ç‰ˆæœ¬æ–‡ä»¶å¤¹
        run_version = f"{start_date.replace('-', '')}_{end_date.replace('-', '')}"
        # 1. åˆ›å»ºæ¸…æ™°çš„å­˜å‚¨è·¯å¾„
        output_path = Path(self.results_dir) / stock_index / factor_name / returns_calculator_func_name / run_version
        output_path.mkdir(parents=True, exist_ok=True)

        # 2. åˆ†è§£å¹¶ä¿å­˜ä¸åŒçš„â€œæˆæœâ€
        # a) ä¿å­˜æ€»ç»“æ€§ç»Ÿè®¡æ•°æ®
        ic_stats_periods_dict_raw = results.get("ic_stats_periods_dict_raw", {})
        ic_stats_periods_dict_processed = results.get("ic_stats_periods_dict_processed", {})
        quantile_stats_periods_dict_raw = results.get("quantile_stats_periods_dict_raw", {})
        quantile_stats_periods_dict_processed = results.get("quantile_stats_periods_dict_processed", {})
        fm_stat_results_periods_dict = results.get("fm_stat_results_periods_dict", {})
        top_q_turnover_stats_periods_dict = results.get("top_q_turnover_stats_periods_dict", {})
        style_correlation_dict = results.get("style_correlation_dict", {})

        summary_stats = {
            'ic_analysis_raw': ic_stats_periods_dict_raw,
            'ic_analysis_processed': ic_stats_periods_dict_processed,
            'quantile_backtest_raw': quantile_stats_periods_dict_raw,
            'quantile_backtest_processed': quantile_stats_periods_dict_processed,
            'fama_macbeth': fm_stat_results_periods_dict,
            'top_q_turnover': top_q_turnover_stats_periods_dict,
            'style_correlation': style_correlation_dict
        }
        with open(output_path / 'summary_stats.json', 'w') as f:
            # ä½¿ç”¨è‡ªå®šä¹‰çš„Encoderæ¥å¤„ç†numpyç±»å‹
            json.dump(self._make_serializable(summary_stats), f, indent=4, cls=NumpyEncoder)

        if "processed_factor_df" in results:
            results["processed_factor_df"].to_parquet(output_path / 'processed_factor.parquet')

        ic_series_periods_dict_raw = results.get("ic_series_periods_dict_raw", {})
        ic_series_periods_dict_processed = results.get("ic_series_periods_dict_processed", {})

        q_daily_returns_df_raw = results.get("q_daily_returns_df_raw", None)
        q_daily_returns_df_processed = results.get("q_daily_returns_df_processed", pd.DataFrame())

        quantile_returns_series_periods_dict_raw = results.get("quantile_returns_series_periods_dict_raw", {})
        quantile_returns_series_periods_dict_processed = results.get("quantile_returns_series_periods_dict_processed",
                                                                     {})
        fm_returns_series_periods_dict = results.get("fm_returns_series_periods_dict", {})

        # b) ä¿å­˜æ—¶é—´åºåˆ—æ•°æ® (ä»¥ Parquet æ ¼å¼ï¼Œæ›´é«˜æ•ˆ)
        if ic_series_periods_dict_raw:
            for period, series in ic_series_periods_dict_raw.items():
                df = series.to_frame(name='ic_series_raw')  # ç»™ä¸€åˆ—èµ·åï¼Œæ¯”å¦‚ 'ic'
                df.to_parquet(output_path / f'ic_series_raw_{period}.parquet')
        for period, series in ic_series_periods_dict_processed.items():
            df = series.to_frame(name='ic_series_processed')  # ç»™ä¸€åˆ—èµ·åï¼Œæ¯”å¦‚ 'ic'
            df.to_parquet(output_path / f'ic_series_processed_{period}.parquet')
        if quantile_returns_series_periods_dict_raw:
            for period, df in quantile_returns_series_periods_dict_raw.items():
                df.to_parquet(output_path / f'quantile_returns_raw_{period}.parquet')
        for period, df in quantile_returns_series_periods_dict_processed.items():
            df.to_parquet(output_path / f'quantile_returns_processed_{period}.parquet')
        if q_daily_returns_df_raw is not None and not q_daily_returns_df_raw.empty:
            q_daily_returns_df_raw.to_parquet(output_path / f'q_daily_returns_df_raw.parquet')
        q_daily_returns_df_processed.to_parquet(output_path / f'q_daily_returns_df_processed.parquet')

        for period, series in fm_returns_series_periods_dict.items():
            df = series.to_frame(name='fm_returns_series')
            df.to_parquet(output_path / f'fm_returns_series_{period}.parquet')

        logger.info(f"âœ“ å› å­'{factor_name}'åœ¨é…ç½®'{returns_calculator_func_name}'ä¸‹çš„æ‰€æœ‰ç»“æœå·²ä¿å­˜è‡³: {output_path}")

    def _make_serializable(self, obj):
        """å°†ç»“æœè½¬æ¢ä¸ºå¯åºåˆ—åŒ–æ ¼å¼"""
        if isinstance(obj, dict):
            return {str(k): self._make_serializable(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [self._make_serializable(v) for v in obj]
        elif isinstance(obj, pd.Series):
            # å°†ç´¢å¼•è½¬æ¢ä¸ºå­—ç¬¦ä¸²
            series_dict = {}
            for k, v in obj.items():
                key = str(k) if hasattr(k, '__str__') else k
                series_dict[key] = self._make_serializable(v)
            return series_dict
        elif isinstance(obj, pd.DataFrame):
            # å°†ç´¢å¼•å’Œåˆ—åéƒ½è½¬æ¢ä¸ºå­—ç¬¦ä¸²
            df_dict = {}
            for idx, row in obj.iterrows():
                row_dict = {}
                for col, val in row.items():
                    col_key = str(col) if hasattr(col, '__str__') else col
                    row_dict[col_key] = self._make_serializable(val)
                idx_key = str(idx) if hasattr(idx, '__str__') else idx
                df_dict[idx_key] = row_dict
            return df_dict
        elif isinstance(obj, (np.integer, np.floating)):
            return float(obj)
        elif isinstance(obj, (np.bool_, bool)):
            return bool(obj)
        elif isinstance(obj, pd.Timestamp):
            return str(obj)
        elif pd.isna(obj):
            return None
        else:
            try:
                # å°è¯•è½¬æ¢ä¸ºåŸºæœ¬Pythonç±»å‹
                if hasattr(obj, 'item'):  # numpyæ ‡é‡
                    return obj.item()
                return obj
            except:
                return str(obj)


class FactorManager:
    """å› å­ç®¡ç†å™¨ç±»"""

    def __init__(self,
                 data_manager: DataManager = None,
                 results_dir: str = Path(__file__).parent.parent / "workspace/factor_results",
                 registry_path: str = "factor_registry.json",
                 config: Dict[str, Any] = None):

        """
        åˆå§‹åŒ–å› å­ç®¡ç†å™¨

        Args:
            results_dir: æµ‹è¯•ç»“æœä¿å­˜ç›®å½•
            registry_path: æ³¨å†Œè¡¨æ–‡ä»¶è·¯å¾„
            config: é…ç½®å­—å…¸ï¼ŒåŒ…å«å› å­å®šä¹‰ç­‰ä¿¡æ¯
        """
        # å› å­ç¼“å­˜å­—å…¸ï¼Œç”¨äºå­˜å‚¨å·²ç»è®¡ç®—å¥½çš„å› å­ï¼Œé¿å…é‡å¤è®¡ç®—
        self.factors_cache: Dict[str, pd.DataFrame] = {}  # æ·»åŠ å…¶ä»–æµ‹è¯•ç»“æœ
        self.calculator = FactorCalculator(self)
        self.data_manager = data_manager
        self.config = config or {}  # ä¿å­˜é…ç½®ï¼Œç”¨äºæ™ºèƒ½æ—¶é—´å¯¹é½
        self.results_dir = Path(results_dir)
        self.results_dir.mkdir(parents=True, exist_ok=True)

        # åˆå§‹åŒ–ç»„ä»¶
        self.registry = FactorRegistry(registry_path)
        self.classifier = FactorClassifier()

        # æµ‹è¯•ç»“æœç¼“å­˜
        self.test_results = {}

        logger.info("å› å­ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")

    def clear_cache(self):
        """
        æ¸…ç†å› å­ç¼“å­˜
        æä¾›æ­£å¼çš„ç¼“å­˜ç®¡ç†æ¥å£ï¼Œé¿å…ç›´æ¥æ“ä½œå†…éƒ¨å±æ€§
        """
        cache_size = len(self.factors_cache)
        self.factors_cache.clear()
        logger.info(f"å› å­ç¼“å­˜å·²æ¸…ç†ï¼Œé‡Šæ”¾äº† {cache_size} ä¸ªç¼“å­˜é¡¹")

    def get_cache_info(self) -> Dict[str, Any]:
        """
        è·å–ç¼“å­˜ä¿¡æ¯
        """
        return {
            'cache_size': len(self.factors_cache),
            'cached_factors': list(self.factors_cache.keys())
        }

    # å¸¦ç€è§„åˆ™ï¼ æ³¨æ„ç”¨çš„æ—¶å€™ è¿™ä¸ªæ–¹å‘ ä¼šä¸ä¼šå¯¹ä½ æœ‰å½±å“ æ³¨æ„2ï¼šæ²¡æœ‰å¯¹é½è‚¡ç¥¨æ± å™¢ï¼Œéœ€è¦å¯¹é½ å¯ä»¥è°ƒç”¨  get_prepare_aligned_factor_for_analysis
    def get_factor_by_rule(self, factor_request: Union[str, tuple]) -> pd.DataFrame:
        """
        ã€æ ¸å¿ƒã€‘è·å–å› å­çš„ç»Ÿä¸€æ¥å£ã€‚
        """
        # 1. è°ƒç”¨æœ€åº•å±‚å‡½æ•°ï¼Œè·å–çº¯å‡€çš„åŸå§‹å› å­
        #    ç›´æ¥å°† factor_request é€ä¼ ä¸‹å»
        raw_factor_df = self.get_raw_factor(factor_request)

        # 2. åº”ç”¨æ–¹å‘æ€§è°ƒæ•´
        #    æˆ‘ä»¬éœ€è¦ä»è¯·æ±‚ä¸­è§£æå‡ºå› å­çš„åŸºç¡€åå­—
        factor_name_str = factor_request[0] if isinstance(factor_request, tuple) else factor_request
        direction = FACTOR_DIRECTIONS.get(factor_name_str, 1)

        if direction == -1:
            final_factor_df = raw_factor_df * -1
        else:
            final_factor_df = raw_factor_df

        return final_factor_df.copy()

    # æœ€åŸå§‹çš„å› å­è·å–ï¼Œæœªç»è¿‡ä»»ä½•å¤„ç†ï¼Œç›®å‰è¢«ä½¿ç”¨äº å› å­è®¡ç®—
    def get_raw_factor(self, factor_request: Union[str, tuple]) -> pd.DataFrame:
        """
        ã€V3.0 - å‚æ•°åŒ–ç‰ˆã€‘è·å–çº¯å‡€çš„åŸå§‹å› å­ã€‚
        èƒ½å¤„ç†ç®€å•çš„å­—ç¬¦ä¸²è¯·æ±‚ï¼Œä¹Ÿèƒ½å¤„ç†å¸¦å‚æ•°çš„å…ƒç»„è¯·æ±‚ã€‚
        """
        # 1. ç¼“å­˜é”®å°±æ˜¯è¯·æ±‚æœ¬èº«ï¼Œå…ƒç»„æ˜¯å¯å“ˆå¸Œçš„ï¼Œå¯ä»¥ç›´æ¥åšé”®
        if factor_request in self.factors_cache:
            return self.factors_cache[factor_request].copy(deep=True)

        # 2. è§£æè¯·æ±‚
        if isinstance(factor_request, str):
            factor_name = factor_request
            params = {}  # æ— å‚æ•°
        elif isinstance(factor_request, tuple):
            factor_name = factor_request[0]
            # å°†å‚æ•°æ‰“åŒ…æˆå­—å…¸ï¼Œä¼ é€’ç»™è®¡ç®—å‡½æ•°
            # ä¾‹å¦‚: ('beta', '000300.SH') -> {'benchmark_index': '000300.SH'}
            # ä½ éœ€è¦æ ¹æ®å› å­å®šä¹‰ï¼Œçº¦å®šå¥½å‚æ•°å
            if factor_name == 'beta':
                params = {'benchmark_index': factor_request[1]}
            elif factor_name in ['close_hfq_filled', 'open_hfq_filled', 'high_hfq_filled',
                                 'low_hfq_filled']:  # å…ƒå‡¶ è¿™é‡Œ if å¯¼è‡´ å‘½ä¸­ä¸‹ä¸€ä¸ªelse
                params = {'limit': factor_request[1]}
            elif factor_name in ['sw_l1_momentum_21d']:
                params = {'pointInTimeIndustryMap': factor_request[1]}
            # æœªæ¥å¯ä»¥æ‰©å±•åˆ°å…¶ä»–å› å­ï¼Œå¦‚ 'momentum'
            # elif factor_name == 'momentum':
            #     params = {'window': factor_request[1]}
            else:
                params = {}
        else:
            raise TypeError("factor_request å¿…é¡»æ˜¯å­—ç¬¦ä¸²æˆ–å…ƒç»„")

        # 3. è°ƒåº¦è®¡ç®—
        calculation_method_name = f"_calculate_{factor_name}"
        if hasattr(self.calculator, calculation_method_name):
            method_to_call = getattr(self.calculator, calculation_method_name)
            # ã€å…³é”®ã€‘å°†è§£æå‡ºçš„å‚æ•°ä¼ é€’ç»™è®¡ç®—å‡½æ•°
            raw_factor_df = method_to_call(**params)
        elif factor_name in self.data_manager.raw_dfs and not params:
            log_warning(
                f"{factor_name}é«˜åº¦é‡è§†---è¿™æ˜¯å®½è¡¨ indexä¸ºå…¨äº¤æ˜“æ—¥ï¼Œæ‰€ä»¥ï¼šåœç‰ŒæœŸçš„è¡Œå…¨æ˜¯nanï¼Œè¯·æ€è€ƒè¿™çªå¦‚å…¶æ¥çš„nanå¯¹ä¸‹é¢å…¬å¼è®¡ç®—æ˜¯å¦æœ‰å½±å“ï¼Œæœ‰å½±å“æ˜¯å¦ffillè§£å†³ ")
            raw_factor_df = self.data_manager.raw_dfs[factor_name]
        else:
            raise ValueError(f"è·å–å› å­å¤±è´¥ï¼š{factor_request}")

        # 4. å­˜å…¥ç¼“å­˜å¹¶è¿”å›
        self.factors_cache[factor_request] = raw_factor_df #æ’æŸ¥é—®é¢˜ä¸­ å…ˆå…³äº†
        return raw_factor_df.copy(deep=True)

    def register_factor(self,
                        name: str,
                        category: Union[str, FactorCategory],
                        description: str = "",
                        data_requirements: List[str] = None,
                        **kwargs) -> bool:
        """
        æ³¨å†Œå› å­

        Args:
            name: å› å­åç§°
            category: å› å­ç±»åˆ«
            description: å› å­æè¿°
            data_requirements: æ•°æ®éœ€æ±‚
            **kwargs: å…¶ä»–å…ƒæ•°æ®

        Returns:
            æ˜¯å¦æ³¨å†ŒæˆåŠŸ
        """
        return self.registry.register_factor(
            name=name,
            category=category,
            description=description,
            data_requirements=data_requirements,
            **kwargs
        )

    def get_factor_metadata(self, name: str) -> Optional[FactorMetadata]:
        """è·å–å› å­å…ƒæ•°æ®"""
        return self.registry.get_factor(name)

    def list_factors(self,
                     category: Union[str, FactorCategory] = None) -> List[str]:
        """
        åˆ—å‡ºå› å­

        Args:
            category: ç­›é€‰çš„å› å­ç±»åˆ«

        Returns:
            å› å­åç§°åˆ—è¡¨
        """
        return self.registry.list_factors(category)

    def get_factor_summary(self) -> pd.DataFrame:
        """è·å–å› å­æ‘˜è¦"""
        return self.registry.get_factor_summary()

    def get_test_result(self, factor_name: str) -> Optional[FactorResultsManager]:
        """
        è·å–æµ‹è¯•ç»“æœ

        Args:
            factor_name: å› å­åç§°

        Returns:
            æµ‹è¯•ç»“æœå¯¹è±¡
        """
        # å…ˆä»ç¼“å­˜ä¸­è·å–
        if factor_name in self.test_results:
            return self.test_results[factor_name]

        # ä»æ–‡ä»¶ä¸­è·å–æœ€æ–°çš„æµ‹è¯•ç»“æœ
        factor_dir = self.results_dir / factor_name
        if not factor_dir.exists():
            return None

        result_files = list(factor_dir.glob("test_result_*.json"))
        if not result_files:
            return None

        # æŒ‰æ–‡ä»¶åæ’åºï¼Œè·å–æœ€æ–°çš„æµ‹è¯•ç»“æœ
        latest_file = sorted(result_files)[-1]

        try:
            with open(latest_file, 'r', encoding='utf-8') as f:
                data = json.load(f)

            result = FactorResultsManager.from_dict(data)

            # æ›´æ–°ç¼“å­˜
            self.test_results[factor_name] = result

            return result
        except Exception as e:
            logger.error(f"åŠ è½½æµ‹è¯•ç»“æœå¤±è´¥: {e}")
            return None

    def classify_factor(self,
                        factor_data: pd.DataFrame,
                        returns_data: pd.DataFrame = None) -> FactorCategory:
        """
        è‡ªåŠ¨åˆ†ç±»å› å­

        Args:
            factor_data: å› å­æ•°æ®
            returns_data: æ”¶ç›Šç‡æ•°æ®

        Returns:
            å› å­ç±»åˆ«
        """
        return self.classifier.classify_factor(factor_data, returns_data)

    def analyze_factor_correlation(self,
                                   factor_data_dict: Dict[str, pd.DataFrame],
                                   figsize: Tuple[int, int] = (12, 10)) -> Tuple[pd.DataFrame, Any]:
        """
        åˆ†æå› å­ç›¸å…³æ€§

        Args:
            factor_data_dict: å› å­æ•°æ®å­—å…¸
            figsize: å›¾è¡¨å¤§å°

        Returns:
            (ç›¸å…³æ€§çŸ©é˜µ, çƒ­åŠ›å›¾)
        """
        return self.classifier.analyze_factor_correlation(factor_data_dict, figsize)

    def cluster_factors(self,
                        factor_data_dict: Dict[str, pd.DataFrame],
                        n_clusters: int = 5) -> Dict[str, int]:
        """
        èšç±»å› å­

        Args:
            factor_data_dict: å› å­æ•°æ®å­—å…¸
            n_clusters: èšç±»æ•°é‡

        Returns:
            å› å­èšç±»ç»“æœå­—å…¸
        """
        return self.classifier.cluster_factors(factor_data_dict, n_clusters)

    def visualize_factor_clusters(self,
                                  factor_data_dict: Dict[str, pd.DataFrame],
                                  n_clusters: int = 5,
                                  method: str = 'pca',
                                  figsize: Tuple[int, int] = (12, 10)) -> Any:
        """
        å¯è§†åŒ–å› å­èšç±»

        Args:
            factor_data_dict: å› å­æ•°æ®å­—å…¸
            n_clusters: èšç±»æ•°é‡
            method: é™ç»´æ–¹æ³•
            figsize: å›¾è¡¨å¤§å°

        Returns:
            å›¾è¡¨å¯¹è±¡
        """
        return self.classifier.visualize_factor_clusters(
            factor_data_dict, n_clusters, method, figsize
        )

    def get_top_factors(self):

        return None

    def _make_serializable(self, obj):
        """å°†ç»“æœè½¬æ¢ä¸ºå¯åºåˆ—åŒ–æ ¼å¼"""
        if isinstance(obj, dict):
            return {str(k): self._make_serializable(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [self._make_serializable(v) for v in obj]
        elif isinstance(obj, pd.Series):
            # å°†ç´¢å¼•è½¬æ¢ä¸ºå­—ç¬¦ä¸²
            series_dict = {}
            for k, v in obj.items():
                key = str(k) if hasattr(k, '__str__') else k
                series_dict[key] = self._make_serializable(v)
            return series_dict
        elif isinstance(obj, pd.DataFrame):
            # å°†ç´¢å¼•å’Œåˆ—åéƒ½è½¬æ¢ä¸ºå­—ç¬¦ä¸²
            df_dict = {}
            for idx, row in obj.iterrows():
                row_dict = {}
                for col, val in row.items():
                    col_key = str(col) if hasattr(col, '__str__') else col
                    row_dict[col_key] = self._make_serializable(val)
                idx_key = str(idx) if hasattr(idx, '__str__') else idx
                df_dict[idx_key] = row_dict
            return df_dict
        elif isinstance(obj, (np.integer, np.floating)):
            return float(obj)
        elif isinstance(obj, (np.bool_, bool)):
            return bool(obj)
        elif isinstance(obj, pd.Timestamp):
            return str(obj)
        elif pd.isna(obj):
            return None
        else:
            try:
                # å°è¯•è½¬æ¢ä¸ºåŸºæœ¬Pythonç±»å‹
                if hasattr(obj, 'item'):  # numpyæ ‡é‡
                    return obj.item()
                return obj
            except:
                return str(obj)

    def _save_results(self, results: Dict[str, Any], file_name_prefix: str) -> None:
        """ä¿å­˜æµ‹è¯•ç»“æœ"""
        # å‡†å¤‡å¯åºåˆ—åŒ–çš„ç»“æœ
        serializable_results = self._make_serializable(results)

        # ä¿å­˜JSONæ ¼å¼
        json_path = os.path.join(self.results_dir, f'all_single_factor_test_{file_name_prefix}_results.json')
        add_single_factor_test_result(json_path, serializable_results)

    # ok ä¿å­˜ ç²¾ç®€ç®€è¦çš„æµ‹è¯•ç»“æœ
    def update_and_save_factor_purify_summary(self, all_summary_rows: list, file_name_prefix: str):
        """
           æ›´æ–°æˆ–åˆ›å»ºå› å­æ’è¡Œæ¦œï¼Œæ”¯æŒå¢é‡æ›´æ–°ã€‚
           å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œåˆ™åˆ é™¤æœ¬æ¬¡æµ‹è¯•æ¶‰åŠçš„å› å­å’Œå‘¨æœŸçš„æ—§è®°å½•ï¼Œå¹¶è¿½åŠ æ–°è®°å½•ã€‚
           å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ™åˆ›å»ºæ–°æ–‡ä»¶ã€‚
           """
        if not all_summary_rows:
            print("è­¦å‘Šï¼šæ²¡æœ‰æ–°çš„æµ‹è¯•ç»“æœå¯ä¾›æ›´æ–°ã€‚")
            return

        # 1. å‡†å¤‡æ–°æ•°æ®
        new_results_df = pd.DataFrame(all_summary_rows)
        # (æ¨è) åœ¨åˆå¹¶å‰å…ˆæ’åºï¼Œä¿æŒæ•°æ®æ¡ç†æ€§
        new_results_df=new_results_df.sort_values(by=['factor_name', 'period'], inplace=False)

        # 2. ã€ä¿®æ­£Bug 3ã€‘æ­£ç¡®æ„å»ºæ–‡ä»¶è·¯å¾„
        output_dir = Path(f'{self.results_dir}')
        output_dir.mkdir(exist_ok=True)
        parquet_path = output_dir / f'all_single_factor_test_{file_name_prefix}.parquet'
        csv_path = output_dir / f'all_single_factor_test_{file_name_prefix}.csv'

        # 3. ã€ä¿®æ­£Bug 1ã€‘å®‰å…¨åœ°è¯»å–æ—§æ•°æ®
        try:
            existing_leaderboard = pd.read_parquet(parquet_path)
        except FileNotFoundError:
            log_warning(f"ä¿¡æ¯ï¼šæœªæ‰¾åˆ°ç°æœ‰çš„æ’è¡Œæ¦œæ–‡ä»¶ at {parquet_path}ã€‚å°†åˆ›å»ºæ–°æ–‡ä»¶ã€‚")
            existing_leaderboard = pd.DataFrame()

        # 4. ã€ä¿®æ­£é€»è¾‘é£é™© 4ã€‘ä»æ–°æ•°æ®ä¸­æå–æ‰€æœ‰å¾…æ›´æ–°çš„â€œä¸»é”®â€
        #    è¿™æ ·å³ä½¿ä¸€æ¬¡ä¼ å…¥å¤šä¸ªå› å­çš„ç»“æœä¹Ÿèƒ½æ­£ç¡®å¤„ç†
        keys_to_update = new_results_df[['factor_name', 'backtest_period', 'backtest_base_on_index']].drop_duplicates()

        # 5. åˆ é™¤æ—§è®°å½•
        if not existing_leaderboard.empty:
            # ä½¿ç”¨ merge + indicator æ¥æ‰¾åˆ°å¹¶æ’é™¤éœ€è¦åˆ é™¤çš„è¡Œ
            ##
            # indicator=True
            # ç»“æœå°±æ˜¯ï¼šæ–°ç”Ÿæˆ_mergeråˆ—ï¼Œå€¼è¦ä¹ˆæ˜¯both è¦ä¹ˆæ˜¯left_only#
            merged = existing_leaderboard.merge(keys_to_update,
                                                on=['factor_name', 'backtest_period', 'backtest_base_on_index'],
                                                how='left',
                                                indicator=True)
            leaderboard_to_keep = merged[merged['_merge'] == 'left_only'].drop(columns=['_merge'])
        else:
            leaderboard_to_keep = existing_leaderboard

        # 6. ã€ä¿®æ­£Bug 2ã€‘åˆå¹¶æ—§çš„â€œä¿ç•™â€æ•°æ®å’Œæ‰€æœ‰æ–°æ•°æ®
        final_leaderboard = pd.concat([leaderboard_to_keep, new_results_df], ignore_index=True)

        # 7. ä¿å­˜æœ€ç»ˆçš„æ’è¡Œæ¦œ
        try:
            final_leaderboard.to_parquet(parquet_path, index=False)
            print(f"âœ… å› å­æ’è¡Œæ¦œå·²æˆåŠŸæ›´æ–°å¹¶ä¿å­˜è‡³: {parquet_path}")

            final_leaderboard.to_csv(csv_path, index=False, encoding='utf-8-sig')
            print(f"âœ… å› å­æ’è¡Œæ¦œå·²æˆåŠŸæ›´æ–°å¹¶ä¿å­˜è‡³: {csv_path}")

        except Exception as e:
            print(f"âŒ ä¿å­˜ç»“æœæ—¶å‘ç”Ÿé”™è¯¯: {e}")
            raise e  # é‡æ–°æŠ›å‡ºå¼‚å¸¸ï¼Œè®©ä¸Šå±‚çŸ¥é“å‘ç”Ÿäº†é”™è¯¯

    def update_and_save_fm_factor_return_matrix(self, new_fm_factor_returns_dict: dict, file_name_prefix: str):
        """
        ã€æ–°ã€‘æ›´æ–°æˆ–åˆ›å»ºç»Ÿä¸€çš„å› å­æ”¶ç›ŠçŸ©é˜µæ–‡ä»¶ã€‚
        å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œåˆ™ç”¨æ–°çš„æ”¶ç›Šåºåˆ—è¦†ç›–æ‰åŒåçš„æ—§åºåˆ—ã€‚

        Args:
            new_fm_factor_returns_dict (dict): æœ¬æ¬¡æµ‹è¯•äº§å‡ºçš„æ–°æ”¶ç›Šåºåˆ—å­—å…¸ã€‚
                                          é”®ä¸º 'factor_name_period' (å¦‚ 'momentum_2_1_20d')ï¼Œ
                                          å€¼ä¸º pd.Seriesã€‚
            file_name_prefix (str): æ–‡ä»¶åå‰ç¼€ã€‚
        """
        if not new_fm_factor_returns_dict:
            print("è­¦å‘Šï¼šæ²¡æœ‰æ–°çš„å› å­æ”¶ç›Šåºåˆ—å¯ä¾›æ›´æ–°ã€‚")
            return

        # 1. å‡†å¤‡æ–°æ•°æ®ï¼šå°†è¾“å…¥çš„å­—å…¸è½¬æ¢ä¸ºä¸€ä¸ªâ€œå®½æ ¼å¼â€çš„DataFrame
        new_returns_df = pd.DataFrame(new_fm_factor_returns_dict)

        # 2. æ­£ç¡®æ„å»ºæ–‡ä»¶è·¯å¾„
        output_dir = Path(f'{self.results_dir}')
        output_dir.mkdir(exist_ok=True)
        parquet_path = output_dir / f'all_single_factor_fm_returns_{file_name_prefix}.parquet'
        csv_path = output_dir / f'all_factor_returns_{file_name_prefix}.csv'

        # 3. å®‰å…¨åœ°è¯»å–æ—§çš„æ”¶ç›ŠçŸ©é˜µ
        try:
            existing_matrix = pd.read_parquet(parquet_path)
        except FileNotFoundError:
            print(f"ä¿¡æ¯ï¼šæœªæ‰¾åˆ°ç°æœ‰çš„æ”¶ç›ŠçŸ©é˜µæ–‡ä»¶ at {parquet_path}ã€‚å°†åˆ›å»ºæ–°æ–‡ä»¶ã€‚")
            existing_matrix = pd.DataFrame()

        # 4. è¯†åˆ«éœ€è¦è¢«æ›¿æ¢çš„æ—§åˆ—
        #    è¿™äº›åˆ—çš„åå­—ï¼Œå°±æ˜¯æ–°æ•°æ® new_returns_df çš„åˆ—å
        cols_to_update = new_returns_df.columns

        # æ‰¾å‡ºåœ¨æ—§çŸ©é˜µä¸­ç¡®å®å­˜åœ¨çš„ã€éœ€è¦è¢«åˆ é™¤çš„åˆ—
        cols_to_drop = [col for col in cols_to_update if col in existing_matrix.columns]

        # 5. åˆ é™¤æ—§åˆ—ï¼Œå¾—åˆ°éœ€è¦ä¿ç•™çš„æ—§çŸ©é˜µéƒ¨åˆ†
        matrix_to_keep = existing_matrix.drop(columns=cols_to_drop)

        # 6. åˆå¹¶â€œä¿ç•™çš„æ—§çŸ©é˜µâ€å’Œâ€œæ‰€æœ‰æ–°æ•°æ®â€
        #    axis=1 è¡¨ç¤ºæŒ‰åˆ—è¿›è¡Œåˆå¹¶ã€‚Pandasä¼šè‡ªåŠ¨æŒ‰ç´¢å¼•ï¼ˆæ—¥æœŸï¼‰å¯¹é½ã€‚
        final_matrix = pd.concat([matrix_to_keep, new_returns_df], axis=1)

        # 7. (æ¨è) æŒ‰åˆ—åæ’åºï¼Œè®©æ–‡ä»¶ç»“æ„æ›´æ¸…æ™°
        final_matrix= final_matrix.sort_index(axis=1, inplace=False)

        # 8. ä¿å­˜æœ€ç»ˆçš„ã€æ›´æ–°åçš„æ”¶ç›ŠçŸ©é˜µ
        try:
            final_matrix.to_parquet(parquet_path, index=True)  # æ”¶ç›Šåºåˆ—ï¼Œç´¢å¼•(æ—¥æœŸ)éœ€è¦ä¿å­˜
            print(f"âœ… å› å­æ”¶ç›ŠçŸ©é˜µå·²æˆåŠŸæ›´æ–°å¹¶ä¿å­˜è‡³: {parquet_path}")

            final_matrix.to_csv(csv_path, index=True, encoding='utf-8-sig')
            print(f"âœ… å› å­æ”¶ç›ŠçŸ©é˜µå·²æˆåŠŸæ›´æ–°å¹¶ä¿å­˜è‡³: {csv_path}")

        except Exception as e:
            print(f"âŒ ä¿å­˜å› å­æ”¶ç›ŠçŸ©é˜µæ—¶å‘ç”Ÿé”™è¯¯: {e}")
            raise e

    # def get_backtest_ready_factor_entity(self):
    #
    #     technical_df_dict = {}
    #     technical_category_dict = {}
    #     technical_school_dict = {}
    #
    #     # æ‰¾å‡ºæ‰€æœ‰ç›®æ ‡target å› å­ã€‚
    #     # é€šè¿‡configçš„æ ‡è¯† æ‰¾å‡ºéœ€è¦å­¦æœ¯è®¡ç®—çš„å› å­
    #     # è‡ªç”Ÿçš„é—¨æ´¾ï¼Œé‡æ–°align Requireçš„å› å­ï¼Œå‚ä¸è®¡ç®—ï¼Œè¿”å›å­¦æœ¯_df
    #     target_factors_for_evaluation = self.data_manager.config_manager['target_factors_for_evaluation']['fields']
    #
    #     for target_factor_name in target_factors_for_evaluation:
    #         logger.info(f"get_backtest_ready_factor_entityåŠ è½½{target_factor_name}")
    #         # category
    #         category = self.get_style_category(target_factor_name)
    #         school = self.get_school_code_by_factor_name(target_factor_name)
    #         target_data_df = self.get_prepare_aligned_factor_for_analysis(target_factor_name,True)
    #         technical_df_dict.update({target_factor_name: target_data_df})
    #         technical_category_dict.update({target_factor_name: category})
    #         technical_school_dict.update({target_factor_name: school})
    #
    #     return technical_df_dict, technical_category_dict, technical_school_dict
    # è·Ÿè‚¡ç¥¨æ± å¯¹é½ï¼Œåœ¨è‚¡ç¥¨æ± é‡Œé¢é©¬ä¸Šè¿›è¡Œæµ‹è¯• å¤„äºå¿«è¦åˆ°åˆ†æé˜¶æ®µï¼Œå¯ä»¥è°ƒç”¨ï¼Œå› ä¸ºç†è§£ç¡®å®éœ€è¦å¯¹é½è‚¡ç¥¨æ± ã€‚ç›®å‰æ²¡å‘ç°ä»€ä¹ˆåœºæ™¯ä¸éœ€è¦å¯¹å…¶çš„ï¼Œæ‰€iæ— è„‘æ‰ æ²¡é”™
    def get_raw_factor_for_analysis(self, factor_request: Union[str, tuple], for_test: bool = True):
        """
        ã€æ™ºèƒ½æ—¶é—´å¯¹é½ã€‘è·å–åŸå§‹å› å­æ•°æ®ï¼Œæ ¹æ®é…ç½®è‡ªåŠ¨å¤„ç†æ—¶é—´åç§»
        
        æ”¯æŒä¸‰ç§æ—¶é—´å¯¹é½æ¨¡å¼ï¼š
        - 'no_shift': ä»·æ ¼æ•°æ®ï¼Œä¿æŒTæ—¥å€¼
        - 'shift': ä¼ ç»Ÿå› å­ï¼Œéœ€è¦shiftåˆ°T-1
        - 'pre_aligned': äº‹ä»¶å› å­ï¼Œåº•å±‚å·²æ ¡æ­£æ—¶é—´
        """
        if not for_test:
            raise ValueError('å¿…é¡»æ˜¯ç”¨äºæµ‹è¯•å‰åšçš„æ•°æ®æå–')

        factor_with_direction = self.get_factor_by_rule(factor_request)
        factor_name_str = factor_request[0] if isinstance(factor_request, tuple) else factor_request

        # ã€æ™ºèƒ½æ—¶é—´å¤„ç†ã€‘ä»é…ç½®ä¸­è·å–æ—¶é—´å¯¹é½æ–¹å¼

        time_alignment = self._get_factor_time_alignment(factor_name_str)
        
        if time_alignment == 'no_shift':
            # ä»·æ ¼æ•°æ®ä¿æŒTæ—¥å€¼ï¼Œç”¨äºè®¡ç®—æ”¶ç›Šç‡
            logger.info(f"{factor_request}: ä»·æ ¼æ•°æ®ä¿æŒTæ—¥å€¼")
            return factor_with_direction
        elif time_alignment == 'pre_aligned':
            # äº‹ä»¶å› å­ï¼Œåº•å±‚å·²æ ¡æ­£æ—¶é—´ï¼Œä¸å†shift
            logger.info(f"{factor_request}: äº‹ä»¶å› å­å·²é¢„æ ¡æ­£ï¼Œä¿æŒåŸå€¼")
            return factor_with_direction
        else:  # time_alignment == 'shift' æˆ–é»˜è®¤
            # ä¼ ç»Ÿå› å­æ•°æ®shiftåˆ°T-1ï¼Œç”¨äºäº¤æ˜“å†³ç­–
            logger.info(f"{factor_request}: å› å­æ•°æ®shiftåˆ°T-1ï¼Œç”¨äºäº¤æ˜“å†³ç­–")
            return factor_with_direction.shift(1)

    def _get_factor_time_alignment(self, factor_name: str) -> str:
        """
        ã€æ™ºèƒ½é…ç½®æŸ¥æ‰¾ã€‘è·å–å› å­çš„æ—¶é—´å¯¹é½é…ç½®
        
        ä¼˜å…ˆçº§ï¼š
        1. é…ç½®æ–‡ä»¶ä¸­çš„æ˜¾å¼å£°æ˜
        2. ä»·æ ¼æ•°æ®è‡ªåŠ¨è¯†åˆ«
        3. é»˜è®¤shiftå¤„ç†
        """
        # 1. ä»é…ç½®æ–‡ä»¶è·å–æ˜¾å¼å£°æ˜
        factor_definitions = self.data_manager.config.get('factor_definition', [])
        for factor_def in factor_definitions:
            if factor_def.get('name') == factor_name:
                alignment = factor_def.get('time_alignment')
                if alignment:
                    return alignment
        
        # 2. ä»·æ ¼æ•°æ®è‡ªåŠ¨è¯†åˆ«ï¼ˆå‘åå…¼å®¹ï¼‰
        price_data_names = {
            'close_raw', 'close_hfq', 'close_hfq_filled',
            'open_raw', 'open_hfq', 'open_hfq_filled',
            'high_raw', 'high_hfq', 'high_hfq_filled',
            'low_raw', 'low_hfq', 'low_hfq_filled',
            'close', 'open', 'high', 'low'  # ç®€åŒ–å‘½å
        }
        
        if factor_name in price_data_names:
            return 'no_shift'
        
        # 3. é»˜è®¤å¤„ç†ï¼šä¼ ç»Ÿå› å­éœ€è¦shift
        return 'shift'

    def get_prepare_aligned_factor_for_analysis(self, factor_request: Union[str, tuple], stock_pool_index_name,
                                                for_test):
        """
        ã€å…¼å®¹æ€§ä¿æŒã€‘è·å–å¯¹é½åçš„å› å­æ•°æ®
        å»ºè®®é€æ­¥è¿ç§»åˆ° get_raw_factor_for_analysis + align_factor_with_pool
        """
        if not for_test:
            raise ValueError('å¿…é¡»æ˜¯ç”¨äºæµ‹è¯•å‰åšçš„æ•°æ®æå– å› ä¸ºè¿™é‡Œçš„å¡«å……å°±åœ¨ä¸“é—¨åªç»™æµ‹è¯•è‡ªèº«å› å­åšçš„å¡«å……ç­–ç•¥')
        REQUEST = self.check_and_return_right_request(factor_request, stock_pool_index_name)
        # 1. è·å–åŸå§‹å› å­æ•°æ® t-1
        factor_data = self.get_raw_factor_for_analysis(REQUEST, for_test)
        #
        # self._validate_data_quality(factor_data, REQUEST, des='æœ€åŸç”Ÿæ•°æ®') #è¿™é‡Œæ£€æŸ¥æ„ä¹‰ä¸å¤§ï¼ï¼å› ä¸ºåŸç”Ÿè®¡ç®—å‡ºæ¥çš„ éšä¾¿ä¸€ä¸ªshift252 éƒ½å¯¼è‡´å¥½å¤šnan

        # 2. ä¸è‚¡ç¥¨æ± å¯¹é½
        ret = self.align_factor_with_pool(factor_data, factor_request, stock_pool_index_name)
        FactorManager._validate_data_quality(ret, REQUEST, des='åŸç”Ÿæ•°æ®æœ€ç»ˆå®Œå…¨å¯¹é½è‚¡ç¥¨æ± ä¹‹å')
        return ret

    def align_factor_with_pool(self, factor_data: pd.DataFrame, factor_request: Union[str, tuple],
                               stock_pool_index_name: str):
        """
        ã€æ–°æ–¹æ³•ã€‘å°†å› å­æ•°æ®ä¸æŒ‡å®šè‚¡ç¥¨æ± å¯¹é½
        """
        factor_name_str = factor_request[0] if isinstance(factor_request, tuple) else factor_request
        pool = self.data_manager.stock_pools_dict[stock_pool_index_name]

        temp_date = my_align(factor_data, self.get_raw_factor('close_hfq').notna().shift(1))
        # self._validate_data_quality(temp_date,factor_name_str,'åŸç”Ÿæ•°æ® ä»…å¯¹é½æœªåœç‰Œçš„close_df ')
        return fill_and_align_by_stock_pool(
            factor_name=factor_name_str,
            df=factor_data,
            stock_pool_df=pool,
            _existence_matrix=self.data_manager._existence_matrix
        )



    #
    #
    # def get_stock_pool_name_by_factor_school(self, factor_school):
    #     if factor_school in ['fundamentals', 'trend']:
    #         return 'institutional_stock_pool'#ä¸­è¯800è‚¡ç¥¨æ± 
    #     if factor_school in ['microstructure']:
    #         return 'microstructure_stock_pool' #å…¨å¤§A è‚¡ç¥¨æ± 
    #     raise ValueError(f'{factor_school}æ²¡æœ‰å®šä¹‰å› å­å±äºå“ªä¸€é—¨æ´¾')


    # def get_stock_pool_index_by_factor_name(self, factor_name):
    #     # æ‹¿åˆ°å¯¹åº”pool_name
    #     pool_name = self.get_stock_pool_name_by_factor_name(factor_name)
    #
    #     index_filter_config = self.data_manager.config_manager['stock_pool_profiles'][pool_name]['index_filter']
    #     if not index_filter_config['enable']:
    #         return INDEX_CODES['ALL_A']
    #     return index_filter_config['index_code']

    def get_style_category(self, factor_name):
        return self.data_manager.get_factor_definition(factor_name)['style_category'].iloc[0]

    #
    #
    # def generate_structuer_base_on_diff_pool_name(self, factor_name_data: Union[str, list]):
    #     if isinstance(factor_name_data, str):
    #         return self.generate_structure_dict_base_on_diff_pool_name(factor_name_data)
    #     if isinstance(factor_name_data, list):
    #         dicts = []
    #         for factor_name in factor_name_data:
    #             pool_df_dict = self.generate_structure_dict_base_on_diff_pool_name(factor_name)
    #             dicts.append((factor_name, pool_df_dict))  # ä¿å­˜å› å­åå’Œå¯¹åº”çš„dict
    #
    #         merged = {}
    #         for factor_name, pool_df_dict in dicts:
    #             for pool, df in pool_df_dict.items():
    #                 if pool not in merged:
    #                     merged[pool] = {}
    #                 merged[pool][factor_name] = df
    #
    #         return merged
    #
    #     raise TypeError("build_df_dict_base_on_diff_pool å…¥å‚ç±»ä¼¼æœ‰è¯¯")

    #
    # def do_shift_and_align_for_dict(self, factor_name=None, data_dict=None, _existence_matrix: pd.DataFrame = None):
    #     result = {}
    #     for stock_name, stock_pool_index in self.data_manager.stock_pools_dict.items():
    #         ret = self.do_shift_and_align_where_stock_pool(factor_name, data_dict[stock_name], stock_pool_index,
    #                                                        _existence_matrix=_existence_matrix)
    #         result[stock_name] = ret
    #     return result
    #
    # def do_align_for_dict(self, factor_name, data_dict):
    #     result = {}
    #     for stock_name, stock_pool_index in self.data_manager.stock_pools_dict.items():
    #         ret = self.do_align(factor_name, data_dict[stock_name], stock_pool_index)
    #         result[stock_name] = {factor_name: ret}
    #     return result
    #
    # def do_shift_and_align_where_stock_pool(self, factor_name, data_to_deal, stock_pool_index,
    #                                         _existence_matrix: pd.DataFrame = None):
    #     # ç‡å…ˆshift
    #     data_to_deal_by_shifted = self.do_shift(data_to_deal)
    #     # å¯¹é½
    #     result = self.do_align(factor_name, data_to_deal_by_shifted, stock_pool_index, _existence_matrix=_existence_matrix)
    #     return result

    # def do_shift(
    #         self,
    #         data_to_shift: Union[pd.DataFrame, Dict[str, pd.DataFrame]]
    # ) -> Union[pd.DataFrame, Dict[str, pd.DataFrame]]:
    #     """
    #     å¯¹è¾“å…¥çš„æ•°æ®æ‰§è¡Œ .shift(1) æ“ä½œï¼Œæ™ºèƒ½å¤„ç†å•ä¸ªDataFrameæˆ–DataFrameå­—å…¸ã€‚
    #     Args:
    #         data_to_shift: éœ€è¦è¿›è¡Œæ»åå¤„ç†çš„æ•°æ®ï¼Œ
    #                        å¯ä»¥æ˜¯ä¸€ä¸ª pandas DataFrameï¼Œ
    #                        ä¹Ÿå¯ä»¥æ˜¯ä¸€ä¸ª keyä¸ºå­—ç¬¦ä¸², valueä¸ºpandas DataFrameçš„å­—å…¸ã€‚
    #     Returns:
    #         ä¸€ä¸ªä¸è¾“å…¥ç±»å‹ç›¸åŒçš„æ–°å¯¹è±¡ï¼Œå…¶ä¸­æ‰€æœ‰çš„DataFrameéƒ½å·²è¢« .shift(1) å¤„ç†ã€‚
    #     """
    #     # --- æƒ…å†µä¸€ï¼šè¾“å…¥æ˜¯å­—å…¸ ---
    #     if isinstance(data_to_shift, dict):
    #         shifted_dict = {}
    #         for key, df in data_to_shift.items():
    #             if not isinstance(df, pd.DataFrame):
    #                 raise ValueError("do_shiftå¤±è´¥,dictå†…éƒ¨ä¸æ˜¯dfç»“æ„")
    #             # å¯¹å­—å…¸ä¸­çš„æ¯ä¸ªDataFrameæ‰§è¡Œshiftæ“ä½œ
    #             shifted_dict[key] = df.shift(1)
    #         return shifted_dict
    #
    #     # --- æƒ…å†µäºŒï¼šè¾“å…¥æ˜¯å•ä¸ªDataFrame ---
    #     elif isinstance(data_to_shift, pd.DataFrame):
    #         return data_to_shift.shift(1)
    #
    #     # --- å…¶ä»–æƒ…å†µï¼šè¾“å…¥ç±»å‹é”™è¯¯ï¼Œä¸»åŠ¨æŠ¥é”™ ---
    #     else:
    #         raise TypeError(
    #             f"è¾“å…¥ç±»å‹ä¸æ”¯æŒï¼ŒæœŸæœ›æ˜¯DataFrameæˆ–Dict[str, DataFrame]ï¼Œ"
    #             f"ä½†æ”¶åˆ°çš„æ˜¯ {type(data_to_shift).__name__}"
    #         )
    #
    # def do_align(self, factor_name, data_to_align: Union[pd.DataFrame, Dict[str, pd.DataFrame]], stock_pool_index,
    #              _existence_matrix: pd.DataFrame = None
    #              ) -> Union[pd.DataFrame, Dict[str, pd.DataFrame]]:
    #     # --- æƒ…å†µä¸€ï¼šè¾“å…¥æ˜¯å­—å…¸ ---
    #     if isinstance(data_to_align, dict):
    #         shifted_dict = {}
    #         for key, df in data_to_align.items():
    #             if not isinstance(df, pd.DataFrame):
    #                 raise ValueError("do_alignå¤±è´¥,dictå†…éƒ¨ä¸æ˜¯dfç»“æ„")
    #             # å¯¹å­—å…¸ä¸­çš„æ¯ä¸ªDataFrameæ‰§è¡Œshiftæ“ä½œ
    #             shifted_dict[key] = fill_and_align_by_stock_pool(factor_name=key, df=df,
    #                                                              stock_pool_df=stock_pool_index,
    #                                                              _existence_matrix=_existence_matrix)
    #         return shifted_dict
    #
    #     # --- æƒ…å†µäºŒï¼šè¾“å…¥æ˜¯å•ä¸ªDataFrame ---
    #     elif isinstance(data_to_align, pd.DataFrame):
    #         return fill_and_align_by_stock_pool(factor_name=factor_name, df=data_to_align,
    #                                             stock_pool_df=stock_pool_index, _existence_matrix=_existence_matrix)
    #
    #     # --- å…¶ä»–æƒ…å†µï¼šè¾“å…¥ç±»å‹é”™è¯¯ï¼Œä¸»åŠ¨æŠ¥é”™ ---
    #     else:
    #         raise TypeError(
    #             f"è¾“å…¥ç±»å‹ä¸æ”¯æŒï¼ŒæœŸæœ›æ˜¯DataFrameæˆ–Dict[str, DataFrame]ï¼Œ"
    #             f"ä½†æ”¶åˆ°çš„æ˜¯ {type(data_to_align).__name__}"
    #         )

    # # ok å› ä¸ºéœ€è¦æ»šåŠ¨è®¡ç®—ï¼Œæ‰€ä»¥ä¸ä¾èµ–è‚¡ç¥¨æ± çš„indexï¼ˆtradeï¼‰ åªè¦å¯¹é½è‚¡ç¥¨åˆ—å°±å¥½
    # def get_pct_chg_beta_dict(self):
    #     dict = {}
    #     for pool_name, _ in self.data_manager.stock_pools_dict.items():
    #         beta_df = self.get_pct_chg_beta_data_for_pool(pool_name)
    #         dict[pool_name] = beta_df
    #     return dict

    # def get_pct_chg_beta_data_for_pool(self, pool_name):
    #     pool_stocks = self.data_manager.stock_pools_dict[pool_name].columns
    #
    #     # ç›´æ¥ä»ä¸»BetaçŸ©é˜µä¸­æŒ‰éœ€é€‰å–ï¼Œæ— éœ€é‡æ–°è®¡ç®—
    #     beta_for_this_pool = self.prepare_master_pct_chg_beta_dataframe()[pool_stocks]  # todoåé¢è€ƒè™‘è®¾è®¡ä¸€ä¸‹ï¼Œå–è‡ªget_Factor()
    #
    #     return beta_for_this_pool
    #
    # def prepare_master_pct_chg_beta_dataframe(self):
    #     """
    #     ç”¨äºç”Ÿæˆä¸€ä»½ç»Ÿä¸€çš„ã€è¦†ç›–æ‰€æœ‰è‚¡ç¥¨çš„BetaçŸ©é˜µã€‚
    #     """
    #     logger.info("å¼€å§‹å‡†å¤‡ä¸»BetaçŸ©é˜µ...")
    #
    #     # 1. æ•´åˆæ‰€æœ‰è‚¡ç¥¨æ± çš„è‚¡ç¥¨ä»£ç ï¼Œå½¢æˆä¸€ä¸ªæ€»çš„è‚¡ç¥¨åˆ—è¡¨
    #     all_unique_stocks = set()
    #     for stock_pool_index in self.data_manager.stock_pools_dict.values():
    #         all_unique_stocks.update(stock_pool_index.columns)
    #
    #     master_stock_list = sorted(list(all_unique_stocks))
    #
    #     # 2. åªè°ƒç”¨ä¸€æ¬¡ calculate_rolling_betaï¼Œè®¡ç®—æ‰€æœ‰è‚¡ç¥¨çš„Beta
    #     logger.info(f"å¼€å§‹ä¸ºæ€»è®¡ {len(master_stock_list)} åªè‚¡ç¥¨è®¡ç®—ç»Ÿä¸€çš„Beta...")
    #     return calculate_rolling_beta(
    #         self.data_manager.config_manager['backtest']['start_date'],
    #         self.data_manager.config_manager['backtest']['end_date'],
    #         master_stock_list
    #     )
    # é‰´äºéƒ¨åˆ†å› å­ï¼Œå¿…é¡»ä¼ é€’å‚æ•°ï¼ è¿™é‡Œå¼ºåŠ åˆ¤æ–­ï¼ ï¼Œæ²¡æœ‰ä¼ é€’å‚æ•°ï¼Œæˆ‘ä»¬å°½å¯èƒ½è¡¥å……ä¸Š
    def check_and_return_right_request(self, factor_request, stock_pool_index_name):
        must_need_params = ['beta','sw_l1_momentum_21d']
        REQUEST = None
        # å¦‚æœæ˜¯strç±»å‹ï¼Œ åˆ¤æ–­æ˜¯å¦å¿…è¦åŠ ä¼ å‚æ•° ä¸åŠ çš„è¯ ç›´æ¥returnï¼Œ
        # å¦‚æœæ˜¯å…ƒç»„ç±»å‹ã€‚   åˆ¤æ–­æ˜¯å¦å·²æœ‰å‚æ•° æœ‰ ç›´æ¥return
        # ä¸Šé¢æ¡ä»¶éƒ½ä¸æ»¡è¶³ ç»Ÿä¸€è¡¥ä¸Š
        if isinstance(factor_request, str):
            if factor_request not in must_need_params:
                return factor_request


        elif isinstance(factor_request, tuple):
            if len(factor_request) > 1:  # è¯æ˜ä¼ å…¥å‚æ•°äº†
                return factor_request

        # ç»Ÿä¸€è¡¥å‚æ•°
        if factor_request == 'beta':
            REQUEST = ('beta',
                       self.data_manager.get_stock_pool_index_code_by_name(stock_pool_index_name))
        if factor_request == 'sw_l1_momentum_21d':
            REQUEST = ('sw_l1_momentum_21d',
                       self.data_manager.pit_map)
        return REQUEST

    @staticmethod
    def _validate_data_quality(factor_data: pd.DataFrame, factor_name: str, des):
        """
        ã€æ–°å¢ã€‘æ•°æ®è´¨é‡æ£€æŸ¥ï¼Œé˜²æ­¢æ—¶é—´é”™é…å¯¼è‡´çš„è™šå‡å•è°ƒæ€§
        """
        # logger.info(f"ğŸ” å¼€å§‹æ•°æ®è´¨é‡æ£€æŸ¥: {factor_name}--{des}")

        # 1. æ£€æŸ¥å› å­å€¼åˆ†å¸ƒ
        factor_flat = factor_data.stack().dropna()

        # 2. æ£€æŸ¥æ˜¯å¦å­˜åœ¨å¼‚å¸¸çš„å®Œç¾åˆ†å¸ƒ
        unique_ratio = factor_flat.nunique() / len(factor_flat)
        if unique_ratio < 0.1:  # å”¯ä¸€å€¼æ¯”ä¾‹è¿‡ä½ å¾ˆæ­£å¸¸å•Šï¼Œ3å¿«-15å¿« 1200ä¸ªæ•°æ®/1000*800
            log_notice(f"å› å­-{factor_name}-{des} å”¯ä¸€å€¼æ¯”ä¾‹è¿‡ä½: {unique_ratio:.3f}")

        check_report  = check_data_quality_detail(factor_data)
        if check_report['serious_data']:
            log_error(f"å› å­-{factor_name}-{des}-æŠ¥å‘Š:{check_report}")
            raise ValueError('æ•°æ®ä¸¥é‡é—®é¢˜')
        logger.info(f"å› å­-{factor_name}-{des}- æ•°æ®è´¨é‡åˆ†æ•°: {check_report['quality_score']:.3f}")
        # 4. æ£€æŸ¥æ—¶é—´åºåˆ—çš„è¿ç»­æ€§
        missing_ratio = factor_data.isna().sum().sum() / (factor_data.shape[0] * factor_data.shape[1])
        #å› ä¸ºé•¿è¾¾5å¹´ï¼Œè‚¡ç¥¨è½®æ¢ï¼Œåˆ—ä¸å†æ˜¯ç›®æ ‡åˆ—ï¼Œzz500 é•¿æ—¶é—´è½®æ¢ ->æœ€åå˜æˆ800åˆ— æµ…æµ…ä¸€ç®—ï¼šå›ºå®šç¼º300/800=ç¼º37.5%éƒ½å¾ˆæ­£å¸¸ï¼
        if missing_ratio >= 0.5:
            log_notice(f"å› å­-{factor_name}-{des}- ç¼ºå¤±å€¼æ¯”ä¾‹è¿‡é«˜: {missing_ratio:.3f}")