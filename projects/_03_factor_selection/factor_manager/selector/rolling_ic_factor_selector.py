"""
åŸºäºæ»šåŠ¨ICçš„ä¸“ä¸šå› å­ç­›é€‰å™¨

æ ¸å¿ƒåŸåˆ™ï¼š
1. åšå†³ä½¿ç”¨æ»šåŠ¨ICï¼Œå½»åº•æœç»å‰è§†åå·®
2. å¤šå‘¨æœŸICè¯„åˆ†ï¼ŒæŒ‡æ•°è¡°å‡æƒé‡
3. ä¸¥æ ¼çš„æ—¶é—´åºåˆ—ç¨³å®šæ€§éªŒè¯
4. ä¸ºå®ç›˜ç¨³å®šç›ˆåˆ©ç­–ç•¥æœåŠ¡


Date: 2025-08-25
"""

import pandas as pd
import numpy as np
import json
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any
import warnings
from dataclasses import dataclass

warnings.filterwarnings('ignore')

from projects._03_factor_selection.factor_manager.ic_manager.rolling_ic_manager import run_cal_and_save_rolling_ic_by_snapshot_config_id
from projects._03_factor_selection.config_manager.config_snapshot.config_snapshot_manager import ConfigSnapshotManager
from quant_lib.config.logger_config import setup_logger

# å±‚æ¬¡èšç±»ç›¸å…³å¯¼å…¥
from scipy.cluster.hierarchy import linkage, fcluster, dendrogram
from scipy.spatial.distance import squareform
import matplotlib.pyplot as plt

logger = setup_logger(__name__)

@dataclass 
class RollingICSelectionConfig:
    """æ»šåŠ¨ICç­›é€‰é…ç½®"""
    # åŸºæœ¬ç­›é€‰é—¨æ§›
    min_snapshots: int = 3           # æœ€å°‘å¿«ç…§æ•°é‡
    min_ic_abs_mean: float = 0.01    # æ»šåŠ¨ICå‡å€¼ç»å¯¹å€¼é—¨æ§›
    min_ir_abs_mean: float = 0.15    # æ»šåŠ¨IRå‡å€¼ç»å¯¹å€¼é—¨æ§›
    min_ic_stability: float = 0.4    # ICç¨³å®šæ€§é—¨æ§›ï¼ˆæ–¹å‘ä¸€è‡´æ€§ï¼‰
    max_ic_volatility: float = 0.05  # ICæ³¢åŠ¨ç‡ä¸Šé™
    
    # å¤šå‘¨æœŸæƒé‡é…ç½®
    decay_rate: float = 0.75         # è¡°å‡ç‡ï¼Œè¶Šå°æƒé‡è¡°å‡è¶Šæ…¢
    prefer_short_term: bool = True   # åå‘çŸ­æœŸ
    
    # ç±»åˆ«å†…é€‰æ‹©
    max_factors_per_category: int = 3  # æ¯ç±»æœ€å¤šå› å­æ•°
    min_category_score: float = 10.0   # ç±»åˆ«æœ€ä½è¯„åˆ†
    
    # æœ€ç»ˆç­›é€‰
    max_final_factors: int = 8         # æœ€å¤šé€‰æ‹©å› å­æ•°
    
    # ç›¸å…³æ€§æ§åˆ¶ï¼ˆä¸‰å±‚å†³ç­–å“²å­¦ï¼‰
    high_corr_threshold: float = 0.7   # é«˜ç›¸å…³é˜ˆå€¼ï¼ˆçº¢è‰²è­¦æŠ¥ï¼šäºŒé€‰ä¸€ï¼‰
    medium_corr_threshold: float = 0.3 # ä¸­ä½ç›¸å…³åˆ†ç•Œï¼ˆé»„è‰²é¢„è­¦ï¼šæ­£äº¤åŒ–æˆ˜åœºï¼‰
    enable_orthogonalization: bool = True  # æ˜¯å¦å¯ç”¨ä¸­ç›¸å…³åŒºé—´æ­£äº¤åŒ–
    
    # å±‚æ¬¡èšç±»é…ç½®
    clustering_method: str = 'graph'   # èšç±»æ–¹æ³•: 'graph'(å›¾ç®—æ³•) æˆ– 'hierarchical'(å±‚æ¬¡èšç±»)
    hierarchical_distance_threshold: float = 0.3  # å±‚æ¬¡èšç±»è·ç¦»é˜ˆå€¼
    hierarchical_linkage_method: str = 'ward'  # è¿æ¥æ–¹æ³•: 'ward', 'complete', 'average'
    max_clusters: int = None  # æœ€å¤§ç°‡æ•°é‡é™åˆ¶ (Noneè¡¨ç¤ºä½¿ç”¨è·ç¦»é˜ˆå€¼)
    
    # å®ç›˜äº¤æ˜“æˆæœ¬æ§åˆ¶ï¼ˆæ¢æ‰‹ç‡ä¸€ç­‰å…¬æ°‘ï¼‰
    max_turnover_rate: float = 0.15    # æœ€å¤§æ¢æ‰‹ç‡é˜ˆå€¼ï¼ˆæœˆåº¦ï¼‰
    turnover_weight: float = 0.25      # æ¢æ‰‹ç‡åœ¨ç»¼åˆè¯„åˆ†ä¸­çš„æƒé‡
    enable_turnover_penalty: bool = True  # æ˜¯å¦å¯ç”¨æ¢æ‰‹ç‡æƒ©ç½š

    # 1. åŸºç¡€ä¹˜æ•°ç›¸å…³é…ç½®
    reward_turnover_rate_daily: float = 0.0025
    max_turnover_rate_daily: float = 0.007
    penalty_slope_daily: float = 45.0
    heavy_penalty_slope_daily: float = 100.0
    base_turnover_multiplier_floor: float = 0.1  # ã€æ–°å¢ã€‘åŸºç¡€ä¹˜æ•°çš„æœ€ä½å€¼ï¼Œé˜²æ­¢å˜ä¸ºè´Ÿæ•°

    # 2. æ³¢åŠ¨ç‡æƒ©ç½šç›¸å…³é…ç½®
    turnover_vol_threshold_ratio: float = 0.5
    turnover_vol_penalty_factor: float = 0.2

    # 3. è¶‹åŠ¿æƒ©ç½šç›¸å…³é…ç½®
    turnover_trend_sensitivity: float = 50.0  # ã€æ–°å¢ã€‘è¶‹åŠ¿æƒ©ç½šæ•æ„Ÿåº¦, å–ä»£äº†æ—§çš„*100

    # 4. æœ€ç»ˆä¹˜æ•°èŒƒå›´æ§åˆ¶
    final_multiplier_min: float = 0.1  # ã€æ–°å¢ã€‘æœ€ç»ˆä¹˜æ•°ä¸‹é™
    final_multiplier_max: float = 1.2  # ã€æ–°å¢ã€‘æœ€ç»ˆä¹˜æ•°ä¸Šé™
    # ç”¨äºç¡¬æ€§æ·˜æ±°çš„æœ€ç»ˆé˜²çº¿ (Final Gatekeeper Thresholds)
    max_turnover_mean_daily: float = 0.15    # ç¡¬é—¨æ§›ï¼šæ—¥å‡æ¢æ‰‹ç‡ä¸å¾—è¶…è¿‡1% (çº¦ç­‰äºæœˆåº¦21%)
    max_turnover_trend_daily: float = 0.00005 # ç¡¬é—¨æ§›ï¼šæ¢æ‰‹ç‡æ¯æ—¥æ¶åŒ–è¶‹åŠ¿ä¸å¾—è¶…è¿‡0.002%
    max_turnover_vol_daily: float = 0.015     # ç¡¬é—¨æ§›ï¼šæ¢æ‰‹ç‡æ³¢åŠ¨ç‡ä¸å¾—è¶…è¿‡1.5%


@dataclass
class FactorRollingICStats:
    """å› å­æ»šåŠ¨ICç»Ÿè®¡æ•°æ®"""
    factor_name: str
    periods_data: Dict[str, Dict]  # å„å‘¨æœŸæ•°æ®
    avg_ic_with_sign: float #å¸¦ç¬¦å·
    avg_ir_ir_with_sign: float
    avg_ic_abs: float              # å¹³å‡ICç»å¯¹å€¼
    avg_ir_abs: float              # å¹³å‡IRç»å¯¹å€¼
    best_period_ic_ir:float  #iræ‰€åœ¨ è¡¨ç°æœ€ä½³çš„å‘¨æœŸval
    nw_t_stat_series_mean:float
    avg_stability: float           # å¹³å‡ç¨³å®šæ€§
    avg_ic_volatility: float       # å¹³å‡ICæ³¢åŠ¨ç‡
    multi_period_score: float      # å¤šå‘¨æœŸç»¼åˆè¯„åˆ†
    snapshot_count: int            # å¿«ç…§æ•°é‡
    time_range: Tuple[str, str]    # æ—¶é—´èŒƒå›´
    
    # å®ç›˜äº¤æ˜“æˆæœ¬æ§åˆ¶
    # avg_daily_rank_change: float = 0.0    # å¹³å‡æœˆåº¦æ¢æ‰‹ç‡
    daily_rank_change_mean:float
    daily_turnover_trend:float
    daily_turnover_volatility:float
    turnover_adjusted_score: float = 0.0  # æ¢æ‰‹ç‡è°ƒæ•´åè¯„åˆ†
    

class RollingICFactorSelector:
    """åŸºäºæ»šåŠ¨ICçš„ä¸“ä¸šå› å­ç­›é€‰å™¨"""
    
    def __init__(self, snap_config_id: str, config: Optional[RollingICSelectionConfig] = None):
        """
        åˆå§‹åŒ–æ»šåŠ¨ICå› å­ç­›é€‰å™¨
        
        Args:
            snap_config_id: é…ç½®å¿«ç…§ID
            config: ç­›é€‰é…ç½®ï¼Œå¦‚æœä¸ºNoneä½¿ç”¨é»˜è®¤é…ç½®
        """
        self.snap_config_id = snap_config_id
        self.config = config or RollingICSelectionConfig()
        self.main_work_path = Path(r"D:\lqs\codeAbout\py\Quantitative\import_file\quant_research_portfolio\workspace\result")
        
        # ä»é…ç½®å¿«ç…§è·å–åŸºç¡€ä¿¡æ¯
        self._load_config_info()
        
        # å› å­åˆ†ç±»å®šä¹‰ - å®Œæ•´ç‰ˆæœ¬
        self.factor_categories = {
            'Value': ['bm_ratio', 'ep_ratio', 'cfp_ratio', 'sp_ratio', 'value_composite', 'pb_ratio', 'pe_ttm', 'ps_ratio'],
            'Quality': ['roe_ttm', 'gross_margin_ttm', 'debt_to_assets', 'earnings_stability', 'quality_momentum', 
                       'operating_accruals', 'asset_turnover', 'roa_ttm', 'current_ratio'],
            'Momentum': ['momentum_20d', 'momentum_120d', 'momentum_12_1', 'momentum_pct_60d', 'sharpe_momentum_60d', 
                        'sw_l1_momentum_21d', 'momentum_6_1', 'momentum_3_1'],
            'Reversal': ['reversal_5d', 'reversal_21d', 'reversal_1d', 'reversal_10d'],
            'Size': ['log_circ_mv', 'log_total_mv', 'market_cap_weight'],
            'Volatility': ['volatility_40d', 'volatility_90d', 'volatility_120d', 'rsi', 'atr_20d',
                          'volatility_40d_ç»è¿‡æ®‹å·®åŒ–', 'volatility_90d_ç»è¿‡æ®‹å·®åŒ–', 'volatility_120d_ç»è¿‡æ®‹å·®åŒ–', 'rsi_ç»è¿‡æ®‹å·®åŒ–'],
            'Liquidity': ['amihud_liquidity', 'turnover_rate_90d_mean', 'turnover_rate_monthly_mean', 'ln_turnover_value_90d', 
                         'turnover_t1_div_t20d_avg', 'bid_ask_spread', 'turnover_rate_90d_mean-ç»è¿‡æ®‹å·®åŒ–', 
                         'turnover_rate_monthly_mean_ç»è¿‡æ®‹å·®åŒ–', 'ln_turnover_value_90d_ç»è¿‡æ®‹å·®åŒ–'],
            'Technical': ['cci', 'pead', 'macd', 'rsi_divergence', 'cci_ç»è¿‡æ®‹å·®åŒ–', 'bollinger_position'],
            'Growth': ['total_revenue_growth_yoy', 'net_profit_growth_yoy', 'eps_growth', 'operating_revenue_growth'],
            'Profitability': ['gross_profit_margin', 'operating_margin', 'net_margin', 'ebit_margin'],
            'Efficiency': ['inventory_turnover', 'receivables_turnover', 'working_capital_turnover']
        }
        
        # ç¼“å­˜æ•°æ®
        self._factor_stats_cache = {}
        
        logger.info(f"æ»šåŠ¨ICå› å­ç­›é€‰å™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"é…ç½®ID: {self.snap_config_id}")
        logger.info(f"è‚¡ç¥¨æ± : {self.pool_index}")
        logger.info(f"æ—¶é—´èŒƒå›´: {self.start_date} - {self.end_date}")
        logger.info(f"æ•°æ®ç‰ˆæœ¬: {self.version}")
    
    def _load_config_info(self):
        """åŠ è½½é…ç½®ä¿¡æ¯"""
        config_manager = ConfigSnapshotManager()
        self.pool_index, self.start_date, self.end_date, self.config_evaluation = config_manager.get_snapshot_config_content_details(self.snap_config_id)
        self.version = f"{self.start_date}_{self.end_date}"
        self.forward_periods = self.config_evaluation.get('forward_periods', ['21'])
    
    def extract_factor_rolling_ic_stats(self, factor_name: str, force_generate: bool = False) -> Optional[FactorRollingICStats]:
        """
        æå–å•ä¸ªå› å­çš„æ»šåŠ¨ICç»Ÿè®¡æ•°æ®
        
        Args:
            factor_name: å› å­åç§°
            force_generate: æ˜¯å¦å¼ºåˆ¶é‡æ–°ç”Ÿæˆæ»šåŠ¨ICæ•°æ®
            
        Returns:
            FactorRollingICStats or None
        """
        # æ£€æŸ¥ç¼“å­˜
        if not force_generate and factor_name in self._factor_stats_cache:
            return self._factor_stats_cache[factor_name]
        
        # æ„å»ºæ•°æ®è·¯å¾„
        rolling_ic_dir = (self.main_work_path / self.pool_index / factor_name / 
                         'o2o' / self.version / 'rolling_ic')
        
        # å¦‚æœç›®å½•ä¸å­˜åœ¨ï¼Œå°è¯•ç”Ÿæˆ
        if not rolling_ic_dir.exists() or force_generate:
            logger.info(f"ä¸ºå› å­ {factor_name} ç”Ÿæˆæ»šåŠ¨ICæ•°æ®...")
            try:
                run_cal_and_save_rolling_ic_by_snapshot_config_id(self.snap_config_id, [factor_name])
                logger.info(f"å› å­ {factor_name} æ»šåŠ¨ICæ•°æ®ç”ŸæˆæˆåŠŸ")
            except Exception as e:
                raise ValueError(f"ç”Ÿæˆæ»šåŠ¨ICæ•°æ®å¤±è´¥ {factor_name}: {e}")

        # æ£€æŸ¥æ–‡ä»¶
        ic_files = list(rolling_ic_dir.glob("ic_snapshot_*.json"))
        if not ic_files:
            raise ValueError(f"æœªæ‰¾åˆ°å› å­ {factor_name} çš„æ»šåŠ¨ICæ–‡ä»¶")

        
        if len(ic_files) < self.config.min_snapshots:
            raise ValueError(f"å› å­ {factor_name} æ»šåŠ¨ICå¿«ç…§æ•°é‡ä¸è¶³: {len(ic_files)} < {self.config.min_snapshots}")

        
        # è§£ææ•°æ®
        periods_data = {}
        dates_range = []
        
        for ic_file in ic_files:
            try:
                with open(ic_file, 'r', encoding='utf-8') as f:
                    snapshot = json.load(f)

                calc_date = snapshot['calculation_date']
                dates_range.append(calc_date)
                ic_stats_snap = snapshot.get('ic_stats', {})

                for period, stats in ic_stats_snap.items():
                    if period not in periods_data:
                        periods_data[period] = []
                    periods_data[period].append({
                        'date': calc_date,
                        'ic_mean': stats.get('ic_mean', 0),#åº•å±‚æ˜¯ewmaæ¥çš„
                        'ic_ir': stats.get('ic_ir', 0),#åº•å±‚æ˜¯ewmaæ¥çš„
                        'ic_win_rate': stats.get('ic_win_rate', 0.5),#åº•å±‚æ˜¯ewmaæ¥çš„
                        'avg_daily_rank_change_stats': stats.get('avg_daily_rank_change_stats'),
                        'ic_std': stats.get('ic_std', 0),
                        'ic_t_stat': stats.get('ic_t_stat', 0),
                        'ic_nw_t_stat': stats.get('ic_nw_t_stat', 0),
                        'ic_nw_p_value': stats.get('ic_nw_p_value', 1.0)#åº•å±‚æ˜¯Newey-West T-stat
                    })


            except Exception as e:
                raise ValueError(f"è¯»å–ICå¿«ç…§æ–‡ä»¶ {ic_file} å¤±è´¥: {e}")

        
        if not periods_data:
            raise ValueError(f"å› å­ {factor_name} æ— æœ‰æ•ˆçš„æ»šåŠ¨ICæ•°æ®")

        
        # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡ periods_dataï¼šå­˜æ¯ä¸ªæœˆåº•çš„icæ•°æ® ä¾¿äºåç»­ç»Ÿaver
        ## periods_data å†…å®¹
        # 11:[1æœˆ31çš„å¿«ç…§icæ•°æ®ï¼Œ2æœˆ31çš„å¿«ç…§icæ•°æ®ï¼Œ3æœˆ31çš„æ•°æ®..]
        # 5d:[1æœˆ31çš„å¿«ç…§icæ•°æ®ï¼Œ2æœˆ31çš„å¿«ç…§icæ•°æ®ï¼Œ3æœˆ31çš„æ•°æ®..]
        # #
        factor_stats = self._calculate_factor_statistics(factor_name, periods_data, dates_range)
        
        # ç¼“å­˜ç»“æœ
        self._factor_stats_cache[factor_name] = factor_stats
        
        return factor_stats
    
    def _calculate_factor_statistics(self, factor_name: str, periods_data: Dict, dates_range: List[str]) -> FactorRollingICStats:
        """è®¡ç®—å› å­ç»Ÿè®¡æŒ‡æ ‡"""
        
        # æ±‡æ€»å„å‘¨æœŸç»Ÿè®¡
        aggregated_periods = {}
        all_ic_means = []
        all_ic_irs = []
        all_stabilities = []
        all_ic_stds = []
        
        for period, time_series in periods_data.items():
            if len(time_series) < self.config.min_snapshots:
                continue
            
            # æå–æ—¶é—´åºåˆ—æ•°æ®
            ic_means = [d['ic_mean'] for d in time_series]
            ic_irs = [d['ic_ir'] for d in time_series]
            ic_win_rates = [d['ic_win_rate'] for d in time_series]
            ic_stds = [d['ic_std'] for d in time_series]
            nw_t_stat_series = [d['ic_nw_t_stat'] for d in time_series]

            # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡ æ±‚ï¼ˆ1æœˆ31å¿«ç…§icæ•°æ®...+næœˆå¿«ç…§æ•°æ®ï¼‰/n å¹³å‡
            avg_ic_period = np.mean(ic_means)
            avg_ir_period = np.mean(ic_irs)
            avg_win_rate_period = np.mean(ic_win_rates)
            ic_volatility_period = np.std(ic_means)
            nw_t_stat_series_mean = float(np.mean(nw_t_stat_series))

            # ICæ–¹å‘ä¸€è‡´æ€§ï¼ˆç¨³å®šæ€§ï¼‰
            if len(ic_means) > 1:
                # æ ¸å¿ƒæ€æƒ³ï¼šç¨³å®šæ€§ï¼Œæ˜¯æŒ‡â€œæ»šåŠ¨çš„ICç¬¦å·â€ä¸è¿™æ®µæ—¶æœŸçš„â€œå¹³å‡ICç¬¦å·â€æ˜¯å¦ä¸€è‡´
                # 1. ç¡®å®šè¿™ä¸ªå‘¨æœŸçš„â€œæœŸæœ›æ–¹å‘â€ï¼Œå³ICå‡å€¼çš„ç¬¦å·
                expected_sign = np.sign(avg_ic_period)

                # 2. å¤„ç†å‡å€¼ä¸º0çš„ç½•è§æƒ…å†µ, é»˜è®¤ä¸ºæ­£å‘
                if expected_sign == 0:
                    expected_sign = 1

                # 3. è®¡ç®—æœ‰å¤šå°‘æ»šåŠ¨ICçš„ç¬¦å·ä¸â€œæœŸæœ›æ–¹å‘â€ä¸€è‡´
                #    è¿™é‡Œæˆ‘ä»¬ç”¨ np.sign æ¥å¤„ç†ï¼Œæ¯” (ic > 0) æ›´ä¸¥è°¨ï¼Œå¯ä»¥æ­£ç¡®å¤„ç†icä¸º0çš„æƒ…å†µ
                num_consistent = sum(1 for ic in ic_means if np.sign(ic) == expected_sign)
                stability = num_consistent / len(ic_means)
            else:
                stability = 1.0
            
            aggregated_periods[period] = {
                'ic_mean_avg': avg_ic_period,
                'ic_ir_avg': avg_ir_period,
                'ic_win_rate_avg': avg_win_rate_period,
                'ic_volatility_period': ic_volatility_period,
                'ic_stability': stability,#æ–¹å‘ä¸€è‡´æ€§
                'sample_count': len(time_series),
                'nw_t_stat_series_mean':nw_t_stat_series_mean,
                'time_series': time_series
            }
            
            # æ”¶é›†å…¨å±€ç»Ÿè®¡
            all_ic_means.append(avg_ic_period)
            all_ic_irs.append(avg_ir_period)
            all_stabilities.append(stability)
            all_ic_stds.append(ic_volatility_period)

        # å‘¨æœŸåŠ æƒ è®¡ç®—å‡ºå¹³å‡ic
        ic_means_with_sign = [aggregated_periods[p]['ic_mean_avg'] for p in periods_data.keys()]
        ic_irs_with_sign = [aggregated_periods[p]['ic_ir_avg'] for p in periods_data.keys()]

        decay_rate = self.config.decay_rate
        weights = np.array([decay_rate ** i for i in range(len(periods_data.keys()))])
        weights /= weights.sum()

        # å¾—åˆ°æœ€æ ¸å¿ƒçš„ä¸¤ä¸ªç»¼åˆæŒ‡æ ‡
        avg_ic_with_sign = float(np.average(ic_means_with_sign, weights=weights))
        avg_ic_ir_with_sign = float(np.average(ic_irs_with_sign, weights=weights))
        best_period_ic_ir = ic_irs_with_sign.max()

        # 3. ä»ç»¼åˆæŒ‡æ ‡æ´¾ç”Ÿå‡ºç”¨äºç­›é€‰çš„ç»å¯¹å€¼æŒ‡æ ‡
        avg_ic_abs = abs(avg_ic_with_sign)
        avg_ir_abs = abs(avg_ic_ir_with_sign)

        # é€‰æ‹©ä¸€ä¸ªå‚è€ƒå‘¨æœŸ (é€šå¸¸é€‰æ‹©æœ€çŸ­çš„ï¼Œæ•°æ®æœ€å®Œæ•´) æˆªæ–­å°‘
        #      æˆ‘ä»¬å¯¹periods_dataçš„é”®ï¼ˆä¹Ÿå°±æ˜¯å‘¨æœŸï¼‰è¿›è¡Œæ•°å­—æ’åºæ¥æ‰¾åˆ°æœ€çŸ­çš„é‚£ä¸ª
        reference_period = sorted(periods_data.keys())[0]
        # 4.2. ä»å‚è€ƒå‘¨æœŸä¸­æå–å®Œæ•´çš„å¿«ç…§æ—¶é—´åºåˆ— (60ä¸ªå¿«ç…§çš„åˆ—è¡¨)
        reference_time_series = [snap['avg_daily_rank_change_stats'] for snap in periods_data[reference_period]]

        # 4.3. æå–ä¸‰ä¸ªæ ¸å¿ƒæŒ‡æ ‡å„è‡ªçš„æ—¶é—´åºåˆ—
        #      ä½¿ç”¨ .get() æ¥å®‰å…¨åœ°è·å–å€¼ï¼Œä»¥é˜²æŸä¸ªå¿«ç…§æ•°æ®ç¼ºå¤±
        avg_daily_rank_change_series  = [d.get('avg_daily_rank_change', 0) for d in reference_time_series]
        daily_turnover_volatility_series = [d.get('daily_turnover_volatility', 0) for d in reference_time_series]
        daily_turnover_trend_series = [d.get('daily_turnover_trend', 0) for d in reference_time_series]
        # 4.4. è®¡ç®—æ•´ä¸ªäº”å¹´æœŸé—´çš„æ€»å¹³å‡ç»Ÿè®¡å€¼
        final_avg_change = float(np.mean(avg_daily_rank_change_series))
        final_avg_vol = float(np.mean(daily_turnover_volatility_series))  # å¯¹æ³¢åŠ¨ç‡æ±‚å‡å€¼ï¼Œè¡¡é‡å¹³å‡ä¸ç¡®å®šæ€§
        final_avg_trend = float(np.mean(daily_turnover_trend_series))  # å¯¹è¶‹åŠ¿æ±‚å‡å€¼ï¼Œè¡¡é‡é•¿æœŸè¡°å‡å€¾å‘
        # if(IS_DEBUG_TEMP and (factor_name in ['turnover_rate_monthly_mean','volatility_40d'])):
        #     final_avg_change = 0.01
        #     final_avg_vol = 0.01

        # 4.5. ç»„è£…æˆæœ€ç»ˆçš„ç»Ÿè®¡å­—å…¸ï¼Œç”¨äºè¯„åˆ†å‡½æ•°
        final_turnover_stats = {
            'avg_daily_rank_change': final_avg_change,
            'daily_turnover_volatility': final_avg_vol,
            'daily_turnover_trend': final_avg_trend
        }

        # è®¡ç®—å¤šå‘¨æœŸç»¼åˆè¯„åˆ†
        multi_period_score = self._calculate_multi_period_score(aggregated_periods)
        
        # è®¡ç®—æ¢æ‰‹ç‡è°ƒæ•´åè¯„åˆ†ï¼ˆå®ç›˜å¯¼å‘ï¼‰
        turnover_adjusted_score = self._calculate_turnover_adjusted_score(
            multi_period_score, final_turnover_stats
        )
        
        factor_stats = FactorRollingICStats(
            factor_name=factor_name,
            periods_data=aggregated_periods,
            avg_ic_with_sign=avg_ic_with_sign,
            avg_ir_ir_with_sign=avg_ic_ir_with_sign,
            avg_ic_abs= avg_ic_abs,
            avg_ir_abs=avg_ir_abs,
            nw_t_stat_series_mean=nw_t_stat_series_mean,
            avg_stability=np.mean(all_stabilities) if all_stabilities else 0.0,
            avg_ic_volatility=np.mean(all_ic_stds) if all_ic_stds else 0.0,
            multi_period_score=multi_period_score,
            snapshot_count=len(dates_range),
            time_range=(min(dates_range), max(dates_range)) if dates_range else ('', ''),
            # å°†ä¸‰ä¸ªæ ¸å¿ƒæ¢æ‰‹ç‡æŒ‡æ ‡å¡«å…¥è¿”å›ç»“æ„
            daily_rank_change_mean=final_turnover_stats['avg_daily_rank_change'],
            daily_turnover_trend=final_turnover_stats['daily_turnover_trend'],
            daily_turnover_volatility=final_turnover_stats['daily_turnover_volatility'],
            turnover_adjusted_score=turnover_adjusted_score
        )
        # æ„å»ºç»“æœ

        return factor_stats

    def _calculate_multi_period_score(self, periods_data: Dict) -> float:
        """
        è®¡ç®—å¤šå‘¨æœŸICç»¼åˆè¯„åˆ†ï¼ˆå¸¦æŒ‡æ•°è¡°å‡æƒé‡ï¼‰

        Args:
            periods_data: å¤šå‘¨æœŸæ•°æ® {period: stats}

        Returns:
            float: ç»¼åˆè¯„åˆ†
        """
        if not periods_data:
            return 0.0

        # æŒ‰å‘¨æœŸæ’åºï¼ˆçŸ­æœŸåˆ°é•¿æœŸï¼‰
        try:
            periods = sorted(periods_data.keys(), key=lambda x: int(x.replace('d', '').replace('D', '')))
        except:
            periods = sorted(periods_data.keys())

        # è®¡ç®—æ¯ä¸ªå‘¨æœŸçš„å¾—åˆ†
        period_scores = []

        # --- æ ¸å¿ƒæ”¹é€ ï¼šå®šä¹‰â€œæ»¡åˆ†æ ‡æ†â€ ---
        IC_MEAN_BENCHMARK = 0.05  # ICå‡å€¼è¾¾åˆ°0.05ï¼Œæˆ‘ä»¬è®¤ä¸ºè¡¨ç°ä¼˜å¼‚
        IC_IR_BENCHMARK = 0.50  # IRè¾¾åˆ°0.5ï¼Œæˆ‘ä»¬è®¤ä¸ºç¨³å®šæ€§ä¼˜å¼‚
        IC_STABILITY_BENCHMARK = 1.0  # ç¨³å®šæ€§æ˜¯0-1ï¼Œæ ‡æ†å°±æ˜¯1.0
        IC_WIN_RATE_BENCHMARK = 0.60  # èƒœç‡è¾¾åˆ°60%ï¼Œæˆ‘ä»¬è®¤ä¸ºå¾ˆä¸é”™
        IC_VOL_PENALTY_BASE = 0.02  # ICæ³¢åŠ¨ç‡è¶…è¿‡2%å¼€å§‹æƒ©ç½š

        # --- æ ¸å¿ƒæ”¹é€ ï¼šå®šä¹‰æœ€ç»ˆçš„æƒé‡é…æ¯” ---
        # è¿™äº›æƒé‡ç°åœ¨æ˜¯çœŸå®çš„ã€å¯æ¯”çš„
        WEIGHTS = {
            'ic_mean': 0.40,  # 40% æƒé‡ç»™æ•ˆæœ
            'ic_ir': 0.30,  # 30% æƒé‡ç»™ç¨³å®šæ€§
            'ic_stability': 0.10,  # 10% æƒé‡ç»™å¦ä¸€ç§ç¨³å®šæ€§
            'ic_win_rate': 0.20  # 20% æƒé‡ç»™èƒœç‡
        }

        for period in periods:
            stats = periods_data[period]

            # --- æ ¸å¿ƒæ”¹é€ ï¼šå…ˆå½’ä¸€åŒ–ï¼Œå¾—åˆ°0-1ä¹‹é—´çš„åˆ†æ•° ---
            ic_norm_score = min(abs(stats.get('ic_mean_avg', 0)) / IC_MEAN_BENCHMARK, 1.0)
            ir_norm_score = min(abs(stats.get('ic_ir_avg', 0)) / IC_IR_BENCHMARK, 1.0)
            stability_norm_score = min(stats.get('ic_stability', 0) / IC_STABILITY_BENCHMARK, 1.0)

            # èƒœç‡ä»¥50%ä¸ºåŸºå‡†
            win_rate_norm_score = max(0, (stats.get('ic_win_rate_avg', 0.5) - 0.5) / (IC_WIN_RATE_BENCHMARK - 0.5))
            win_rate_norm_score = min(win_rate_norm_score, 1.0)

            # æƒ©ç½šé¡¹
            volatility_penalty = max(0, (stats.get('ic_volatility', 0) - IC_VOL_PENALTY_BASE) * 20)  # æƒ©ç½šåŠ›åº¦å¯ä»¥è°ƒæ•´

            # --- æ ¸å¿ƒæ”¹é€ ï¼šå†åŠ æƒ ---
            # ç°åœ¨ï¼Œæ‰€æœ‰åˆ†æ•°éƒ½åœ¨0-1èŒƒå›´ï¼Œæƒé‡å¯ä»¥å…¬å¹³åœ°å‘æŒ¥ä½œç”¨
            weighted_score = (ic_norm_score * WEIGHTS['ic_mean'] +
                              ir_norm_score * WEIGHTS['ic_ir'] +
                              stability_norm_score * WEIGHTS['ic_stability'] +
                              win_rate_norm_score * WEIGHTS['ic_win_rate'])

            # åº”ç”¨æƒ©ç½šå¹¶ç¡®ä¿åˆ†æ•°åœ¨0-100ä¹‹é—´ (ä¹˜ä»¥100æ–¹ä¾¿é˜…è¯»)
            total_score = (weighted_score - volatility_penalty) * 100
            period_scores.append(max(0, total_score))

        if not period_scores:
            return 0.0

        # åº”ç”¨æŒ‡æ•°è¡°å‡æƒé‡ï¼ˆçŸ­æœŸæƒé‡æ›´é«˜ï¼‰
        decay_rate = self.config.decay_rate
        weights = np.array([decay_rate ** i for i in range(len(period_scores))])
        weights /= weights.sum()  # æƒé‡å½’ä¸€åŒ–

        # è®¡ç®—åŠ æƒå¹³å‡åˆ†æ•°
        final_score = np.average(period_scores, weights=weights)

        return final_score

    def _estimate_factor_turnover(self, factor_name: str, periods_data: Dict) -> float:
        """
        ä¼°ç®—å› å­æ¢æ‰‹ç‡ï¼ˆå®ç›˜äº¤æ˜“æˆæœ¬æ ¸å¿ƒæŒ‡æ ‡ï¼‰
        
        Args:
            factor_name: å› å­åç§°
            periods_data: å„å‘¨æœŸæ•°æ®
            
        Returns:
            float: æœˆåº¦å¹³å‡æ¢æ‰‹ç‡ä¼°ç®—
        """
        try:
            # æ ¹æ®å› å­ç±»å‹ä¼°ç®—æ¢æ‰‹ç‡ï¼ˆåŸºäºç»éªŒå’Œç ”ç©¶ï¼‰
            turnover_estimates = {
                # é«˜é¢‘ç±»å› å­ï¼ˆæŠ€æœ¯é¢ï¼‰
                'reversal_1d': 0.30, 'reversal_5d': 0.25, 'reversal_10d': 0.20,
                'momentum_20d': 0.18, 'rsi': 0.22, 'cci': 0.24,
                'macd': 0.20, 'bollinger_position': 0.28,
                
                # ä¸­é¢‘ç±»å› å­ï¼ˆä»·é‡ç»“åˆï¼‰
                'momentum_60d': 0.15, 'momentum_120d': 0.12, 'momentum_12_1': 0.10,
                'volatility_40d': 0.16, 'volatility_90d': 0.14, 'volatility_120d': 0.12,
                'amihud_liquidity': 0.14, 'turnover_rate_90d_mean': 0.16,
                
                # ä½é¢‘ç±»å› å­ï¼ˆåŸºæœ¬é¢ï¼‰
                'ep_ratio': 0.08, 'bm_ratio': 0.07, 'sp_ratio': 0.08, 'cfp_ratio': 0.09,
                'roe_ttm': 0.06, 'gross_margin_ttm': 0.05, 'earnings_stability': 0.04,
                'total_revenue_growth_yoy': 0.07, 'net_profit_growth_yoy': 0.08,
                
                # è§„æ¨¡å› å­ï¼ˆæä½é¢‘ï¼‰
                'log_circ_mv': 0.03, 'log_total_mv': 0.03, 'market_cap_weight': 0.02,
                
                # è´¨é‡å› å­ï¼ˆä½é¢‘ï¼‰
                'debt_to_assets': 0.05, 'current_ratio': 0.04, 'asset_turnover': 0.06,
                'quality_momentum': 0.09
            }
            
            # åŸºç¡€æ¢æ‰‹ç‡ä¼°ç®—
            base_turnover = turnover_estimates.get(factor_name, 0.12)  # é»˜è®¤12%
            
            # æ ¹æ®ICç¨³å®šæ€§è°ƒæ•´ï¼ˆç¨³å®šæ€§ä½çš„å› å­é€šå¸¸æ¢æ‰‹ç‡æ›´é«˜ï¼‰
            if periods_data:
                avg_stability = np.mean([
                    stats.get('ic_stability', 0.5) 
                    for stats in periods_data.values()
                ])
                # ç¨³å®šæ€§è¶Šä½->avg_stabilityè¶Šä½->stability_adjustmentè¶Šå¤§-ã€‹adjusted_turnoverè¶Šå¤§==æ¢æ‰‹ç‡è°ƒæ•´ç³»æ•°è¶Šé«˜
                stability_adjustment = 1.0 + (0.5 - avg_stability) * 0.8
                adjusted_turnover = base_turnover * stability_adjustment
            else:
                adjusted_turnover = base_turnover
            
            # æ¢æ‰‹ç‡åˆç†èŒƒå›´æ§åˆ¶
            final_turnover = np.clip(adjusted_turnover, 0.02, 0.50)
            
            return final_turnover
            
        except Exception as e:
            logger.debug(f"æ¢æ‰‹ç‡ä¼°ç®—å¤±è´¥ {factor_name}: {e}")
            return 0.12  # é»˜è®¤æ¢æ‰‹ç‡12%
    #å•ä¾§é€šè¿‡
    def _calculate_turnover_adjusted_score(self, base_score: float, turnover_stats: Dict) -> float:
        """
        è®¡ç®—åŸºäºå¤šç»´åº¦æ¢æ‰‹ç‡æŒ‡æ ‡çš„è°ƒæ•´åè¯„åˆ† (V3 - æœ€ç»ˆç”Ÿäº§ç‰ˆ)

        æ­¤ç‰ˆæœ¬ç»è¿‡ä¸¥æ ¼å®¡æŸ¥å’ŒåŠ å›ºï¼Œè§£å†³äº†ä¸­é—´å€¼ä¿æŠ¤ã€è¶‹åŠ¿æƒ©ç½šæ•æ„Ÿåº¦ã€
        åˆ†æ•°ç¬¦å·ä¿ç•™å’Œæ•°å€¼ç¨³å®šæ€§ç­‰é—®é¢˜ï¼Œç¬¦åˆå®ç›˜ç”Ÿäº§è¦æ±‚ã€‚

        Args:
            base_score: åŸºç¡€ICè¯„åˆ† (å¯èƒ½ä¸ºè´Ÿ)
            turnover_stats: æ¥è‡ª _calculate_daily_rank_change çš„å®Œæ•´ç»Ÿè®¡å­—å…¸

        Returns:
            float: æ¢æ‰‹ç‡è°ƒæ•´åè¯„åˆ†ï¼Œä¿ç•™åŸå§‹base_scoreçš„ç¬¦å·
        """
        if not self.config.enable_turnover_penalty:
            return base_score

        # ä½¿ç”¨ä¸€ä¸ªæå°å€¼æ¥ä¿è¯æ•°å€¼ç¨³å®šæ€§
        epsilon = 1e-8

        # --- 1. åŸºç¡€ä¹˜æ•° (åŸºäºæ¢æ‰‹ç‡å‡å€¼) ---
        avg_daily_rank_change = turnover_stats.get('avg_daily_rank_change', 0.01)

        reward_rate_daily = self.config.reward_turnover_rate_daily
        max_rate_daily = self.config.max_turnover_rate_daily
        penalty_slope = self.config.penalty_slope_daily
        heavy_penalty_slope = self.config.heavy_penalty_slope_daily

        if avg_daily_rank_change <= reward_rate_daily:
            base_turnover_multiplier = 1.0 + (avg_daily_rank_change / (reward_rate_daily + epsilon)) * 0.1
        elif avg_daily_rank_change <= max_rate_daily:
            base_turnover_multiplier = 1.1 - (avg_daily_rank_change - reward_rate_daily) * penalty_slope
        else:
            boundary_multiplier = 1.1 - (max_rate_daily - reward_rate_daily) * penalty_slope
            excess_turnover = avg_daily_rank_change - max_rate_daily
            base_turnover_multiplier = boundary_multiplier - excess_turnover * heavy_penalty_slope

        # ã€V3 æ ¸å¿ƒæ”¹è¿›ã€‘å¯¹åŸºç¡€ä¹˜æ•°æœ¬èº«è¿›è¡Œæ•°å€¼ä¿æŠ¤ï¼Œé˜²æ­¢å…¶å˜ä¸ºè´Ÿæˆ–è¿‡å°
        base_turnover_multiplier = max(base_turnover_multiplier, self.config.base_turnover_multiplier_floor)

        # --- 2. æ³¢åŠ¨ç‡æƒ©ç½šä¹˜æ•° ---
        volatility = turnover_stats.get('daily_turnover_volatility', 0)
        volatility_threshold_ratio = self.config.turnover_vol_threshold_ratio
        volatility_penalty_factor = self.config.turnover_vol_penalty_factor

        volatility_penalty_multiplier = 1.0

        ratio = volatility / (avg_daily_rank_change + epsilon)
        if ratio > volatility_threshold_ratio:
            excess_ratio = ratio - volatility_threshold_ratio
            penalty = excess_ratio * volatility_penalty_factor
            volatility_penalty_multiplier = max(0.8, 1.0 - penalty)  # æƒ©ç½šä¸‹é™0.8ä¿æŒä¸å˜

        # --- 3. è¶‹åŠ¿æƒ©ç½šä¹˜æ•° ---
        trend = turnover_stats.get('daily_turnover_trend', 0)
        trend_penalty_multiplier = 1.0

        if trend > 0:
            relative_trend = trend / (avg_daily_rank_change + epsilon)
            sensitivity = self.config.turnover_trend_sensitivity

            # ã€V3 æ ¸å¿ƒæ”¹è¿›ã€‘ç§»é™¤äº† *100 çš„ç¡¬ç¼–ç ï¼Œä½¿ç”¨æ›´çµæ´»çš„æ•æ„Ÿåº¦å‚æ•°
            trend_penalty_multiplier = np.exp(-relative_trend * sensitivity)
            trend_penalty_multiplier = max(0.7, trend_penalty_multiplier)  # æƒ©ç½šä¸‹é™0.7ä¿æŒä¸å˜

        # === 4. æœ€ç»ˆè®¡ç®— ===
        total_turnover_multiplier = base_turnover_multiplier * volatility_penalty_multiplier * trend_penalty_multiplier

        weight = self.config.turnover_weight
        final_multiplier = (1 - weight) + weight * total_turnover_multiplier

        # ä½¿ç”¨å¯é…ç½®çš„ä¸Šä¸‹é™è¿›è¡Œæœ€ç»ˆè£å‰ª
        final_multiplier = np.clip(
            final_multiplier,
            self.config.final_multiplier_min,
            self.config.final_multiplier_max
        )

        adjusted_score = base_score * final_multiplier

        # ã€V3 æ ¸å¿ƒæ”¹è¿›ã€‘ç§»é™¤ max(0.0, ...)ï¼Œä¿ç•™åˆ†æ•°çš„åŸå§‹ç¬¦å·
        logger.info(f"final_multiplier:{final_multiplier} total_turnover_multiplier:{total_turnover_multiplier}")
        return adjusted_score

    def screen_factors_by_rolling_ic(self, factor_names: List[str], force_generate: bool = False) -> Dict[str, FactorRollingICStats]:
        """
        åŸºäºæ»šåŠ¨ICç­›é€‰å› å­
        
        Args:
            factor_names: å€™é€‰å› å­åˆ—è¡¨
            force_generate: æ˜¯å¦å¼ºåˆ¶é‡æ–°ç”Ÿæˆæ»šåŠ¨IC
            
        Returns:
            Dict[factor_name, FactorRollingICStats]: é€šè¿‡ç­›é€‰çš„å› å­
        """
        logger.info(f"å¼€å§‹åŸºäºæ»šåŠ¨ICç­›é€‰ {len(factor_names)} ä¸ªå› å­...")
        
        qualified_factors = {}
        
        for i, factor_name in enumerate(factor_names, 1):
            logger.info(f"å¤„ç†å› å­ {i}/{len(factor_names)}: {factor_name}")
            
            try:
                # æå–å› å­ç»Ÿè®¡
                factor_stats = self.extract_factor_rolling_ic_stats(factor_name, force_generate)
                
                if factor_stats is None:
                    raise ValueError(f"å› å­ {factor_name}: æ— æ³•è·å–æ»šåŠ¨ICç»Ÿè®¡")

                # åº”ç”¨ç­›é€‰æ¡ä»¶
                passes_screening = self._evaluate_factor_quality(factor_stats)
                
                if passes_screening:
                    qualified_factors[factor_name] = factor_stats
                    direction = "+" if  np.sign(factor_stats.avg_ic_with_sign) > 0 else "-"
                    logger.info(f"  {direction} {factor_name}: é€šè¿‡ç­›é€‰")
                    logger.info(f"    IC={factor_stats.avg_ic_abs:.3f}, IR={factor_stats.avg_ir_abs:.2f}")
                    logger.info(f"    ç¨³å®šæ€§={factor_stats.avg_stability:.2f}, æ—¥æ¢æ‰‹ç‡={factor_stats.daily_rank_change_mean:.1%}")
                    logger.info(f"    åŸºç¡€è¯„åˆ†={factor_stats.multi_period_score:.1f}, è°ƒæ•´è¯„åˆ†={factor_stats.turnover_adjusted_score:.1f}")
                else:
                    logger.info(f"  - {factor_name}: æœªé€šè¿‡ç­›é€‰")
                    
            except Exception as e:
                raise ValueError(f"å¤„ç†å› å­ {factor_name} æ—¶å‡ºé”™: {e}")
                # continue
        
        logger.info(f"byæ»šåŠ¨IC_ç­›é€‰(icï¼ˆç¨³å®šã€èƒœç‡ï¼‰ã€å‘¨æœŸã€æ¢æ‰‹ç‡)å®Œæˆ: {len(qualified_factors)}/{len(factor_names)} ä¸ªå› å­é€šè¿‡")
        return qualified_factors
    
    def _evaluate_factor_quality(self, factor_stats: FactorRollingICStats) -> bool:
        """
        è¯„ä¼°å› å­è´¨é‡æ˜¯å¦é€šè¿‡ç­›é€‰ï¼ˆå®ç›˜å¯¼å‘ï¼Œæ¢æ‰‹ç‡ä¸€ç­‰å…¬æ°‘ï¼‰
        """
        
        # åŸºæœ¬é—¨æ§›æ£€æŸ¥
        basic_conditions = [
            factor_stats.avg_ic_abs >= self.config.min_ic_abs_mean,
            factor_stats.avg_ir_abs >= self.config.min_ir_abs_mean,
            factor_stats.avg_stability >= self.config.min_ic_stability,
            factor_stats.avg_ic_volatility <= self.config.max_ic_volatility,
            factor_stats.multi_period_score >= self.config.min_category_score,
            factor_stats.snapshot_count >= self.config.min_snapshots
        ]

        # æ¢æ‰‹ç‡é—¨æ§›æ£€æŸ¥ï¼ˆå®ç›˜äº¤æ˜“æˆæœ¬æ§åˆ¶ï¼‰
        turnover_condition = (
                not self.config.enable_turnover_penalty  or (
                # ç¡¬é—¨æ§›1: å¹³å‡æ¢æ‰‹ç‡ä¸èƒ½è¿‡é«˜ ("ç®€å†å…³")
                factor_stats.daily_rank_change_mean <= self.config.max_turnover_mean_daily and

                # ç¡¬é—¨æ§›2: æ¢æ‰‹ç‡æ¶åŒ–è¶‹åŠ¿ä¸èƒ½ä¸ºæ­£ ("é¢è¯•å…³ - é‡å¤§é£é™©é¡¹")
                factor_stats.daily_turnover_trend <= self.config.max_turnover_trend_daily and

                # ç¡¬é—¨æ§›3: æ¢æ‰‹ç‡æ³¢åŠ¨ç‡ä¸èƒ½è¿‡é«˜ ("èƒŒæ™¯è°ƒæŸ¥å…³")
                factor_stats.daily_turnover_volatility <= self.config.max_turnover_vol_daily
        )
        )
        
        all_conditions = basic_conditions + [turnover_condition]
        
        # è®°å½•è¯¦ç»†çš„æœªé€šè¿‡åŸå› ï¼ˆä¾¿äºè°ƒè¯•ï¼‰
        if not all(all_conditions):
            failed_checks = []
            if factor_stats.avg_ic_abs < self.config.min_ic_abs_mean:
                failed_checks.append(f"ICå‡å€¼è¿‡ä½({factor_stats.avg_ic_abs:.3f}<{self.config.min_ic_abs_mean})")
            if factor_stats.avg_ir_abs < self.config.min_ir_abs_mean:
                failed_checks.append(f"IRè¿‡ä½({factor_stats.avg_ir_abs:.2f}<{self.config.min_ir_abs_mean})")
            if factor_stats.avg_stability < self.config.min_ic_stability:
                failed_checks.append(f"ç¨³å®šæ€§ä¸è¶³({factor_stats.avg_stability:.2%}<{self.config.min_ic_stability:.0%})")
            if factor_stats.avg_ic_volatility > self.config.max_ic_volatility:
                failed_checks.append(f"ICæ³¢åŠ¨è¿‡é«˜({factor_stats.avg_ic_volatility:.3f}>{self.config.max_ic_volatility})")
            if factor_stats.multi_period_score < self.config.min_category_score:
                failed_checks.append(f"ç»¼åˆè¯„åˆ†è¿‡ä½({factor_stats.multi_period_score:.1f}<{self.config.min_category_score})")
            if factor_stats.snapshot_count < self.config.min_snapshots:
                failed_checks.append(f"å¿«ç…§ä¸è¶³({factor_stats.snapshot_count}<{self.config.min_snapshots})")
            if (self.config.enable_turnover_penalty and 
                factor_stats.daily_rank_change_mean > self.config.max_turnover_mean_daily):
                failed_checks.append(f"æ—¥æ¢æ‰‹ç‡è¿‡é«˜({factor_stats.daily_rank_change_mean:.1%}>{self.config.max_turnover_mean_daily:.0%})")

            if (self.config.enable_turnover_penalty and
                    factor_stats.daily_turnover_trend > self.config.max_turnover_trend_daily):
                failed_checks.append(
                    f"æ¢æ‰‹ç‡æ¯æ—¥æ¶åŒ–è¶‹åŠ¿ä¸å¾—è¶…è¿‡2%({factor_stats.daily_turnover_trend:.1%}>{self.config.max_turnover_trend_daily:.0%})")

            if (self.config.enable_turnover_penalty and
                    factor_stats.daily_turnover_volatility > self.config.max_turnover_vol_daily):
                failed_checks.append(
                    f"æ¢æ‰‹ç‡æ³¢åŠ¨ç‡ä¸èƒ½è¿‡é«˜({factor_stats.daily_turnover_volatility:.3%}>{self.config.max_turnover_vol_daily:.1%})")

            logger.debug(f"å› å­ {factor_stats.factor_name} æœªé€šè¿‡ç­›é€‰: {'; '.join(failed_checks)}")

        return all(all_conditions)
    
    def select_category_champions(self, qualified_factors: Dict[str, FactorRollingICStats]) -> Dict[str, List[str]]:
        """
        ç±»åˆ«å†…å† å†›é€‰æ‹©
        
        Args:
            qualified_factors: é€šè¿‡åŸºæœ¬ç­›é€‰çš„å› å­
             for :nä¸ªç±»åˆ«"
                ç±»å†…æ’åé€»è¾‘ï¼šæŒ‰æ¢æ‰‹ç‡åŠ æƒçš„å‘¨æœŸè¡°å‡æ€»icåˆ†æ•°
                æ¯ä¸ªç±»åˆ«åªè¦2ä¸ª
            
        Returns:
            Dict[category, List[factor_names]]: å„ç±»åˆ«çš„å† å†›å› å­
        """
        logger.info("å¼€å§‹ç±»åˆ«å†…å† å†›é€‰æ‹©...")
        
        category_champions = {}
        #æ³¨æ„ éå†çš„æ˜¯ç±»åˆ«ï¼ï¼Œè€Œä¸æ˜¯å› å­ï¼Œæ‰€ä»¥åŠ¡å¿…éœ€è¦ä¿è¯ç±»åˆ«åœ¨configé…ç½®æ–‡ä»¶ï¼
        for category, factor_list in self.factor_categories.items():
            # æ‰¾åˆ°è¯¥ç±»åˆ«ä¸­çš„åˆæ ¼å› å­
            category_factors = {
                name: stats for name, stats in qualified_factors.items() 
                if name in factor_list
            }
            
            if not category_factors:
                continue
            
            # æŒ‰æ¢æ‰‹ç‡è°ƒæ•´åè¯„åˆ†æ’åºï¼ˆå®ç›˜å¯¼å‘ä¼˜åŒ–ï¼‰
            sorted_factors = sorted(
                category_factors.items(), 
                key=lambda x: x[1].turnover_adjusted_score if self.config.enable_turnover_penalty else x[1].multi_period_score, 
                reverse=True
            )
            
            # é€‰æ‹©å‰Nå
            max_count = min(len(sorted_factors), self.config.max_factors_per_category)
            champions = [name for name, _ in sorted_factors[:max_count]]
            
            if champions:
                category_champions[category] = champions
                logger.info(f"{category}: {len(champions)} ä¸ªå† å†›")
                for name in champions:
                    stats = qualified_factors[name]
                    direction = "+" if  np.sign(stats.avg_ic_with_sign) > 0 else "-"
                    score_used = stats.turnover_adjusted_score if self.config.enable_turnover_penalty else stats.multi_period_score
                    logger.info(f"  {direction} {name}: è°ƒæ•´è¯„åˆ†={score_used:.1f} (æ—¥æ¢æ‰‹ç‡={stats.daily_rank_change_mean:.1%})")
        
        return category_champions
    
    def apply_correlation_control(
            self, 
            candidate_factors: List[str],
            qualified_factors: Dict[str, FactorRollingICStats]
    ) -> Tuple[List[str], Dict[str, Any]]:
        """
        åº”ç”¨ä¸‰å±‚ç›¸å…³æ€§æ§åˆ¶å“²å­¦ï¼ˆä¸¤é˜¶æ®µæ— é¡ºåºä¾èµ–ç‰ˆæœ¬ï¼‰
        
        ğŸ¯ æ ¸å¿ƒæ”¹è¿›ï¼šæ¶ˆé™¤é¡ºåºä¾èµ–æ€§ï¼Œç¡®ä¿ç»“æœå”¯ä¸€ç¡®å®š
        
        ğŸ“Š ä¸¤é˜¶æ®µæ¶æ„ï¼š
        é˜¶æ®µ1: ğŸš¨ çº¢è‰²åŒºåŸŸé›†ç¾¤æ¶ˆæ€ (|corr|>0.7) - æ¯ä¸ªé«˜ç›¸å…³é›†ç¾¤åªä¿ç•™æœ€å¼ºè€…
        é˜¶æ®µ2: âš ï¸ é»„è‰²åŒºåŸŸæ­£äº¤åŒ–å¤„ç† (0.3<|corr|<0.7) - åŸºäºå¹¸å­˜è€…ç”Ÿæˆæ­£äº¤åŒ–è®¡åˆ’
        
        Args:
            candidate_factors: å€™é€‰å› å­åˆ—è¡¨
            qualified_factors: åˆæ ¼å› å­ç»Ÿè®¡
            
        Returns:
            (final_factors, correlation_report)
        """
        logger.info("ğŸ” å¼€å§‹æ‰§è¡Œä¸‰å±‚ç›¸å…³æ€§æ§åˆ¶ï¼ˆæ— é¡ºåºä¾èµ–ç‰ˆæœ¬ï¼‰...")
        logger.info(f"ğŸ“Š è¾“å…¥å› å­æ•°é‡: {len(candidate_factors)}")
        
        # è®¡ç®—å› å­ç›¸å…³æ€§çŸ©é˜µ
        correlation_matrix = self._calculate_factor_correlations(candidate_factors)
        if correlation_matrix is None:
            logger.warning("âš ï¸ æ— æ³•è®¡ç®—ç›¸å…³æ€§çŸ©é˜µï¼Œè·³è¿‡ç›¸å…³æ€§æ§åˆ¶")
            return candidate_factors, {}
        
        # === é˜¶æ®µ1ï¼šæ ¹æ®é…ç½®é€‰æ‹©èšç±»æ–¹æ³• ===
        if self.config.clustering_method == 'hierarchical':
            logger.info("ğŸ”¬ é˜¶æ®µ1ï¼šå±‚æ¬¡èšç±»æ•°æ®é©±åŠ¨åˆ†æ...")
            red_zone_survivors, red_zone_decisions = self._process_clusters_hierarchical(
                candidate_factors, correlation_matrix, qualified_factors
            )
        else:#todo å¯¹æ¯”çœ‹çœ‹ æ–°æ–¹æ³•ç»“æœä¸€è‡´ä¸
            logger.info("ğŸš¨ é˜¶æ®µ1ï¼šçº¢è‰²åŒºåŸŸé›†ç¾¤æ¶ˆæ€...")
            red_zone_survivors, red_zone_decisions = self._process_red_zone_clusters(
                candidate_factors, correlation_matrix, qualified_factors
            )
        
        logger.info(f"  ğŸ“ˆ é›†ç¾¤æ¶ˆæ€ç»“æœ: {len(candidate_factors)} â†’ {len(red_zone_survivors)}")
        
        # === é˜¶æ®µ2ï¼šé»„è‰²åŒºåŸŸæ­£äº¤åŒ–å¤„ç† ===
        logger.info("âš ï¸ é˜¶æ®µ2ï¼šé»„è‰²åŒºåŸŸæ­£äº¤åŒ–å¤„ç†...")
        final_factors, orthogonalization_plan, yellow_zone_decisions = self._process_yellow_zone_orthogonalization(
            red_zone_survivors, qualified_factors
        )
        
        logger.info(f"  ğŸ“Š æ­£äº¤åŒ–å¤„ç†ç»“æœ: {len(red_zone_survivors)} â†’ {len(final_factors)} + {len(orthogonalization_plan)} ä¸ªæ­£äº¤åŒ–è®¡åˆ’")
        
        # === åˆå¹¶å†³ç­–è®°å½• ===
        all_decisions = red_zone_decisions + yellow_zone_decisions
        
        # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
        correlation_report = {
            'algorithm_version': 'ä¸¤é˜¶æ®µæ— é¡ºåºä¾èµ–ç‰ˆæœ¬',
            'input_count': len(candidate_factors),
            'red_zone_survivors_count': len(red_zone_survivors), 
            'final_count': len(final_factors),
            'orthogonalized_count': len(orthogonalization_plan),
            'decisions': all_decisions,
            'orthogonalized_factors': orthogonalization_plan,
            'correlation_matrix': correlation_matrix.to_dict(),
            'thresholds': {
                'high_corr': self.config.high_corr_threshold,
                'medium_corr': self.config.medium_corr_threshold
            },
            'processing_stages': {
                'stage1_red_zone': {
                    'input_count': len(candidate_factors),
                    'output_count': len(red_zone_survivors),
                    'decisions_count': len(red_zone_decisions)
                },
                'stage2_yellow_zone': {
                    'input_count': len(red_zone_survivors),
                    'output_count': len(final_factors),
                    'orthogonalization_count': len(orthogonalization_plan),
                    'decisions_count': len(yellow_zone_decisions)
                }
            }
        }
        
        logger.info("ğŸ¯ ä¸‰å±‚ç›¸å…³æ€§æ§åˆ¶å®Œæˆ:")
        logger.info(f"  ğŸ“ˆ è¾“å…¥å› å­: {len(candidate_factors)}")
        logger.info(f"  ğŸ”¥ çº¢è‰²åŒºåŸŸå¹¸å­˜è€…: {len(red_zone_survivors)}")
        logger.info(f"  ğŸ† æœ€ç»ˆå› å­: {len(final_factors)}")
        logger.info(f"  ğŸ”„ æ­£äº¤åŒ–å› å­: {len(orthogonalization_plan)}")
        logger.info(f"  ğŸ“Š æ€»å†³ç­–è®°å½•: {len(all_decisions)}")
        
        return final_factors, correlation_report

    def _calculate_factor_correlations(self, factor_names: List[str]) -> Optional[pd.DataFrame]:
        """è®¡ç®—å› å­é—´ç›¸å…³æ€§çŸ©é˜µï¼ˆå‘é‡åŒ–é«˜æ•ˆç‰ˆï¼‰"""
        """è®¡ç®—å› å­é—´ç›¸å…³æ€§çŸ©é˜µï¼ˆå†…ç½®é…å¯¹å¯¹é½çš„æœ€ç»ˆç‰ˆï¼‰"""
        try:
            # Step 1: ä»…åŠ è½½æ‰€æœ‰å› å­æ•°æ®
            factor_data_dict = self._load_all_factor_data(factor_names)

            final_factor_names = list(factor_data_dict.keys())
            if len(final_factor_names) < 2:
                logger.warning("æœ‰æ•ˆå› å­ä¸è¶³ï¼Œè·³è¿‡ç›¸å…³æ€§è®¡ç®—")
                return None

            correlation_matrix = pd.DataFrame(index=final_factor_names, columns=final_factor_names, dtype=float)

            # Step 2: è®¡ç®—ç›¸å…³æ€§ (åœ¨å¾ªç¯å†…éƒ¨è¿›è¡Œé…å¯¹å¯¹é½)
            for i in range(len(final_factor_names)):
                for j in range(i, len(final_factor_names)):
                    factor1_name = final_factor_names[i]
                    factor2_name = final_factor_names[j]

                    if i == j:
                        correlation_matrix.loc[factor1_name, factor1_name] = 1.0
                        continue

                    data1 = factor_data_dict[factor1_name]
                    data2 = factor_data_dict[factor2_name]

                    # --- æ ¸å¿ƒæ”¹è¿›ï¼šåœ¨è¿™é‡Œè¿›è¡Œé…å¯¹å¯¹é½ ---
                    common_index = data1.index.intersection(data2.index)
                    common_columns = data1.columns.intersection(data2.columns)

                    aligned_data1 = data1.loc[common_index, common_columns]
                    aligned_data2 = data2.loc[common_index, common_columns]
                    # --- å¯¹é½ç»“æŸ ---

                    # ä½¿ç”¨å‘é‡åŒ–è®¡ç®—æˆªé¢ç›¸å…³æ€§æ—¶é—´åºåˆ—
                    time_corrs = aligned_data1.corrwith(aligned_data2, axis=1, method='spearman')

                    # æ£€æŸ¥æ¯æ—¥æœ‰æ•ˆæ ·æœ¬æ•° (è¿™ä¸€æ­¥ä¾ç„¶éå¸¸ä¸“ä¸šä¸”å¿…è¦)
                    valid_counts = aligned_data1.notna() & aligned_data2.notna()
                    valid_daily_counts = valid_counts.sum(axis=1)

                    valid_time_corrs = time_corrs[valid_daily_counts > 10]

                    if not valid_time_corrs.empty:
                        avg_corr = valid_time_corrs.mean()
                        correlation_matrix.loc[factor1_name, factor2_name] = avg_corr
                        correlation_matrix.loc[factor2_name, factor1_name] = avg_corr
                    else:
                        # å¦‚æœæ²¡æœ‰ä»»ä½•ä¸€å¤©æ»¡è¶³è®¡ç®—æ¡ä»¶ï¼Œåˆ™è®¤ä¸ºæ— ç›¸å…³æ€§
                        correlation_matrix.loc[factor1_name, factor2_name] = 0.0
                        correlation_matrix.loc[factor2_name, factor1_name] = 0.0

            return correlation_matrix.astype(float)

        except Exception as e:
            # åœ¨é¡¶å±‚å‡½æ•°æ•è·å¼‚å¸¸ï¼Œè€Œä¸æ˜¯åœ¨åŠ è½½å‡½æ•°ä¸­æŠ›å‡º
            raise ValueError(f"âŒ ç›¸å…³æ€§çŸ©é˜µè®¡ç®—å¤±è´¥: {e}")

    def _load_factor_data(self, factor_name: str) -> Optional[pd.DataFrame]:
        """åŠ è½½å•ä¸ªå› å­æ•°æ®ç”¨äºç›¸å…³æ€§è®¡ç®—"""
        try:
            # æ„å»ºæ•°æ®è·¯å¾„
            factor_dir = (self.main_work_path / self.pool_index / factor_name / 
                         'o2o' / self.version)
            
            # å¯»æ‰¾å¤„ç†åçš„å› å­æ–‡ä»¶
            processed_file = factor_dir / 'processed_factor.parquet'
            if processed_file.exists():
                return pd.read_parquet(processed_file)
            
            # å¤‡ç”¨ï¼šå¯»æ‰¾å…¶ä»–å¯èƒ½çš„æ•°æ®æ–‡ä»¶
            parquet_files = list(factor_dir.glob("*.parquet"))
            if parquet_files:
                return pd.read_parquet(parquet_files[0])
            
            raise ValueError(f"  æœªæ‰¾åˆ° {factor_name} çš„æ•°æ®æ–‡ä»¶")

        except Exception as e:
            raise ValueError(f"  åŠ è½½ {factor_name} æ•°æ®å¤±è´¥: {e}")

    def _select_best_factor(
            self, 
            competitors: List[str], 
            qualified_factors: Dict[str, FactorRollingICStats]
    ) -> str:
        """ä»ç«äº‰å› å­ä¸­é€‰æ‹©æœ€ä½³å› å­ï¼ˆç”¨äºçº¢è‰²è­¦æŠ¥åŒºåŸŸï¼‰"""
        
        # æŒ‰å¤šå‘¨æœŸç»¼åˆè¯„åˆ†æ’åº
        scored_competitors = []
        for factor in competitors:
            if factor in qualified_factors:
                score = qualified_factors[factor].multi_period_score
                scored_competitors.append((factor, score))
            else:
                # å¦‚æœæ²¡æœ‰ç»Ÿè®¡æ•°æ®ï¼Œç»™äºˆæœ€ä½è¯„åˆ†
                scored_competitors.append((factor, 0.0))
        
        # é€‰æ‹©è¯„åˆ†æœ€é«˜çš„å› å­
        scored_competitors.sort(key=lambda x: x[1], reverse=True)
        winner = scored_competitors[0][0]
        
        return winner
    
    def generate_final_selection(self, category_champions: Dict[str, List[str]], 
                                qualified_factors: Dict[str, FactorRollingICStats]) -> List[str]:
        """
        ç”Ÿæˆæœ€ç»ˆå› å­é€‰æ‹©
        ï¼ˆåªæ˜¯è¿‡æ»¤æ•°é‡çš„è¿‡æ»¤è€Œå·²ï¼‰ï¼Œé™åˆ¶æœ€å¤šå…«ä¸ª
        Args:
            category_champions: å„ç±»åˆ«å† å†›
            qualified_factors: åˆæ ¼å› å­ç»Ÿè®¡
            
        Returns:
            List[str]: æœ€ç»ˆé€‰æ‹©çš„å› å­åå•
        """
        logger.info("ç”Ÿæˆæœ€ç»ˆå› å­é€‰æ‹©...")
        
        # æ”¶é›†æ‰€æœ‰å† å†›
        all_champions = []
        for category, champions in category_champions.items():
            for champion in champions:
                if champion in qualified_factors:
                    all_champions.append((champion, qualified_factors[champion]))
        
        # æŒ‰å¤šå‘¨æœŸè¯„åˆ†æ’åº
        all_champions.sort(key=lambda x: x[1].multi_period_score, reverse=True)
        
        # é€‰æ‹©å‰Nå
        max_selection = min(len(all_champions), self.config.max_final_factors)
        final_selection = [name for name, _ in all_champions[:max_selection]]
        
        logger.info(f"æœ€ç»ˆé€‰æ‹© {len(final_selection)} ä¸ªå› å­:")
        for i, (name, stats) in enumerate(all_champions[:max_selection], 1):
            direction = "+" if list(stats.periods_data.values())[0]['ic_mean_avg'] > 0 else "-"
            logger.info(f"{i}. {direction} {name}")
            logger.info(f"   è¯„åˆ†: {stats.multi_period_score:.1f}")
            logger.info(f"   IC: {stats.avg_ic_abs:.3f}, IR: {stats.avg_ir_abs:.2f}")
            logger.info(f"   ç¨³å®šæ€§: {stats.avg_stability:.1%}")
            logger.info(f"   æ—¶é—´è·¨åº¦: {stats.time_range[0]} ~ {stats.time_range[1]}")
        
        return final_selection
    
    def run_complete_selection(self, factor_names: List[str], force_generate: bool = False) -> Tuple[List[str], Dict[str, Any]]:
        """
        è¿è¡Œå®Œæ•´çš„å› å­ç­›é€‰æµç¨‹
        
        Args:
            factor_names: å€™é€‰å› å­åˆ—è¡¨
            force_generate: æ˜¯å¦å¼ºåˆ¶é‡æ–°ç”Ÿæˆæ»šåŠ¨IC
            
        Returns:
            Tuple[List[str], Dict]: (é€‰ä¸­å› å­åˆ—è¡¨, è¯¦ç»†æŠ¥å‘Š)
        """
        logger.info("=" * 60)
        logger.info("å¼€å§‹åŸºäºæ»šåŠ¨ICçš„å®Œæ•´å› å­ç­›é€‰")
        logger.info("=" * 60)
        
        # ç¬¬ä¸€æ­¥ï¼šåŸºäºæ»šåŠ¨ICç­›é€‰
        qualified_factors = self.screen_factors_by_rolling_ic(factor_names, force_generate)
        
        if not qualified_factors:
            logger.warning("è­¦å‘Šï¼šæ²¡æœ‰å› å­é€šè¿‡æ»šåŠ¨ICç­›é€‰")
            return [], {}
        
        # ç¬¬äºŒæ­¥ï¼šç±»åˆ«å†…é€‰æ‹©
        category_champions = self.select_category_champions(qualified_factors)
        
        if not category_champions:
            logger.warning("è­¦å‘Šï¼šæ²¡æœ‰ç±»åˆ«å† å†›")
            return [], {}
        
        # ç¬¬ä¸‰æ­¥ï¼šåˆæ­¥æœ€ç»ˆé€‰æ‹© ï¼ˆåªæ˜¯è¿‡æ»¤æ•°é‡çš„è¿‡æ»¤è€Œå·²ï¼‰ï¼Œé™åˆ¶æœ€å¤šå…«ä¸ª
        preliminary_selection = self.generate_final_selection(category_champions, qualified_factors)
        
        # ç¬¬å››æ­¥ï¼šä¸‰å±‚ç›¸å…³æ€§æ§åˆ¶å“²å­¦
        final_selection, correlation_report = self.apply_correlation_control( #debug here
            preliminary_selection, qualified_factors
        )
        
        # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
        report = self._generate_selection_report(
            factor_names, qualified_factors, category_champions, final_selection, correlation_report
        )
        
        logger.info("=" * 60)
        logger.info("æ»šåŠ¨ICå› å­ç­›é€‰å®Œæˆï¼")
        logger.info(f"æ¨èç”¨äºICåŠ æƒåˆæˆ: {final_selection}")
        logger.info("=" * 60)
        
        return final_selection, report
    
    def _generate_selection_report(self, candidate_factors: List[str], 
                                  qualified_factors: Dict[str, FactorRollingICStats],
                                  category_champions: Dict[str, List[str]], 
                                  final_selection: List[str],
                                  correlation_report: Dict[str, Any] = None) -> Dict[str, Any]:
        """ç”Ÿæˆé€‰æ‹©æŠ¥å‘Š"""
        
        # ç»Ÿè®¡ä¿¡æ¯
        qualified_count = len(qualified_factors)
        champions_count = sum(len(champions) for champions in category_champions.values())
        final_count = len(final_selection)
        
        # è¯„åˆ†ç»Ÿè®¡
        if qualified_factors:
            scores = [stats.multi_period_score for stats in qualified_factors.values()]
            avg_score = np.mean(scores)
            max_score = np.max(scores)
            min_score = np.min(scores)
        else:
            avg_score = max_score = min_score = 0.0
        
        # ç±»åˆ«åˆ†å¸ƒ
        category_distribution = {}
        for factor in final_selection:
            for category, factor_list in self.factor_categories.items():
                if factor in factor_list:
                    category_distribution[category] = category_distribution.get(category, 0) + 1
                    break
        
        # æ„å»ºæŠ¥å‘Š
        report = {
            'selection_config': {
                'snap_config_id': self.snap_config_id,
                'pool_index': self.pool_index,
                'time_range': f"{self.start_date} - {self.end_date}",
                'selection_criteria': {
                    'min_ic_abs_mean': self.config.min_ic_abs_mean,
                    'min_ir_abs_mean': self.config.min_ir_abs_mean,
                    'min_ic_stability': self.config.min_ic_stability,
                    'decay_rate': self.config.decay_rate
                }
            },
            'selection_summary': {
                'candidate_count': len(candidate_factors),
                'qualified_count': qualified_count,
                'champions_count': champions_count,
                'final_count': final_count,
                'pass_rate': qualified_count / len(candidate_factors) if candidate_factors else 0.0
            },
            'score_statistics': {
                'avg_score': avg_score,
                'max_score': max_score,
                'min_score': min_score
            },
            'category_distribution': category_distribution,
            'final_selection': final_selection,
            'factor_details': {
                factor: {
                    'multi_period_score': qualified_factors[factor].multi_period_score,
                    'avg_ic_abs': qualified_factors[factor].avg_ic_abs,
                    'avg_ir_abs': qualified_factors[factor].avg_ir_abs,
                    'avg_stability': qualified_factors[factor].avg_stability,
                    'snapshot_count': qualified_factors[factor].snapshot_count,
                    'time_range': qualified_factors[factor].time_range
                }
                for factor in final_selection if factor in qualified_factors
            }
        }
        
        # æ·»åŠ ç›¸å…³æ€§æ§åˆ¶æŠ¥å‘Š
        if correlation_report:
            report['correlation_control'] = {
                'enabled': True,
                'philosophy': 'ä¸‰å±‚ç›¸å…³æ€§æ§åˆ¶å“²å­¦',
                'thresholds': correlation_report.get('thresholds', {}),
                'processing_summary': {
                    'input_factors': correlation_report.get('input_count', 0),
                    'final_factors': correlation_report.get('final_count', 0),
                    'orthogonalized_factors': correlation_report.get('orthogonalized_count', 0),
                    'total_decisions': len(correlation_report.get('decisions', []))
                },
                'decisions_breakdown': self._summarize_correlation_decisions(correlation_report.get('decisions', [])),
                'orthogonalized_factors': correlation_report.get('orthogonalized_factors', []),
                'detailed_decisions': correlation_report.get('decisions', [])
            }
        else:
            report['correlation_control'] = {
                'enabled': False,
                'reason': 'ç›¸å…³æ€§æ§åˆ¶è·³è¿‡æˆ–å¤±è´¥'
            }
        
        return report
    
    def _summarize_correlation_decisions(self, decisions: List[Dict]) -> Dict[str, int]:
        """æ±‡æ€»ç›¸å…³æ€§å†³ç­–ç»Ÿè®¡"""
        summary = {
            'çº¢è‰²è­¦æŠ¥-äºŒé€‰ä¸€': 0,
            'é»„è‰²é¢„è­¦-æ­£äº¤åŒ–': 0,
            'ç»¿è‰²å®‰å…¨-ç›´æ¥ä¿ç•™': 0
        }
        
        for decision in decisions:
            decision_type = decision.get('decision', '')
            if decision_type in summary:
                summary[decision_type] += 1
        
        return summary

        # å‡½æ•°1: åªè´Ÿè´£åŠ è½½ï¼Œä¸å†è´Ÿè´£å¯¹é½

    def _load_all_factor_data(self, factor_names: List[str]) -> Dict[str, pd.DataFrame]:
        """ä»…åŠ è½½æ‰€æœ‰å› å­æ•°æ®åˆ°å­—å…¸ä¸­ï¼Œä¸è¿›è¡Œå¯¹é½"""
        factor_data_dict = {}
        for factor_name in factor_names:
            try:
                factor_data = self._load_factor_data(factor_name)
                if factor_data is not None and not factor_data.empty:
                    factor_data_dict[factor_name] = factor_data
                else:
                    logger.warning(f"  âš ï¸ {factor_name}: æ•°æ®åŠ è½½å¤±è´¥æˆ–ä¸ºç©º")
            except Exception as e:
                logger.warning(f"  âŒ {factor_name}: æ•°æ®åŠ è½½å¼‚å¸¸ - {e}")
                continue

        if len(factor_data_dict) < 2:
            raise ValueError("âš ï¸ æœ‰æ•ˆå› å­æ•°é‡ä¸è¶³ï¼Œæ— æ³•è®¡ç®—ç›¸å…³æ€§")

        return factor_data_dict
    def _process_red_zone_clusters(
            self, 
            candidate_factors: List[str], 
            correlation_matrix: pd.DataFrame,
            qualified_factors: Dict[str, FactorRollingICStats]
    ) -> Tuple[List[str], List[Dict]]:
        """
        é˜¶æ®µ1ï¼šçº¢è‰²åŒºåŸŸé›†ç¾¤æ¶ˆæ€ - å¤„ç†é«˜ç›¸å…³æ€§é›†ç¾¤
        
        ğŸ¯ æ ¸å¿ƒç®—æ³•ï¼š
        1. æ„å»ºé«˜ç›¸å…³å›¾ï¼ˆ|corr| > thresholdï¼‰
        2. ä½¿ç”¨å›¾ç®—æ³•æ‰¾å‡ºè¿é€šåˆ†é‡ï¼ˆé›†ç¾¤ï¼‰
        3. æ¯ä¸ªé›†ç¾¤å†…é€‰æ‹©è¯„åˆ†æœ€é«˜çš„å› å­ä½œä¸ºä»£è¡¨
        4. äº§å‡ºï¼šå¹¸å­˜è€…åˆ—è¡¨ + å†³ç­–è®°å½•
        
        Args:
            candidate_factors: å€™é€‰å› å­åˆ—è¡¨
            correlation_matrix: ç›¸å…³æ€§çŸ©é˜µ
            qualified_factors: å› å­è¯„åˆ†ç»Ÿè®¡
            
        Returns:
            (survivors, decisions): å¹¸å­˜è€…åˆ—è¡¨å’Œå†³ç­–è®°å½•
        """
        from collections import defaultdict
        
        # Step 1: æ„å»ºé«˜ç›¸å…³å›¾
        high_corr_graph = defaultdict(set)
        high_corr_pairs = []
        
        for i in range(len(candidate_factors)):
            for j in range(i + 1, len(candidate_factors)):
                factor1 = candidate_factors[i]
                factor2 = candidate_factors[j]
                corr = abs(correlation_matrix.loc[factor1, factor2])
                
                if corr >= self.config.high_corr_threshold:
                    high_corr_graph[factor1].add(factor2)
                    high_corr_graph[factor2].add(factor1)
                    high_corr_pairs.append((factor1, factor2, corr))
        
        # Step 2: ä½¿ç”¨DFSæ‰¾å‡ºè¿é€šåˆ†é‡ï¼ˆé«˜ç›¸å…³é›†ç¾¤ï¼‰
        def find_clusters():
            visited = set()
            clusters = []
            
            def dfs(node, current_cluster):#node:éœ€ç»™è¿™ä¸ªnodeæ‰¾å¸®å‡¶ï¼Œ éƒ½æ”¾åœ¨è¿™ä¸ªclusterä¸­
                if node in visited:
                    return
                visited.add(node)#æŸ“é»‘ï¼Œä¸‹æ¬¡è¿›æ¥å‘ç°ï¼å·²ç»è¢«å¤„ç†
                current_cluster.add(node)
                for neighbor in high_corr_graph[node]:#æ‰¾å‡ºä¸ä¹‹ç›¸å…³çš„ï¼ŒB C ï¼ŒBåˆå»æ‰¾ä¸Bç›¸å…³çš„xx ï¼Œï¼ˆç®€ç›´å°±æ˜¯è¿æ ¹æ‹”èµ·ï¼Œç„¶åæ”¾å…¥ä¸€ä¸ªé›†åˆï¼Œæœ€åå¯èƒ½å¤šä¸ªé›†åˆï¼Œæˆ‘ä»¬åªè¦æ¯ä¸ªé›†åˆçš„é«˜åˆ†é€‰æ‰‹ï¼
                    dfs(neighbor, current_cluster)
            
            for factor in candidate_factors:
                if factor not in visited:
                    cluster = set()
                    dfs(factor, cluster)
                    if len(cluster) > 1:  # åªå…³å¿ƒæœ‰ç›¸å…³æ€§çš„é›†ç¾¤
                        clusters.append(cluster)
                    elif len(cluster) == 1:  # å• ï¼ˆæ²¡æœ‰å¸®æ‰‹ï¼‰ é‚£ä¹ˆå¯ä»¥ç›´æ¥åŠ å…¥å¹¸å­˜è€…
                        pass
            
            return clusters
        
        clusters = find_clusters()
        
        # Step 3: æ¯ä¸ªé›†ç¾¤é€‰æ‹©ä»£è¡¨ï¼ˆè¯„åˆ†æœ€é«˜è€…ï¼‰
        survivors = []
        decisions = []
        processed_factors = set()
        
        # å¤„ç†é«˜ç›¸å…³é›†ç¾¤
        for i, cluster in enumerate(clusters):
            cluster_list = list(cluster)
            
            # é€‰æ‹©é›†ç¾¤å†…è¯„åˆ†æœ€é«˜çš„å› å­
            cluster_scores = []
            for factor in cluster_list:
                if factor in qualified_factors:
                    score = qualified_factors[factor].multi_period_score
                    cluster_scores.append((factor, score))
                else:
                    cluster_scores.append((factor, 0.0))
            
            # æŒ‰è¯„åˆ†æ’åºï¼Œé€‰æ‹©æœ€é«˜è€…
            cluster_scores.sort(key=lambda x: x[1], reverse=True)
            champion = cluster_scores[0][0] #é«˜ç›¸å…³é‡Œ æœ€å‰å®³çš„
            losers = [name for name, _ in cluster_scores[1:]]
            
            survivors.append(champion)
            processed_factors.update(cluster)
            
            # è®°å½•å†³ç­–
            for loser in losers:
                # æ‰¾å‡ºchampionå’Œloserçš„å…·ä½“ç›¸å…³ç³»æ•°
                loser_corr = abs(correlation_matrix.loc[champion, loser])
                decisions.append({
                    'stage': 'red_zone_cluster',
                    'cluster_id': i,
                    'cluster_size': len(cluster),
                    'champion': champion,
                    'loser': loser,
                    'correlation': loser_corr,
                    'decision': 'çº¢è‰²è­¦æŠ¥-é›†ç¾¤æ¶ˆæ€',
                    'reason': f'é«˜ç›¸å…³é›†ç¾¤å†…ç«äº‰(|corr|={loser_corr:.3f}>{self.config.high_corr_threshold})'
                })
            
            logger.info(f"  ğŸ”¥ é›†ç¾¤{i+1}: {len(cluster)}ä¸ªå› å­ â†’ é€‰æ‹© {champion}ï¼Œæ·˜æ±° {losers}")
        
        # Step 4: å¤„ç†æ— é«˜ç›¸å…³çš„ç‹¬ç«‹å› å­ï¼ˆç›´æ¥å¹¸å­˜ï¼‰
        independent_factors = [f for f in candidate_factors if f not in processed_factors]
        survivors.extend(independent_factors)
        
        for factor in independent_factors:
            logger.info(f"  âœ… ç‹¬ç«‹å› å­: {factor} ç›´æ¥å¹¸å­˜")
        
        logger.info(f"ğŸš¨ çº¢è‰²åŒºåŸŸå¤„ç†å®Œæˆ: å‘ç° {len(clusters)} ä¸ªé«˜ç›¸å…³é›†ç¾¤ï¼Œ{len(independent_factors)} ä¸ªç‹¬ç«‹å› å­")
        logger.info(f"   æœ€ç»ˆå¹¸å­˜è€…: {len(survivors)} ä¸ª")
        
        return survivors, decisions
    
    def _process_clusters_hierarchical(
        self,
        candidate_factors: List[str],
        correlation_matrix: pd.DataFrame,
        qualified_factors: Dict[str, FactorRollingICStats]
    ) -> Tuple[List[str], List[Dict]]:
        """
        é˜¶æ®µ1ï¼šä½¿ç”¨å±‚æ¬¡èšç±»è¿›è¡Œæ•°æ®é©±åŠ¨çš„é›†ç¾¤åˆ’åˆ†å’Œä»£è¡¨é€‰ä¸¾
        
        ğŸ¯ æ ¸å¿ƒä¼˜åŠ¿:
        1. å…¨å±€è§†è§’ï¼šåŒæ—¶è€ƒè™‘æ‰€æœ‰å› å­é—´çš„ç›¸å…³æ€§ç»“æ„
        2. æ•°æ®é©±åŠ¨ï¼šæ— éœ€äººå·¥è®¾å®šé˜ˆå€¼ï¼Œè‡ªåŠ¨å‘ç°æœ€ä¼˜ç°‡ç»“æ„
        3. å±‚æ¬¡ä¿¡æ¯ï¼šä¿ç•™å› å­é—´çš„å±‚æ¬¡ç›¸ä¼¼å…³ç³»
        4. ç¨³å¥æ€§ï¼šWardè¿æ¥æ–¹æ³•æœ€å°åŒ–ç°‡å†…æ–¹å·®ï¼Œç»“æœæ›´ç¨³å®š
        
        Args:
            candidate_factors: å€™é€‰å› å­åˆ—è¡¨
            correlation_matrix: ç›¸å…³æ€§çŸ©é˜µ
            qualified_factors: å› å­è¯„åˆ†ç»Ÿè®¡
            
        Returns:
            (survivors, decisions): å¹¸å­˜è€…åˆ—è¡¨å’Œå†³ç­–è®°å½•
        """
        if len(candidate_factors) < 2:
            logger.info("  âš ï¸ å€™é€‰å› å­ä¸è¶³2ä¸ªï¼Œè·³è¿‡å±‚æ¬¡èšç±»")
            return candidate_factors, []

        try:
            # Step 1: å°†ç›¸å…³æ€§çŸ©é˜µè½¬åŒ–ä¸ºè·ç¦»çŸ©é˜µ
            # è·ç¦» = 1 - |ç›¸å…³ç³»æ•°|ï¼Œè¿™æ ·å¼ºç›¸å…³ï¼ˆcorr=1ï¼‰çš„å› å­è·ç¦»ä¸º0
            abs_corr_matrix = abs(correlation_matrix)
            distance_matrix = 1 - abs_corr_matrix
            
            # ç¡®ä¿è·ç¦»çŸ©é˜µå¯¹è§’çº¿ä¸º0ï¼ˆè‡ªå·±ä¸è‡ªå·±çš„è·ç¦»ï¼‰
            np.fill_diagonal(distance_matrix.values, 0)
            
            # è½¬æ¢ä¸ºscipyå±‚æ¬¡èšç±»æ‰€éœ€çš„å‹ç¼©è·ç¦»å‘é‡
            condensed_distance = squareform(distance_matrix.values, force='tovector')
            
            # Step 2: æ‰§è¡Œå±‚æ¬¡èšç±»
            linkage_method = self.config.hierarchical_linkage_method
            logger.info(f"  ğŸ”¬ æ‰§è¡Œå±‚æ¬¡èšç±» (method={linkage_method})...")
            
            linkage_matrix = linkage(condensed_distance, method=linkage_method)
            
            # Step 3: æ ¹æ®é…ç½®å†³å®šç°‡åˆ’åˆ†ç­–ç•¥
            if self.config.max_clusters is not None:
                # ç­–ç•¥A: å›ºå®šç°‡æ•°é‡
                cluster_labels = fcluster(linkage_matrix, self.config.max_clusters, criterion='maxclust')
                logger.info(f"  ğŸ“Š å›ºå®šç°‡æ•°é‡ç­–ç•¥: {self.config.max_clusters} ä¸ªç°‡")
            else:
                # ç­–ç•¥B: è·ç¦»é˜ˆå€¼è‡ªé€‚åº”
                distance_threshold = self.config.hierarchical_distance_threshold
                cluster_labels = fcluster(linkage_matrix, distance_threshold, criterion='distance')
                logger.info(f"  ğŸ“Š è·ç¦»é˜ˆå€¼ç­–ç•¥: threshold={distance_threshold}")
            
            # Step 4: æ„å»ºç°‡ä¿¡æ¯
            clusters = {}
            for i, factor in enumerate(candidate_factors):
                cluster_id = cluster_labels[i]
                if cluster_id not in clusters:
                    clusters[cluster_id] = []
                clusters[cluster_id].append(factor)
            
            n_clusters = len(clusters)
            logger.info(f"  ğŸ¯ å‘ç° {n_clusters} ä¸ªå±‚æ¬¡ç°‡")
            
            # Step 5: æ¯ä¸ªç°‡é€‰æ‹©æœ€ä½³ä»£è¡¨å› å­
            survivors = []
            decisions = []
            
            for cluster_id, cluster_factors in clusters.items():
                cluster_size = len(cluster_factors)
                
                if cluster_size == 1:
                    # å•å› å­ç°‡ï¼šç›´æ¥ä¿ç•™
                    survivor = cluster_factors[0]
                    survivors.append(survivor)
                    logger.info(f"  ğŸ† ç°‡{cluster_id}: å•å› å­ {survivor} ç›´æ¥ä¿ç•™")
                    
                else:
                    # å¤šå› å­ç°‡ï¼šé€‰æ‹©æœ€ä½³ä»£è¡¨
                    champion = self._elect_best_factor_in_cluster(cluster_factors, qualified_factors)
                    losers = [f for f in cluster_factors if f != champion]
                    survivors.append(champion)
                    
                    # è®¡ç®—ç°‡å†…å¹³å‡ç›¸å…³æ€§ï¼ˆç”¨äºè®°å½•ï¼‰
                    cluster_correlations = []
                    for i in range(len(cluster_factors)):
                        for j in range(i+1, len(cluster_factors)):
                            factor1, factor2 = cluster_factors[i], cluster_factors[j]
                            corr = abs_corr_matrix.loc[factor1, factor2]
                            cluster_correlations.append(corr)
                    
                    avg_intra_cluster_corr = np.mean(cluster_correlations) if cluster_correlations else 0.0
                    
                    logger.info(f"  ğŸ† ç°‡{cluster_id}: {cluster_size}ä¸ªå› å­ â†’ é€‰æ‹© {champion}")
                    logger.info(f"      æ·˜æ±°: {losers}")
                    logger.info(f"      ç°‡å†…å¹³å‡ç›¸å…³æ€§: {avg_intra_cluster_corr:.3f}")
                    
                    # è®°å½•å†³ç­–
                    for loser in losers:
                        loser_corr = abs_corr_matrix.loc[champion, loser]
                        decisions.append({
                            'stage': 'hierarchical_clustering',
                            'cluster_id': cluster_id,
                            'cluster_size': cluster_size,
                            'champion': champion,
                            'loser': loser,
                            'correlation': loser_corr,
                            'avg_intra_cluster_corr': avg_intra_cluster_corr,
                            'decision': 'å±‚æ¬¡èšç±»-ç°‡å†…ç«é€‰',
                            'reason': f'å±‚æ¬¡èšç±»ç°‡å†…ç«äº‰(ç°‡{cluster_id},å¹³å‡|corr|={avg_intra_cluster_corr:.3f})',
                            'clustering_method': linkage_method,
                            'distance_threshold': self.config.hierarchical_distance_threshold
                        })
            
            # Step 6: ç”Ÿæˆèšç±»æ´å¯ŸæŠ¥å‘Š
            self._generate_clustering_insights(
                linkage_matrix, cluster_labels, candidate_factors, survivors, correlation_matrix
            )
            
            logger.info(f"ğŸ”¬ å±‚æ¬¡èšç±»å®Œæˆ:")
            logger.info(f"   è¾“å…¥å› å­: {len(candidate_factors)}")
            logger.info(f"   å‘ç°ç°‡æ•°: {n_clusters}")
            logger.info(f"   é€‰å‡ºä»£è¡¨: {len(survivors)}")
            logger.info(f"   æ·˜æ±°å› å­: {len(candidate_factors) - len(survivors)}")
            
            return survivors, decisions
            
        except Exception as e:
            logger.error(f"âŒ å±‚æ¬¡èšç±»å¤±è´¥: {e}")
            logger.info("   å›é€€åˆ°å›¾ç®—æ³•æ–¹æ³•...")
            # å›é€€åˆ°åŸå§‹å›¾ç®—æ³•æ–¹æ³•
            return self._process_red_zone_clusters(candidate_factors, correlation_matrix, qualified_factors)
    
    def _elect_best_factor_in_cluster(
        self, 
        cluster_factors: List[str], 
        qualified_factors: Dict[str, FactorRollingICStats]
    ) -> str:
        """
        åœ¨ç°‡å†…é€‰ä¸¾æœ€ä½³ä»£è¡¨å› å­
        
        ç»¼åˆè¯„åˆ†æ ‡å‡†:
        1. å¤šå‘¨æœŸICè¯„åˆ† (60%æƒé‡)
        2. Newey-Westæ˜¾è‘—æ€§ (25%æƒé‡) 
        3. å› å­ç¨³å®šæ€§ (15%æƒé‡)
        """
        if len(cluster_factors) == 1:
            return cluster_factors[0]
        
        # è®¡ç®—æ¯ä¸ªå› å­çš„ç»¼åˆç«é€‰åˆ†æ•°
        candidates_scores = []
        
        for factor in cluster_factors:
            if factor in qualified_factors:
                stats = qualified_factors[factor]
                
                # 1. ICè¯„åˆ† (å½’ä¸€åŒ–åˆ°0-1)
                ic_score = min(stats.multi_period_score / 100.0, 1.0)
                
                # 2. æ˜¾è‘—æ€§è¯„åˆ† (åŸºäºNewey-West tç»Ÿè®¡é‡)
                nw_significance_score = min(abs(stats.nw_t_stat_series_mean) / 3.0, 1.0)
                
                # 3. ç¨³å®šæ€§è¯„åˆ†
                stability_score = stats.avg_stability
                
                # ç»¼åˆè¯„åˆ†
                comprehensive_score = (
                    ic_score * 0.60 + 
                    nw_significance_score * 0.25 + 
                    stability_score * 0.15
                )
                
                candidates_scores.append((factor, comprehensive_score, {
                    'ic_score': ic_score,
                    'nw_significance': nw_significance_score, 
                    'stability': stability_score
                }))
            else:
                # æ²¡æœ‰ç»Ÿè®¡æ•°æ®çš„å› å­ç»™äºˆæœ€ä½åˆ†
                candidates_scores.append((factor, 0.0, {}))
        
        # æŒ‰ç»¼åˆåˆ†æ•°æ’åºï¼Œé€‰æ‹©æœ€é«˜åˆ†
        candidates_scores.sort(key=lambda x: x[1], reverse=True)
        
        champion = candidates_scores[0][0]
        champion_score = candidates_scores[0][1]
        
        logger.debug(f"      ç°‡å†…ç«é€‰ç»“æœ: {champion} (ç»¼åˆåˆ†æ•°: {champion_score:.3f})")
        
        return champion
    
    def _generate_clustering_insights(
        self, 
        linkage_matrix: np.ndarray,
        cluster_labels: np.ndarray, 
        factor_names: List[str],
        survivors: List[str],
        correlation_matrix: pd.DataFrame
    ) -> None:
        """
        ç”Ÿæˆå±‚æ¬¡èšç±»æ´å¯ŸæŠ¥å‘Š (å¯é€‰å¯è§†åŒ–)
        """
        try:
            # 1. ç°‡é—´è·ç¦»åˆ†æ
            n_clusters = len(set(cluster_labels))
            
            # 2. å› å­ä¿ç•™ç‡åˆ†æ
            retention_rate = len(survivors) / len(factor_names) if factor_names else 0
            
            # 3. å¹³å‡ç°‡å†…ç›¸å…³æ€§
            clusters = {}
            for i, factor in enumerate(factor_names):
                cluster_id = cluster_labels[i]
                if cluster_id not in clusters:
                    clusters[cluster_id] = []
                clusters[cluster_id].append(factor)
            
            cluster_internal_correlations = []
            for cluster_factors in clusters.values():
                if len(cluster_factors) > 1:
                    cluster_corrs = []
                    for i in range(len(cluster_factors)):
                        for j in range(i+1, len(cluster_factors)):
                            corr = abs(correlation_matrix.loc[cluster_factors[i], cluster_factors[j]])
                            cluster_corrs.append(corr)
                    if cluster_corrs:
                        cluster_internal_correlations.append(np.mean(cluster_corrs))
            
            avg_intra_cluster_corr = np.mean(cluster_internal_correlations) if cluster_internal_correlations else 0
            
            logger.info(f"  ğŸ“ˆ èšç±»æ´å¯Ÿ:")
            logger.info(f"     å› å­ä¿ç•™ç‡: {retention_rate:.1%}")
            logger.info(f"     å¹³å‡ç°‡å†…ç›¸å…³æ€§: {avg_intra_cluster_corr:.3f}")
            logger.info(f"     å¤šå› å­ç°‡æ•°é‡: {len(cluster_internal_correlations)}")
            
            # å¯é€‰ï¼šä¿å­˜æ ‘çŠ¶å›¾ (åœ¨ç ”ç©¶ç¯å¢ƒä¸­å¾ˆæœ‰ç”¨)
            # self._save_dendrogram(linkage_matrix, factor_names)
            
        except Exception as e:
            logger.debug(f"èšç±»æ´å¯Ÿç”Ÿæˆå¤±è´¥: {e}")
    
    def _save_dendrogram(self, linkage_matrix: np.ndarray, factor_names: List[str]) -> None:
        """ä¿å­˜å±‚æ¬¡èšç±»æ ‘çŠ¶å›¾ (å¯é€‰åŠŸèƒ½)"""
        try:
            plt.figure(figsize=(15, 8))
            dendrogram(
                linkage_matrix,
                labels=factor_names,
                orientation='top',
                distance_sort='descending',
                show_leaf_counts=True
            )
            plt.title('Factor Hierarchical Clustering Dendrogram')
            plt.xlabel('Factors')
            plt.ylabel('Distance')
            plt.xticks(rotation=45, ha='right')
            plt.tight_layout()
            
            # ä¿å­˜åˆ°å·¥ä½œç›®å½•
            output_path = self.main_work_path / f"dendrogram_{self.snap_config_id}.png"
            plt.savefig(output_path, dpi=300, bbox_inches='tight')
            plt.close()
            
            logger.info(f"  ğŸ“Š æ ‘çŠ¶å›¾å·²ä¿å­˜: {output_path}")
            
        except Exception as e:
            logger.debug(f"æ ‘çŠ¶å›¾ä¿å­˜å¤±è´¥: {e}")
            plt.close()

    def _process_yellow_zone_orthogonalization(
            self, 
            red_zone_survivors: List[str],
            qualified_factors: Dict[str, FactorRollingICStats]
    ) -> Tuple[List[str], List[Dict], List[Dict]]:
        """
        é˜¶æ®µ2ï¼šé»„è‰²åŒºåŸŸæ­£äº¤åŒ–å¤„ç† - åŸºäºå¹¸å­˜è€…å¤„ç†ä¸­åº¦ç›¸å…³æ€§
        
        ğŸ¯ æ ¸å¿ƒé€»è¾‘ï¼š
        1. åŸºäºçº¢è‰²åŒºåŸŸå¹¸å­˜è€…é‡æ–°è®¡ç®—ç›¸å…³æ€§
        2. æ‰¾å‡ºæ‰€æœ‰ä¸­åº¦ç›¸å…³å¯¹ (0.3 < |corr| < 0.7)
        3. ç”Ÿæˆæ­£äº¤åŒ–æ”¹é€ è®¡åˆ’ï¼ˆä¸ç›´æ¥ä¿®æ”¹å› å­åˆ—è¡¨ï¼‰
        4. äº§å‡ºï¼šæœ€ç»ˆå› å­åˆ—è¡¨ + æ­£äº¤åŒ–è®¡åˆ’ + å†³ç­–è®°å½•
        
        Args:
            red_zone_survivors: çº¢è‰²åŒºåŸŸå¹¸å­˜è€…
            qualified_factors: å› å­è¯„åˆ†ç»Ÿè®¡
            
        Returns:
            (final_factors, orthogonalization_plan, decisions)
        """
        # Step 1: åŸºäºå¹¸å­˜è€…é‡æ–°è®¡ç®—ç›¸å…³æ€§
        if len(red_zone_survivors) < 2:
            logger.info("  âš ï¸ å¹¸å­˜è€…ä¸è¶³2ä¸ªï¼Œè·³è¿‡é»„è‰²åŒºåŸŸå¤„ç†")
            return red_zone_survivors, [], []
        
        try:
            survivors_correlation_matrix = self._calculate_factor_correlations(red_zone_survivors)
            if survivors_correlation_matrix is None:
                raise ValueError("  âš ï¸ æ— æ³•è®¡ç®—å¹¸å­˜è€…ç›¸å…³æ€§çŸ©é˜µï¼Œè·³è¿‡æ­£äº¤åŒ–å¤„ç†")
                # return red_zone_survivors, [], []
        except Exception as e:
            raise ValueError(f"  âš ï¸ å¹¸å­˜è€…ç›¸å…³æ€§è®¡ç®—å¤±è´¥: {e}ï¼Œè·³è¿‡æ­£äº¤åŒ–å¤„ç†")
            # return red_zone_survivors, [], []
        
        # Step 2: æ‰¾å‡ºä¸­åº¦ç›¸å…³å¯¹
        medium_corr_pairs = []
        for i in range(len(red_zone_survivors)):
            for j in range(i + 1, len(red_zone_survivors)):
                factor1 = red_zone_survivors[i]
                factor2 = red_zone_survivors[j]
                corr = abs(survivors_correlation_matrix.loc[factor1, factor2])
                
                if self.config.medium_corr_threshold <= corr < self.config.high_corr_threshold:
                    medium_corr_pairs.append((factor1, factor2, corr))
        
        logger.info(f"  ğŸ“Š å‘ç° {len(medium_corr_pairs)} å¯¹ä¸­åº¦ç›¸å…³å› å­")
        
        # Step 3: ç”Ÿæˆæ­£äº¤åŒ–è®¡åˆ’
        orthogonalization_plan = []
        decisions = []
        final_factors = red_zone_survivors.copy()  # å…ˆä¿ç•™æ‰€æœ‰å¹¸å­˜è€…
        
        if not self.config.enable_orthogonalization:
            logger.info("  âš ï¸ æ­£äº¤åŒ–åŠŸèƒ½å·²ç¦ç”¨ï¼Œæ‰€æœ‰å¹¸å­˜è€…ç›´æ¥ä¿ç•™")
            return final_factors, [], []
        
        # æŒ‰ç›¸å…³æ€§ä»é«˜åˆ°ä½å¤„ç†
        medium_corr_pairs.sort(key=lambda x: x[2], reverse=True)
        
        for factor1, factor2, corr in medium_corr_pairs:
            # é€‰æ‹©è¯„åˆ†æ›´é«˜çš„ä½œä¸ºåŸºå‡†
            score1 = qualified_factors[factor1].multi_period_score if factor1 in qualified_factors else 0.0
            score2 = qualified_factors[factor2].multi_period_score if factor2 in qualified_factors else 0.0
            
            if score1 >= score2:
                base_factor, target_factor = factor1, factor2
            else:
                base_factor, target_factor = factor2, factor1
            
            # ç”Ÿæˆæ­£äº¤åŒ–è®¡åˆ’
            orthogonal_name = f"{target_factor}_orth_vs_{base_factor}"#base é«˜åˆ†ï¼
            
            orthogonalization_plan.append({
                'original_factor': target_factor,
                'base_factor': base_factor,
                'orthogonal_name': orthogonal_name,
                'correlation': corr,
                'base_score': qualified_factors[base_factor].multi_period_score if base_factor in qualified_factors else 0.0,
                'target_score': qualified_factors[target_factor].multi_period_score if target_factor in qualified_factors else 0.0
            })
            
            # è®°å½•å†³ç­–
            decisions.append({
                'stage': 'yellow_zone_orthogonalization',
                'base_factor': base_factor,
                'target_factor': target_factor,
                'orthogonal_name': orthogonal_name,
                'correlation': corr,
                'decision': 'é»„è‰²é¢„è­¦-æ­£äº¤åŒ–',
                'reason': f'ä¸­åº¦ç›¸å…³({self.config.medium_corr_threshold}<=|corr|={corr:.3f}<{self.config.high_corr_threshold})'
            })
            
            logger.info(f"  ğŸ”„ æ­£äº¤åŒ–è®¡åˆ’: {target_factor} â†’ {orthogonal_name} (åŸºäº {base_factor}ï¼Œç›¸å…³æ€§={corr:.3f})")
        
        # Step 4: æœ€ç»ˆæ£€æŸ¥ - ç¡®ä¿æ²¡æœ‰é«˜ç›¸å…³é—æ¼
        remaining_high_corr = []
        for i in range(len(final_factors)):
            for j in range(i + 1, len(final_factors)):
                factor1 = final_factors[i]
                factor2 = final_factors[j]
                corr = abs(survivors_correlation_matrix.loc[factor1, factor2])
                if corr >= self.config.high_corr_threshold:
                    remaining_high_corr.append((factor1, factor2, corr))
        
        if remaining_high_corr:
            raise ValueError(f"  âŒ ä¸¥é‡é—®é¢˜ï¼šæœ€ç»ˆå› å­ä¸­ä»å­˜åœ¨é«˜ç›¸å…³å› å­ {factor1} vs {factor2}: {corr:.3f}")
        logger.info(f"âš ï¸ é»„è‰²åŒºåŸŸå¤„ç†å®Œæˆ:")
        logger.info(f"   æœ€ç»ˆå› å­æ•°: {len(final_factors)}")
        logger.info(f"   æ­£äº¤åŒ–è®¡åˆ’: {len(orthogonalization_plan)} ä¸ª")
        logger.info(f"   å†³ç­–è®°å½•: {len(decisions)} æ¡")
        
        return final_factors, orthogonalization_plan, decisions