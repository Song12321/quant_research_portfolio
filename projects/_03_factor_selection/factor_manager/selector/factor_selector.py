import json
from scipy.cluster.hierarchy import linkage, fcluster, dendrogram
from scipy.spatial.distance import squareform
import matplotlib.pyplot as plt
from scipy.spatial.distance import squareform

from collections import defaultdict
from dataclasses import dataclass
from pathlib import Path
from typing import Any, List
from typing import Optional
from typing import Union, Dict, Tuple

import numpy as np
import pandas as pd

from projects._03_factor_selection.config_manager.base_config import INDEX_CODES, workspaces_result_dir
from projects._03_factor_selection.config_manager.config_snapshot.config_snapshot_manager import ConfigSnapshotManager
from projects._03_factor_selection.config_manager.function_load.load_config_file import _load_local_config_functional
from projects._03_factor_selection.factor_manager.storage.result_load_manager import ResultLoadManager
from projects._03_factor_selection.utils.factor_scoring_v33_final import calculate_factor_score_v33
from projects._03_factor_selection.visualization_manager import VisualizationManager
from quant_lib import logger


@dataclass
class FactorStats:
    factor_name: str
    ic_mean_21d: float
    ic_ir_21d: float
    detail_score_21d: float
    top_q_turnover_dict: dict
    # periods_data: Dict[str, Dict]  # å„å‘¨æœŸæ•°æ®
    # avg_ic_with_sign: float  # å¸¦ç¬¦å·
    # avg_ir_ir_with_sign: float
    # avg_ic_abs: float  # å¹³å‡ICç»å¯¹å€¼
    # avg_ir_abs: float  # å¹³å‡IRç»å¯¹å€¼
    # best_period_ic_ir: float  # iræ‰€åœ¨ è¡¨ç°æœ€ä½³çš„å‘¨æœŸval
    # nw_t_stat_series_mean: float
    # avg_stability: float  # å¹³å‡ç¨³å®šæ€§
    # avg_ic_volatility: float  # å¹³å‡ICæ³¢åŠ¨ç‡
    # detail_score_21d: float  # å¤šå‘¨æœŸç»¼åˆè¯„åˆ†
    # snapshot_count: int  # å¿«ç…§æ•°é‡
    # time_range: Tuple[str, str]  # æ—¶é—´èŒƒå›´
    #
    # # å®ç›˜äº¤æ˜“æˆæœ¬æ§åˆ¶
    # # avg_daily_rank_change: float = 0.0    # å¹³å‡æœˆåº¦æ¢æ‰‹ç‡
    # daily_rank_change_mean: float
    # daily_turnover_trend: float
    # daily_turnover_volatility: float
    # turnover_adjusted_score: float = 0.0  # æ¢æ‰‹ç‡è°ƒæ•´åè¯„åˆ†


@dataclass
class SelectionConfig:
    """æ»šåŠ¨ICç­›é€‰é…ç½®"""
    # åŸºæœ¬ç­›é€‰é—¨æ§›
    min_snapshots: int = 3  # æœ€å°‘å¿«ç…§æ•°é‡
    min_ic_abs_mean: float = 0.01  # æ»šåŠ¨ICå‡å€¼ç»å¯¹å€¼é—¨æ§›
    min_ir_abs_mean: float = 0.15  # æ»šåŠ¨IRå‡å€¼ç»å¯¹å€¼é—¨æ§›
    min_ic_stability: float = 0.4  # ICç¨³å®šæ€§é—¨æ§›ï¼ˆæ–¹å‘ä¸€è‡´æ€§ï¼‰
    max_ic_volatility: float = 0.05  # ICæ³¢åŠ¨ç‡ä¸Šé™

    # å¤šå‘¨æœŸæƒé‡é…ç½®
    decay_rate: float = 0.75  # è¡°å‡ç‡ï¼Œè¶Šå°æƒé‡è¡°å‡è¶Šæ…¢
    prefer_short_term: bool = True  # åå‘çŸ­æœŸ

    # ç±»åˆ«å†…é€‰æ‹©
    max_factors_per_category: int = 10  # æ¯ç±»æœ€å¤šå› å­æ•°
    min_category_score: float = 10.0  # ç±»åˆ«æœ€ä½è¯„åˆ†

    # æœ€ç»ˆç­›é€‰
    max_final_factors: int = 30  # æœ€å¤šé€‰æ‹©å› å­æ•°

    # ç›¸å…³æ€§æ§åˆ¶ï¼ˆä¸‰å±‚å†³ç­–å“²å­¦ï¼‰
    high_corr_threshold: float = 0.7  # é«˜ç›¸å…³é˜ˆå€¼ï¼ˆçº¢è‰²è­¦æŠ¥ï¼šäºŒé€‰ä¸€ï¼‰
    medium_corr_threshold: float = 0.3  # ä¸­ä½ç›¸å…³åˆ†ç•Œï¼ˆé»„è‰²é¢„è­¦ï¼šæ­£äº¤åŒ–æˆ˜åœºï¼‰
    enable_orthogonalization: bool = True  # æ˜¯å¦å¯ç”¨ä¸­ç›¸å…³åŒºé—´æ­£äº¤åŒ–

    # å±‚æ¬¡èšç±»é…ç½®
    clustering_method: str = 'graph'  # èšç±»æ–¹æ³•: 'graph'(å›¾ç®—æ³•) æˆ– 'hierarchical'(å±‚æ¬¡èšç±»)
    hierarchical_distance_threshold: float = 0.3  # å±‚æ¬¡èšç±»è·ç¦»é˜ˆå€¼
    hierarchical_linkage_method: str = 'ward'  # è¿æ¥æ–¹æ³•: 'ward', 'complete', 'average'
    max_clusters: int = None  # æœ€å¤§ç°‡æ•°é‡é™åˆ¶ (Noneè¡¨ç¤ºä½¿ç”¨è·ç¦»é˜ˆå€¼)

    # å®ç›˜äº¤æ˜“æˆæœ¬æ§åˆ¶ï¼ˆæ¢æ‰‹ç‡ä¸€ç­‰å…¬æ°‘ï¼‰
    max_turnover_rate: float = 0.15  # æœ€å¤§æ¢æ‰‹ç‡é˜ˆå€¼ï¼ˆæœˆåº¦ï¼‰
    turnover_weight: float = 0.25  # æ¢æ‰‹ç‡åœ¨ç»¼åˆè¯„åˆ†ä¸­çš„æƒé‡
    enable_turnover_penalty: bool = False  # æ˜¯å¦å¯ç”¨æ¢æ‰‹ç‡æƒ©ç½š todo åç»­åœ¨è¡¥å……

    # 1. åŸºç¡€ä¹˜æ•°ç›¸å…³é…ç½®
    reward_turnover_rate_daily: float = 0.0025
    max_turnover_rate_daily: float = 0.007
    penalty_slope_daily: float = 45.0
    heavy_penalty_slope_daily: float = 100.0
    base_turnover_multiplier_floor: float = 0.1  # ã€æ–°å¢ã€‘åŸºç¡€ä¹˜æ•°çš„æœ€ä½å€¼ï¼Œé˜²æ­¢å˜ä¸ºè´Ÿæ•°

    # 2. æ³¢åŠ¨ç‡æƒ©ç½šç›¸å…³é…ç½®
    turnover_vol_threshold_ratio: float = 0.5
    turnover_vol_penalty_factor: float = 0.2

    # 3. è¶‹åŠ¿æƒ©ç½šç›¸å…³é…ç½®
    turnover_trend_sensitivity: float = 50.0  # ã€æ–°å¢ã€‘è¶‹åŠ¿æƒ©ç½šæ•æ„Ÿåº¦, å–ä»£äº†æ—§çš„*100

    # 4. æœ€ç»ˆä¹˜æ•°èŒƒå›´æ§åˆ¶
    final_multiplier_min: float = 0.1  # ã€æ–°å¢ã€‘æœ€ç»ˆä¹˜æ•°ä¸‹é™
    final_multiplier_max: float = 1.2  # ã€æ–°å¢ã€‘æœ€ç»ˆä¹˜æ•°ä¸Šé™
    # ç”¨äºç¡¬æ€§æ·˜æ±°çš„æœ€ç»ˆé˜²çº¿ (Final Gatekeeper Thresholds)
    max_turnover_mean_daily: float = 0.15  # ç¡¬é—¨æ§›ï¼šæ—¥å‡æ¢æ‰‹ç‡ä¸å¾—è¶…è¿‡1% (çº¦ç­‰äºæœˆåº¦21%)
    max_turnover_trend_daily: float = 0.00005  # ç¡¬é—¨æ§›ï¼šæ¢æ‰‹ç‡æ¯æ—¥æ¶åŒ–è¶‹åŠ¿ä¸å¾—è¶…è¿‡0.002%
    max_turnover_vol_daily: float = 0.015  # ç¡¬é—¨æ§›ï¼šæ¢æ‰‹ç‡æ³¢åŠ¨ç‡ä¸å¾—è¶…è¿‡1.5%

class FactorSelector:
    def __init__(self,snapshot_config_id,config: SelectionConfig):
        self.snap_config_id = snapshot_config_id
        manager = ConfigSnapshotManager()
        pool_index, s, e, config_evaluation = manager.get_snapshot_config_content_details(snapshot_config_id)
        version = f'{s}_{e}'
        self.start_date = s
        self.end_date = e
        self.pool_index = pool_index

        self.resultLoadManager = ResultLoadManager(pool_index = pool_index,s=s,e=e,version=version)
        self.config = config or SelectionConfig()
        # å‡è®¾å› å­æµ‹è¯•ä¸­æ‰€æœ‰å¯èƒ½ç”¨åˆ°çš„å‘¨æœŸéƒ½åœ¨è¿™é‡Œå®šä¹‰
        self.ALL_PERIODS = ['1d', '5d', '10d', '21d', '40d', '60d', '120d']
        self.visualization_manager = VisualizationManager(
        )
        self.factor_categories = self.build_factor_categorie_maps()

        # å‡½æ•°1: åªè´Ÿè´£åŠ è½½ï¼Œä¸å†è´Ÿè´£å¯¹é½

    def load_all_factor_data(self,factor_names: List[str]) -> Dict[str, pd.DataFrame]:
        """ä»…åŠ è½½æ‰€æœ‰å› å­æ•°æ®åˆ°å­—å…¸ä¸­ï¼Œä¸è¿›è¡Œå¯¹é½"""
        factor_data_dict = {}
        for factor_name in factor_names:
            try:
                factor_data = self._load_factor_data(factor_name)
                if factor_data is not None and not factor_data.empty:
                    factor_data_dict[factor_name] = factor_data
                else:
                    raise ValueError(f"  âš ï¸ {factor_name}: æ•°æ®åŠ è½½å¤±è´¥æˆ–ä¸ºç©º")
            except Exception as e:
                raise ValueError(f"  âŒ {factor_name}: æ•°æ®åŠ è½½å¼‚å¸¸ - {e}")
                continue

        if len(factor_data_dict) < 2:
            raise ValueError("âš ï¸ æœ‰æ•ˆå› å­æ•°é‡ä¸è¶³ï¼Œæ— æ³•è®¡ç®—ç›¸å…³æ€§")

        return factor_data_dict

    def _load_factor_data(self, factor_name: str) -> Optional[pd.DataFrame]:
        return self.resultLoadManager.get_factor_data(factor_name)
    def build_factor_icir_data(self,
                                   run_version: str = 'latest') :
        base_path = workspaces_result_dir / self.resultLoadManager.pool_index
        ret = {}
        for factor_dir in base_path.iterdir():
            if not factor_dir.is_dir(): continue
            one_period = {}
            factor_name = factor_dir.name
            for period in self.ALL_PERIODS:
                # 1. ä¸ºå½“å‰å› å­å’Œå‘¨æœŸæ„å»ºä¸€ä¸ªå®Œæ•´çš„æŒ‡æ ‡è¡Œ
                current_period_row = self._build_single_period_row(factor_dir, period, run_version)
                if current_period_row == None:
                    continue
                ic_ir = current_period_row['ic_ir_processed_o2o']
                ic_mean = current_period_row['ic_mean_processed_o2o']
                ic_t_stat = current_period_row['ic_t_stat_processed_o2o']
                one_period[period] = {'ic_mean':ic_mean,'ic_ir':ic_ir, 'ic_t_stat':ic_t_stat}
            if  len (one_period)!=0:
                ret[factor_name] = one_period
        return ret
    # æœ€æ–°ç‰ˆè¯„ä»·å› å­ï¼æŒ‘é€‰å› å­ï¼
    # å…¨å±€ic ir è¿›è¡Œè¯„ä»·ï¼
    def get_base_passed_factors(self
                                ):
        all_factors_summary_data = self.build_factor_icir_data()
        # --- 2. æ‰§è¡Œç­›é€‰ä¸ç”»åƒ ---
        elite_factor_reports = profile_elite_factors(
            all_factors_summary=all_factors_summary_data
        )
        names = []
        # --- 3. æŸ¥çœ‹ç²¾è‹±å› å­çš„æ·±åº¦ç”»åƒæŠ¥å‘Š ---
        for factor_name, report in elite_factor_reports.items():
            names.append(factor_name)
            print(f"\n----- {factor_name} ç²¾è‹±å› å­æŠ¥å‘Š -----")
            # ä½¿ç”¨jsonç¾åŒ–è¾“å‡º
            print(json.dumps(report, indent=4, ensure_ascii=False))
        print("\n" + "=" * 50)
        print(f"å› å­listï¼šï¼š{names}")
        return names

    def _process_red_zone_clusters(
            self,
            candidate_factors: List[str],
            correlation_matrix: pd.DataFrame,
            qualified_factors: Dict[str, FactorStats]
    ) -> Tuple[List[str], List[Dict]]:
        """
        é˜¶æ®µ1ï¼šçº¢è‰²åŒºåŸŸé›†ç¾¤æ¶ˆæ€ - å¤„ç†é«˜ç›¸å…³æ€§é›†ç¾¤

        ğŸ¯ æ ¸å¿ƒç®—æ³•ï¼š
        1. æ„å»ºé«˜ç›¸å…³å›¾ï¼ˆ|corr| > thresholdï¼‰
        2. ä½¿ç”¨å›¾ç®—æ³•æ‰¾å‡ºè¿é€šåˆ†é‡ï¼ˆé›†ç¾¤ï¼‰
        3. æ¯ä¸ªé›†ç¾¤å†…é€‰æ‹©è¯„åˆ†æœ€é«˜çš„å› å­ä½œä¸ºä»£è¡¨
        4. äº§å‡ºï¼šå¹¸å­˜è€…åˆ—è¡¨ + å†³ç­–è®°å½•

        Args:
            candidate_factors: å€™é€‰å› å­åˆ—è¡¨
            correlation_matrix: ç›¸å…³æ€§çŸ©é˜µ
            qualified_factors: å› å­è¯„åˆ†ç»Ÿè®¡

        Returns:
            (survivors, decisions): å¹¸å­˜è€…åˆ—è¡¨å’Œå†³ç­–è®°å½•
        """
        from collections import defaultdict

        # Step 1: æ„å»ºé«˜ç›¸å…³å›¾
        high_corr_graph = defaultdict(set)
        high_corr_pairs = []

        for i in range(len(candidate_factors)):
            for j in range(i + 1, len(candidate_factors)):
                factor1 = candidate_factors[i]
                factor2 = candidate_factors[j]
                corr = abs(correlation_matrix.loc[factor1, factor2])

                if corr >= self.config.high_corr_threshold:
                    high_corr_graph[factor1].add(factor2)
                    high_corr_graph[factor2].add(factor1)
                    high_corr_pairs.append((factor1, factor2, corr))

        # Step 2: ä½¿ç”¨DFSæ‰¾å‡ºè¿é€šåˆ†é‡ï¼ˆé«˜ç›¸å…³é›†ç¾¤ï¼‰
        def find_clusters():
            visited = set()
            clusters = []

            def dfs(node, current_cluster):  # node:éœ€ç»™è¿™ä¸ªnodeæ‰¾å¸®å‡¶ï¼Œ éƒ½æ”¾åœ¨è¿™ä¸ªclusterä¸­
                if node in visited:
                    return
                visited.add(node)  # æŸ“é»‘ï¼Œä¸‹æ¬¡è¿›æ¥å‘ç°ï¼å·²ç»è¢«å¤„ç†
                current_cluster.add(node)
                for neighbor in high_corr_graph[
                    node]:  # æ‰¾å‡ºä¸ä¹‹ç›¸å…³çš„ï¼ŒB C ï¼ŒBåˆå»æ‰¾ä¸Bç›¸å…³çš„xx ï¼Œï¼ˆç®€ç›´å°±æ˜¯è¿æ ¹æ‹”èµ·ï¼Œç„¶åæ”¾å…¥ä¸€ä¸ªé›†åˆï¼Œæœ€åå¯èƒ½å¤šä¸ªé›†åˆï¼Œæˆ‘ä»¬åªè¦æ¯ä¸ªé›†åˆçš„é«˜åˆ†é€‰æ‰‹ï¼
                    dfs(neighbor, current_cluster)

            for factor in candidate_factors:
                if factor not in visited:
                    cluster = set()
                    dfs(factor, cluster)
                    if len(cluster) > 1:  # åªå…³å¿ƒæœ‰ç›¸å…³æ€§çš„é›†ç¾¤
                        clusters.append(cluster)
                    elif len(cluster) == 1:  # å• ï¼ˆæ²¡æœ‰å¸®æ‰‹ï¼‰ é‚£ä¹ˆå¯ä»¥ç›´æ¥åŠ å…¥å¹¸å­˜è€…
                        pass

            return clusters

        clusters = find_clusters()

        # Step 3: æ¯ä¸ªé›†ç¾¤é€‰æ‹©ä»£è¡¨ï¼ˆè¯„åˆ†æœ€é«˜è€…ï¼‰
        survivors = []
        decisions = []
        processed_factors = set()

        # å¤„ç†é«˜ç›¸å…³é›†ç¾¤
        for i, cluster in enumerate(clusters):
            cluster_list = list(cluster)

            # é€‰æ‹©é›†ç¾¤å†…è¯„åˆ†æœ€é«˜çš„å› å­
            cluster_scores = []
            for factor in cluster_list:
                if factor in qualified_factors:
                    score = qualified_factors[factor].detail_score_21d['Final_Score']
                    cluster_scores.append((factor, score))
                else:
                    cluster_scores.append((factor, 0.0))

            # æŒ‰è¯„åˆ†æ’åºï¼Œé€‰æ‹©æœ€é«˜è€…
            cluster_scores.sort(key=lambda x: x[1], reverse=True)
            champion = cluster_scores[0][0]  # é«˜ç›¸å…³é‡Œ æœ€å‰å®³çš„
            losers = [name for name, _ in cluster_scores[1:]]

            survivors.append(champion)
            processed_factors.update(cluster)

            # è®°å½•å†³ç­–
            for loser in losers:
                # æ‰¾å‡ºchampionå’Œloserçš„å…·ä½“ç›¸å…³ç³»æ•°
                loser_corr = abs(correlation_matrix.loc[champion, loser])
                decisions.append({
                    'stage': 'red_zone_cluster',
                    'cluster_id': i,
                    'cluster_size': len(cluster),
                    'champion': champion,
                    'loser': loser,
                    'correlation': loser_corr,
                    'decision': 'çº¢è‰²è­¦æŠ¥-é›†ç¾¤æ¶ˆæ€',
                    'reason': f'é«˜ç›¸å…³é›†ç¾¤å†…ç«äº‰(|corr|={loser_corr:.3f}>{self.config.high_corr_threshold})'
                })

            logger.info(f"  ğŸ”¥ é›†ç¾¤{i + 1}: {len(cluster)}ä¸ªå› å­ â†’ é€‰æ‹© {champion}ï¼Œæ·˜æ±° {losers}")

        # Step 4: å¤„ç†æ— é«˜ç›¸å…³çš„ç‹¬ç«‹å› å­ï¼ˆç›´æ¥å¹¸å­˜ï¼‰
        independent_factors = [f for f in candidate_factors if f not in processed_factors]
        survivors.extend(independent_factors)

        for factor in independent_factors:
            logger.info(f"  âœ… ç‹¬ç«‹å› å­: {factor} ç›´æ¥å¹¸å­˜")

        logger.info(f"ğŸš¨ çº¢è‰²åŒºåŸŸå¤„ç†å®Œæˆ: å‘ç° {len(clusters)} ä¸ªé«˜ç›¸å…³é›†ç¾¤ï¼Œ{len(independent_factors)} ä¸ªç‹¬ç«‹å› å­")
        logger.info(f"   æœ€ç»ˆå¹¸å­˜è€…: {len(survivors)} ä¸ª")

        return survivors, decisions

    def _generate_clustering_insights(
            self,
            linkage_matrix: np.ndarray,
            cluster_labels: np.ndarray,
            factor_names: List[str],
            survivors: List[str],
            correlation_matrix: pd.DataFrame
    ) -> None:
        """
        ç”Ÿæˆå±‚æ¬¡èšç±»æ´å¯ŸæŠ¥å‘Š (å¯é€‰å¯è§†åŒ–)
        """
        try:
            # 1. ç°‡é—´è·ç¦»åˆ†æ
            n_clusters = len(set(cluster_labels))

            # 2. å› å­ä¿ç•™ç‡åˆ†æ
            retention_rate = len(survivors) / len(factor_names) if factor_names else 0

            # 3. å¹³å‡ç°‡å†…ç›¸å…³æ€§
            clusters = {}
            for i, factor in enumerate(factor_names):
                cluster_id = cluster_labels[i]
                if cluster_id not in clusters:
                    clusters[cluster_id] = []
                clusters[cluster_id].append(factor)

            cluster_internal_correlations = []
            for cluster_factors in clusters.values():
                if len(cluster_factors) > 1:
                    cluster_corrs = []
                    for i in range(len(cluster_factors)):
                        for j in range(i + 1, len(cluster_factors)):
                            corr = abs(correlation_matrix.loc[cluster_factors[i], cluster_factors[j]])
                            cluster_corrs.append(corr)
                    if cluster_corrs:
                        cluster_internal_correlations.append(np.mean(cluster_corrs))

            avg_intra_cluster_corr = np.mean(cluster_internal_correlations) if cluster_internal_correlations else 0

            logger.info(f"  ğŸ“ˆ èšç±»æ´å¯Ÿ:")
            logger.info(f"     å› å­ä¿ç•™ç‡: {retention_rate:.1%}")
            logger.info(f"     å¹³å‡ç°‡å†…ç›¸å…³æ€§: {avg_intra_cluster_corr:.3f}")
            logger.info(f"     å¤šå› å­ç°‡æ•°é‡: {len(cluster_internal_correlations)}")

            # å¯é€‰ï¼šä¿å­˜æ ‘çŠ¶å›¾ (åœ¨ç ”ç©¶ç¯å¢ƒä¸­å¾ˆæœ‰ç”¨)
            # self._save_dendrogram(linkage_matrix, factor_names)

        except Exception as e:
            logger.debug(f"èšç±»æ´å¯Ÿç”Ÿæˆå¤±è´¥: {e}")

    def _process_clusters_hierarchical(
            self,
            candidate_factors: List[str],
            correlation_matrix: pd.DataFrame,
            qualified_factors: Dict[str, FactorStats]
    ) -> Tuple[List[str], List[Dict]]:
        """
        é˜¶æ®µ1ï¼šä½¿ç”¨å±‚æ¬¡èšç±»è¿›è¡Œæ•°æ®é©±åŠ¨çš„é›†ç¾¤åˆ’åˆ†å’Œä»£è¡¨é€‰ä¸¾

        ğŸ¯ æ ¸å¿ƒä¼˜åŠ¿:
        1. å…¨å±€è§†è§’ï¼šåŒæ—¶è€ƒè™‘æ‰€æœ‰å› å­é—´çš„ç›¸å…³æ€§ç»“æ„
        2. æ•°æ®é©±åŠ¨ï¼šæ— éœ€äººå·¥è®¾å®šé˜ˆå€¼ï¼Œè‡ªåŠ¨å‘ç°æœ€ä¼˜ç°‡ç»“æ„
        3. å±‚æ¬¡ä¿¡æ¯ï¼šä¿ç•™å› å­é—´çš„å±‚æ¬¡ç›¸ä¼¼å…³ç³»
        4. ç¨³å¥æ€§ï¼šWardè¿æ¥æ–¹æ³•æœ€å°åŒ–ç°‡å†…æ–¹å·®ï¼Œç»“æœæ›´ç¨³å®š

        Args:
            candidate_factors: å€™é€‰å› å­åˆ—è¡¨
            correlation_matrix: ç›¸å…³æ€§çŸ©é˜µ
            qualified_factors: å› å­è¯„åˆ†ç»Ÿè®¡

        Returns:
            (survivors, decisions): å¹¸å­˜è€…åˆ—è¡¨å’Œå†³ç­–è®°å½•
        """
        if len(candidate_factors) < 2:
            logger.info("  âš ï¸ å€™é€‰å› å­ä¸è¶³2ä¸ªï¼Œè·³è¿‡å±‚æ¬¡èšç±»")
            return candidate_factors, []

        try:
            # Step 1: å°†ç›¸å…³æ€§çŸ©é˜µè½¬åŒ–ä¸ºè·ç¦»çŸ©é˜µ
            # è·ç¦» = 1 - |ç›¸å…³ç³»æ•°|ï¼Œè¿™æ ·å¼ºç›¸å…³ï¼ˆcorr=1ï¼‰çš„å› å­è·ç¦»ä¸º0
            abs_corr_matrix = abs(correlation_matrix)
            distance_matrix = 1 - abs_corr_matrix

            # ç¡®ä¿è·ç¦»çŸ©é˜µå¯¹è§’çº¿ä¸º0ï¼ˆè‡ªå·±ä¸è‡ªå·±çš„è·ç¦»ï¼‰
            np.fill_diagonal(distance_matrix.values, 0)

            # è½¬æ¢ä¸ºscipyå±‚æ¬¡èšç±»æ‰€éœ€çš„å‹ç¼©è·ç¦»å‘é‡
            condensed_distance = squareform(distance_matrix.values, force='tovector')

            # Step 2: æ‰§è¡Œå±‚æ¬¡èšç±»
            linkage_method = self.config.hierarchical_linkage_method
            logger.info(f"  ğŸ”¬ æ‰§è¡Œå±‚æ¬¡èšç±» (method={linkage_method})...")

            linkage_matrix = linkage(condensed_distance, method=linkage_method)

            # Step 3: æ ¹æ®é…ç½®å†³å®šç°‡åˆ’åˆ†ç­–ç•¥
            if self.config.max_clusters is not None:
                # ç­–ç•¥A: å›ºå®šç°‡æ•°é‡
                cluster_labels = fcluster(linkage_matrix, self.config.max_clusters, criterion='maxclust')
                logger.info(f"  ğŸ“Š å›ºå®šç°‡æ•°é‡ç­–ç•¥: {self.config.max_clusters} ä¸ªç°‡")
            else:
                # ç­–ç•¥B: è·ç¦»é˜ˆå€¼è‡ªé€‚åº”
                distance_threshold = self.config.hierarchical_distance_threshold
                cluster_labels = fcluster(linkage_matrix, distance_threshold, criterion='distance')
                logger.info(f"  ğŸ“Š è·ç¦»é˜ˆå€¼ç­–ç•¥: threshold={distance_threshold}")

            # Step 4: æ„å»ºç°‡ä¿¡æ¯
            clusters = {}
            for i, factor in enumerate(candidate_factors):
                cluster_id = cluster_labels[i]
                if cluster_id not in clusters:
                    clusters[cluster_id] = []
                clusters[cluster_id].append(factor)

            n_clusters = len(clusters)
            logger.info(f"  ğŸ¯ å‘ç° {n_clusters} ä¸ªå±‚æ¬¡ç°‡")

            # Step 5: æ¯ä¸ªç°‡é€‰æ‹©æœ€ä½³ä»£è¡¨å› å­
            survivors = []
            decisions = []

            for cluster_id, cluster_factors in clusters.items():
                cluster_size = len(cluster_factors)

                if cluster_size == 1:
                    # å•å› å­ç°‡ï¼šç›´æ¥ä¿ç•™
                    survivor = cluster_factors[0]
                    survivors.append(survivor)
                    logger.info(f"  ğŸ† ç°‡{cluster_id}: å•å› å­ {survivor} ç›´æ¥ä¿ç•™")

                else:
                    # å¤šå› å­ç°‡ï¼šé€‰æ‹©æœ€ä½³ä»£è¡¨
                    champion = self._elect_best_factor_in_cluster(cluster_factors, qualified_factors)
                    losers = [f for f in cluster_factors if f != champion]
                    survivors.append(champion)

                    # è®¡ç®—ç°‡å†…å¹³å‡ç›¸å…³æ€§ï¼ˆç”¨äºè®°å½•ï¼‰
                    cluster_correlations = []
                    for i in range(len(cluster_factors)):
                        for j in range(i + 1, len(cluster_factors)):
                            factor1, factor2 = cluster_factors[i], cluster_factors[j]
                            corr = abs_corr_matrix.loc[factor1, factor2]
                            cluster_correlations.append(corr)

                    avg_intra_cluster_corr = np.mean(cluster_correlations) if cluster_correlations else 0.0

                    logger.info(f"  ğŸ† ç°‡{cluster_id}: {cluster_size}ä¸ªå› å­ â†’ é€‰æ‹© {champion}")
                    logger.info(f"      æ·˜æ±°: {losers}")
                    logger.info(f"      ç°‡å†…å¹³å‡ç›¸å…³æ€§: {avg_intra_cluster_corr:.3f}")

                    # è®°å½•å†³ç­–
                    for loser in losers:
                        loser_corr = abs_corr_matrix.loc[champion, loser]
                        decisions.append({
                            'stage': 'hierarchical_clustering',
                            'cluster_id': cluster_id,
                            'cluster_size': cluster_size,
                            'champion': champion,
                            'loser': loser,
                            'correlation': loser_corr,
                            'avg_intra_cluster_corr': avg_intra_cluster_corr,
                            'decision': 'å±‚æ¬¡èšç±»-ç°‡å†…ç«é€‰',
                            'reason': f'å±‚æ¬¡èšç±»ç°‡å†…ç«äº‰(ç°‡{cluster_id},å¹³å‡|corr|={avg_intra_cluster_corr:.3f})',
                            'clustering_method': linkage_method,
                            'distance_threshold': self.config.hierarchical_distance_threshold
                        })

            # Step 6: ç”Ÿæˆèšç±»æ´å¯ŸæŠ¥å‘Š
            self._generate_clustering_insights(
                linkage_matrix, cluster_labels, candidate_factors, survivors, correlation_matrix
            )

            logger.info(f"ğŸ”¬ å±‚æ¬¡èšç±»å®Œæˆ:")
            logger.info(f"   è¾“å…¥å› å­: {len(candidate_factors)}")
            logger.info(f"   å‘ç°ç°‡æ•°: {n_clusters}")
            logger.info(f"   é€‰å‡ºä»£è¡¨: {len(survivors)}")
            logger.info(f"   æ·˜æ±°å› å­: {len(candidate_factors) - len(survivors)}")

            return survivors, decisions

        except Exception as e:
            logger.error(f"âŒ å±‚æ¬¡èšç±»å¤±è´¥: {e}")
            logger.info("   å›é€€åˆ°å›¾ç®—æ³•æ–¹æ³•...")
            # å›é€€åˆ°åŸå§‹å›¾ç®—æ³•æ–¹æ³•
            return self._process_red_zone_clusters(candidate_factors, correlation_matrix, qualified_factors)

    def apply_correlation_control(
            self,
            candidate_factors: List[str],
            qualified_factors: Dict[str, FactorStats]
    ) -> Tuple[List[str], Dict[str, Any]]:
        """
        åº”ç”¨ä¸‰å±‚ç›¸å…³æ€§æ§åˆ¶å“²å­¦ï¼ˆä¸¤é˜¶æ®µæ— é¡ºåºä¾èµ–ç‰ˆæœ¬ï¼‰

        ğŸ¯ æ ¸å¿ƒæ”¹è¿›ï¼šæ¶ˆé™¤é¡ºåºä¾èµ–æ€§ï¼Œç¡®ä¿ç»“æœå”¯ä¸€ç¡®å®š

        ğŸ“Š ä¸¤é˜¶æ®µæ¶æ„ï¼š
        é˜¶æ®µ1: ğŸš¨ çº¢è‰²åŒºåŸŸé›†ç¾¤æ¶ˆæ€ (|corr|>0.7) - æ¯ä¸ªé«˜ç›¸å…³é›†ç¾¤åªä¿ç•™æœ€å¼ºè€…
        é˜¶æ®µ2: âš ï¸ é»„è‰²åŒºåŸŸæ­£äº¤åŒ–å¤„ç† (0.3<|corr|<0.7) - åŸºäºå¹¸å­˜è€…ç”Ÿæˆæ­£äº¤åŒ–è®¡åˆ’

        Args:
            candidate_factors: å€™é€‰å› å­åˆ—è¡¨
            qualified_factors: åˆæ ¼å› å­ç»Ÿè®¡

        Returns:
            (final_factors, correlation_report)
        """
        logger.info("ğŸ” å¼€å§‹æ‰§è¡Œä¸‰å±‚ç›¸å…³æ€§æ§åˆ¶ï¼ˆæ— é¡ºåºä¾èµ–ç‰ˆæœ¬ï¼‰...")
        logger.info(f"ğŸ“Š è¾“å…¥å› å­æ•°é‡: {len(candidate_factors)}")

        # è®¡ç®—å› å­ç›¸å…³æ€§çŸ©é˜µ
        correlation_matrix = self._calculate_factor_correlations(candidate_factors)
        if correlation_matrix is None:
            logger.warning("âš ï¸ æ— æ³•è®¡ç®—ç›¸å…³æ€§çŸ©é˜µï¼Œè·³è¿‡ç›¸å…³æ€§æ§åˆ¶")
            return candidate_factors, {}

        # === é˜¶æ®µ1ï¼šæ ¹æ®é…ç½®é€‰æ‹©èšç±»æ–¹æ³• ===
        if self.config.clustering_method == 'hierarchical':
            logger.info("ğŸ”¬ é˜¶æ®µ1ï¼šå±‚æ¬¡èšç±»æ•°æ®é©±åŠ¨åˆ†æ...")
            red_zone_survivors, red_zone_decisions = self._process_clusters_hierarchical(
                candidate_factors, correlation_matrix, qualified_factors
            )
        else:  # todo å¯¹æ¯”çœ‹çœ‹ æ–°æ–¹æ³•ç»“æœä¸€è‡´ä¸
            logger.info("ğŸš¨ é˜¶æ®µ1ï¼šçº¢è‰²åŒºåŸŸé›†ç¾¤æ¶ˆæ€...")
            red_zone_survivors, red_zone_decisions = self._process_red_zone_clusters(
                candidate_factors, correlation_matrix, qualified_factors
            )

        logger.info(f"  ğŸ“ˆ é›†ç¾¤æ¶ˆæ€ç»“æœ: {len(candidate_factors)} â†’ {len(red_zone_survivors)}")

        # === é˜¶æ®µ2ï¼šé»„è‰²åŒºåŸŸæ­£äº¤åŒ–å¤„ç† ===
        logger.info("âš ï¸ é˜¶æ®µ2ï¼šé»„è‰²åŒºåŸŸæ­£äº¤åŒ–å¤„ç†...")
        final_factors, orthogonalization_plan, yellow_zone_decisions = self._process_yellow_zone_orthogonalization(
            red_zone_survivors, qualified_factors
        )

        logger.info(
            f"  ğŸ“Š æ­£äº¤åŒ–å¤„ç†ç»“æœ: {len(red_zone_survivors)} â†’ {len(final_factors)} + {len(orthogonalization_plan)} ä¸ªæ­£äº¤åŒ–è®¡åˆ’")

        # === åˆå¹¶å†³ç­–è®°å½• ===
        all_decisions = red_zone_decisions + yellow_zone_decisions

        # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
        correlation_report = {
            'algorithm_version': 'ä¸¤é˜¶æ®µæ— é¡ºåºä¾èµ–ç‰ˆæœ¬',
            'input_count': len(candidate_factors),
            'red_zone_survivors_count': len(red_zone_survivors),
            'final_count': len(final_factors),
            'orthogonalized_count': len(orthogonalization_plan),
            'decisions': all_decisions,
            'orthogonalized_factors': orthogonalization_plan,
            'correlation_matrix': correlation_matrix.to_dict(),
            'thresholds': {
                'high_corr': self.config.high_corr_threshold,
                'medium_corr': self.config.medium_corr_threshold
            },
            'processing_stages': {
                'stage1_red_zone': {
                    'input_count': len(candidate_factors),
                    'output_count': len(red_zone_survivors),
                    'decisions_count': len(red_zone_decisions)
                },
                'stage2_yellow_zone': {
                    'input_count': len(red_zone_survivors),
                    'output_count': len(final_factors),
                    'orthogonalization_count': len(orthogonalization_plan),
                    'decisions_count': len(yellow_zone_decisions)
                }
            }
        }

        logger.info("ğŸ¯ ä¸‰å±‚ç›¸å…³æ€§æ§åˆ¶å®Œæˆ:")
        logger.info(f"  ğŸ“ˆ è¾“å…¥å› å­: {len(candidate_factors)}")
        logger.info(f"  ğŸ”¥ çº¢è‰²åŒºåŸŸå¹¸å­˜è€…: {len(red_zone_survivors)}")
        logger.info(f"  ğŸ† æœ€ç»ˆå› å­: {len(final_factors)}")
        logger.info(f"  ğŸ”„ æ­£äº¤åŒ–å› å­: {len(orthogonalization_plan)}")
        logger.info(f"  ğŸ“Š æ€»å†³ç­–è®°å½•: {len(all_decisions)}")

        return final_factors, correlation_report

    def _calculate_factor_correlations(self, factor_names: List[str]) -> Optional[pd.DataFrame]:
        """è®¡ç®—å› å­é—´ç›¸å…³æ€§çŸ©é˜µï¼ˆå‘é‡åŒ–é«˜æ•ˆç‰ˆï¼‰"""
        """è®¡ç®—å› å­é—´ç›¸å…³æ€§çŸ©é˜µï¼ˆå†…ç½®é…å¯¹å¯¹é½çš„æœ€ç»ˆç‰ˆï¼‰"""
        try:
            # Step 1: ä»…åŠ è½½æ‰€æœ‰å› å­æ•°æ®
            factor_data_dict = self.load_all_factor_data(factor_names)

            final_factor_names = list(factor_data_dict.keys())
            if len(final_factor_names) < 2:
                logger.warning("æœ‰æ•ˆå› å­ä¸è¶³ï¼Œè·³è¿‡ç›¸å…³æ€§è®¡ç®—")
                return None

            correlation_matrix = pd.DataFrame(index=final_factor_names, columns=final_factor_names, dtype=float)

            # Step 2: è®¡ç®—ç›¸å…³æ€§ (åœ¨å¾ªç¯å†…éƒ¨è¿›è¡Œé…å¯¹å¯¹é½)
            for i in range(len(final_factor_names)):
                for j in range(i, len(final_factor_names)):
                    factor1_name = final_factor_names[i]
                    factor2_name = final_factor_names[j]

                    if i == j:
                        correlation_matrix.loc[factor1_name, factor1_name] = 1.0
                        continue

                    data1 = factor_data_dict[factor1_name]
                    data2 = factor_data_dict[factor2_name]

                    # --- æ ¸å¿ƒæ”¹è¿›ï¼šåœ¨è¿™é‡Œè¿›è¡Œé…å¯¹å¯¹é½ ---
                    common_index = data1.index.intersection(data2.index)
                    common_columns = data1.columns.intersection(data2.columns)

                    aligned_data1 = data1.loc[common_index, common_columns]
                    aligned_data2 = data2.loc[common_index, common_columns]
                    # --- å¯¹é½ç»“æŸ ---

                    # ä½¿ç”¨å‘é‡åŒ–è®¡ç®—æˆªé¢ç›¸å…³æ€§æ—¶é—´åºåˆ—
                    time_corrs = aligned_data1.corrwith(aligned_data2, axis=1, method='spearman')

                    # æ£€æŸ¥æ¯æ—¥æœ‰æ•ˆæ ·æœ¬æ•° (è¿™ä¸€æ­¥ä¾ç„¶éå¸¸ä¸“ä¸šä¸”å¿…è¦)
                    valid_counts = aligned_data1.notna() & aligned_data2.notna()
                    valid_daily_counts = valid_counts.sum(axis=1)

                    valid_time_corrs = time_corrs[valid_daily_counts > 10]

                    if not valid_time_corrs.empty:
                        avg_corr = valid_time_corrs.mean()
                        correlation_matrix.loc[factor1_name, factor2_name] = avg_corr
                        correlation_matrix.loc[factor2_name, factor1_name] = avg_corr
                    else:
                        # å¦‚æœæ²¡æœ‰ä»»ä½•ä¸€å¤©æ»¡è¶³è®¡ç®—æ¡ä»¶ï¼Œåˆ™è®¤ä¸ºæ— ç›¸å…³æ€§
                        correlation_matrix.loc[factor1_name, factor2_name] = 0.0
                        correlation_matrix.loc[factor2_name, factor1_name] = 0.0

            return correlation_matrix.astype(float)

        except Exception as e:
            # åœ¨é¡¶å±‚å‡½æ•°æ•è·å¼‚å¸¸ï¼Œè€Œä¸æ˜¯åœ¨åŠ è½½å‡½æ•°ä¸­æŠ›å‡º
            raise ValueError(f"âŒ ç›¸å…³æ€§çŸ©é˜µè®¡ç®—å¤±è´¥: {e}")

    def _process_yellow_zone_orthogonalization(
            self,
            red_zone_survivors: List[str],
            qualified_factors: Dict[str, FactorStats]
    ) -> Tuple[List[str], List[Dict], List[Dict]]:
        """
        é˜¶æ®µ2ï¼šé»„è‰²åŒºåŸŸæ­£äº¤åŒ–å¤„ç† - åŸºäºå¹¸å­˜è€…å¤„ç†ä¸­åº¦ç›¸å…³æ€§

        ğŸ¯ æ ¸å¿ƒé€»è¾‘ï¼š
        1. åŸºäºçº¢è‰²åŒºåŸŸå¹¸å­˜è€…é‡æ–°è®¡ç®—ç›¸å…³æ€§
        2. æ‰¾å‡ºæ‰€æœ‰ä¸­åº¦ç›¸å…³å¯¹ (0.3 < |corr| < 0.7)
        3. ç”Ÿæˆæ­£äº¤åŒ–æ”¹é€ è®¡åˆ’ï¼ˆä¸ç›´æ¥ä¿®æ”¹å› å­åˆ—è¡¨ï¼‰
        4. äº§å‡ºï¼šæœ€ç»ˆå› å­åˆ—è¡¨ + æ­£äº¤åŒ–è®¡åˆ’ + å†³ç­–è®°å½•

        Args:
            red_zone_survivors: çº¢è‰²åŒºåŸŸå¹¸å­˜è€…
            qualified_factors: å› å­è¯„åˆ†ç»Ÿè®¡

        Returns:
            (final_factors, orthogonalization_plan, decisions)
        """
        # Step 1: åŸºäºå¹¸å­˜è€…é‡æ–°è®¡ç®—ç›¸å…³æ€§
        if len(red_zone_survivors) < 2:
            logger.info("  âš ï¸ å¹¸å­˜è€…ä¸è¶³2ä¸ªï¼Œè·³è¿‡é»„è‰²åŒºåŸŸå¤„ç†")
            return red_zone_survivors, [], []

        try:
            survivors_correlation_matrix = self._calculate_factor_correlations(red_zone_survivors)
            if survivors_correlation_matrix is None:
                raise ValueError("  âš ï¸ æ— æ³•è®¡ç®—å¹¸å­˜è€…ç›¸å…³æ€§çŸ©é˜µï¼Œè·³è¿‡æ­£äº¤åŒ–å¤„ç†")
                # return red_zone_survivors, [], []
        except Exception as e:
            raise ValueError(f"  âš ï¸ å¹¸å­˜è€…ç›¸å…³æ€§è®¡ç®—å¤±è´¥: {e}ï¼Œè·³è¿‡æ­£äº¤åŒ–å¤„ç†")
            # return red_zone_survivors, [], []

        # Step 2: æ‰¾å‡ºä¸­åº¦ç›¸å…³å¯¹
        medium_corr_pairs = []
        for i in range(len(red_zone_survivors)):
            for j in range(i + 1, len(red_zone_survivors)):
                factor1 = red_zone_survivors[i]
                factor2 = red_zone_survivors[j]
                corr = abs(survivors_correlation_matrix.loc[factor1, factor2])

                if self.config.medium_corr_threshold <= corr < self.config.high_corr_threshold:
                    medium_corr_pairs.append((factor1, factor2, corr))

        logger.info(f"  ğŸ“Š å‘ç° {len(medium_corr_pairs)} å¯¹ä¸­åº¦ç›¸å…³å› å­")

        # Step 3: ç”Ÿæˆæ­£äº¤åŒ–è®¡åˆ’
        orthogonalization_plan = []
        decisions = []
        final_factors = red_zone_survivors.copy()  # å…ˆä¿ç•™æ‰€æœ‰å¹¸å­˜è€…

        if not self.config.enable_orthogonalization:
            logger.info("  âš ï¸ æ­£äº¤åŒ–åŠŸèƒ½å·²ç¦ç”¨ï¼Œæ‰€æœ‰å¹¸å­˜è€…ç›´æ¥ä¿ç•™")
            return final_factors, [], []

        # æŒ‰ç›¸å…³æ€§ä»é«˜åˆ°ä½å¤„ç†
        medium_corr_pairs.sort(key=lambda x: x[2], reverse=True)

        for factor1, factor2, corr in medium_corr_pairs:
            # é€‰æ‹©è¯„åˆ†æ›´é«˜çš„ä½œä¸ºåŸºå‡†
            score1 = qualified_factors[factor1].detail_score_21d['Final_Score'] if factor1 in qualified_factors else 0.0
            score2 = qualified_factors[factor2].detail_score_21d['Final_Score'] if factor2 in qualified_factors else 0.0

            if score1 >= score2:
                base_factor, target_factor = factor1, factor2
            else:
                base_factor, target_factor = factor2, factor1

            # ç”Ÿæˆæ­£äº¤åŒ–è®¡åˆ’
            orthogonal_name = f"{target_factor}_orth_vs_{base_factor}"  # base é«˜åˆ†ï¼

            orthogonalization_plan.append({
                'original_factor': target_factor,
                'base_factor': base_factor,
                'orthogonal_name': orthogonal_name,
                'correlation': corr,
                'base_score': qualified_factors[
                    base_factor].detail_score_21d['Final_Score'] if base_factor in qualified_factors else 0.0,
                'target_score': qualified_factors[
                    target_factor].detail_score_21d['Final_Score'] if target_factor in qualified_factors else 0.0
            })

            # è®°å½•å†³ç­–
            decisions.append({
                'stage': 'yellow_zone_orthogonalization',
                'base_factor': base_factor,
                'target_factor': target_factor,
                'orthogonal_name': orthogonal_name,
                'correlation': corr,
                'decision': 'é»„è‰²é¢„è­¦-æ­£äº¤åŒ–',
                'reason': f'ä¸­åº¦ç›¸å…³({self.config.medium_corr_threshold}<=|corr|={corr:.3f}<{self.config.high_corr_threshold})'
            })

            logger.info(
                f"  ğŸ”„ æ­£äº¤åŒ–è®¡åˆ’: {target_factor} â†’ {orthogonal_name} (åŸºäº {base_factor}ï¼Œç›¸å…³æ€§={corr:.3f})")

        # Step 4: æœ€ç»ˆæ£€æŸ¥ - ç¡®ä¿æ²¡æœ‰é«˜ç›¸å…³é—æ¼
        remaining_high_corr = []
        for i in range(len(final_factors)):
            for j in range(i + 1, len(final_factors)):
                factor1 = final_factors[i]
                factor2 = final_factors[j]
                corr = abs(survivors_correlation_matrix.loc[factor1, factor2])
                if corr >= self.config.high_corr_threshold:
                    remaining_high_corr.append((factor1, factor2, corr))

        if remaining_high_corr:
            raise ValueError(f"  âŒ ä¸¥é‡é—®é¢˜ï¼šæœ€ç»ˆå› å­ä¸­ä»å­˜åœ¨é«˜ç›¸å…³å› å­ {factor1} vs {factor2}: {corr:.3f}")
        logger.info(f"âš ï¸ é»„è‰²åŒºåŸŸå¤„ç†å®Œæˆ:")
        logger.info(f"   æœ€ç»ˆå› å­æ•°: {len(final_factors)}")
        logger.info(f"   æ­£äº¤åŒ–è®¡åˆ’: {len(orthogonalization_plan)} ä¸ª")
        logger.info(f"   å†³ç­–è®°å½•: {len(decisions)} æ¡")

        return final_factors, orthogonalization_plan, decisions
    #todo åé¢æ£€éªŒçœŸç¡®æ€§
    def screen_factors_by_recent_rolling_ic(
            self,
            phase1_passed_factors: List[str],  # è¾“å…¥æ˜¯ç¬¬ä¸€é˜¶æ®µç­›é€‰å‡ºçš„ç²¾è‹±å› å­åˆ—è¡¨
            force_generate: bool = False  # è¿™ä¸ªå‚æ•°ç”¨äºæ§åˆ¶æ˜¯å¦é‡æ–°åŠ è½½æ•°æ®
    ) -> List[str]:
        """
        ã€Phase 2ã€‘å¯¹å·²ç»é€šè¿‡å…¨æ ·æœ¬æ£€éªŒçš„ç²¾è‹±å› å­ï¼Œæ ¹æ®å…¶è¿‘æœŸæ»šåŠ¨ICè¡¨ç°è¿›è¡Œä¼˜ä¸­é€‰ä¼˜ã€‚
        Args:
            phase1_passed_factors: ç¬¬ä¸€é˜¶æ®µç­›é€‰å‡ºçš„ã€ç®€å†è¿‡ç¡¬çš„å› å­åˆ—è¡¨ã€‚
        Returns:
            List[str]: é€šè¿‡äº†è¿‘æœŸçŠ¶æ€æ£€éªŒçš„ã€æœ€ç»ˆåˆæ ¼çš„å› å­åˆ—è¡¨ã€‚
        """
        # --- 1. å®šä¹‰æ»šåŠ¨ç­›é€‰çš„é…ç½®å’Œé—¨æ§› ---
        ROLLING_SCREENING_CONFIG = {
            "rolling_window_size": 12,  # å›çœ‹çª—å£ï¼š12ä¸ªå‘¨æœŸ (å¯¹äºæœˆåº¦ICåºåˆ—ï¼Œå³12ä¸ªæœˆ)
            "min_rolling_icir_threshold": 0.25,  # è¿‘æœŸæ»šåŠ¨ICIRçš„æœ€ä½é—¨æ§›
            "min_rolling_ic_mean_threshold": 0.01  # è¿‘æœŸæ»šåŠ¨ICå‡å€¼çš„æœ€ä½é—¨æ§› (ç¡®ä¿æ–¹å‘æ­£ç¡®)
        }
        logger.info("--- å¼€å§‹é˜¶æ®µäºŒï¼šåŸºäºè¿‘æœŸæ»šåŠ¨ICè¡¨ç°è¿›è¡Œä¼˜ä¸­é€‰ä¼˜ ---")
        passed_phase2_factors = []

        # éå†æ¯ä¸€ä¸ªâ€œç®€å†â€è¿‡ç¡¬çš„å› å­
        for factor_name in phase1_passed_factors:

            # --- æ­¥éª¤ 1: åŠ è½½è¯¥å› å­çš„â€œé»„é‡‘ICåºåˆ—â€ ---
            # è¿™ä¸ªåºåˆ—æ˜¯æˆ‘ä»¬ä¹‹å‰é€šè¿‡ calculate_non_overlapping_ic_series ç”Ÿæˆçš„ã€
            # è´¯ç©¿æ•´ä¸ªå†å²çš„ã€å¹²å‡€çš„æœˆåº¦ICåºåˆ—ã€‚
            # åœ¨å®é™…ç³»ç»Ÿä¸­ï¼Œè¿™é‡Œä¼šæœ‰ç¼“å­˜é€»è¾‘ã€‚
            golden_ic_series = self.resultLoadManager.get_ic_series_by_period(
                self.TARGET_UNIVERSE,
                factor_name,
                period_days=21  # å‡è®¾æˆ‘ä»¬åˆ†æçš„æ˜¯æœˆåº¦ICåºåˆ—
            )

            if golden_ic_series is None or len(golden_ic_series) < ROLLING_SCREENING_CONFIG["rolling_window_size"]:
                logger.warning(f"å› å­ {factor_name} çš„ICåºåˆ—è¿‡çŸ­ï¼Œæ— æ³•è¿›è¡Œæ»šåŠ¨åˆ†æï¼Œå·²è·³è¿‡ã€‚")
                continue

            # --- æ­¥éª¤ 2: è®¡ç®—æ»šåŠ¨ç»Ÿè®¡æŒ‡æ ‡ ---
            # ä½¿ç”¨pandasçš„ .rolling() æ–¹æ³•è¿›è¡Œè®¡ç®—
            window_size = ROLLING_SCREENING_CONFIG["rolling_window_size"]
            rolling_ic_mean = golden_ic_series.rolling(window=window_size).mean()
            rolling_ic_std = golden_ic_series.rolling(window=window_size).std()

            # è®¡ç®—æ»šåŠ¨çš„ICIRåºåˆ—
            rolling_icir = rolling_ic_mean / rolling_ic_std

            # --- æ­¥éª¤ 3: æå–æœ€æ–°çš„æ»šåŠ¨å€¼ä½œä¸ºâ€œè¿‘æœŸè¡¨ç°â€ ---
            # .iloc[-1] å¯ä»¥è·å–åˆ°æ—¶é—´åºåˆ—çš„æœ€åä¸€ä¸ªå€¼
            latest_rolling_ic_mean = rolling_ic_mean.iloc[-1]
            latest_rolling_icir = rolling_icir.iloc[-1]

            # --- æ­¥éª¤ 4: æ‰§è¡Œç­›é€‰ ---
            # æ ¸å¿ƒå†³ç­–é€»è¾‘ï¼šè¿‘æœŸè¡¨ç°æ˜¯å¦è¾¾æ ‡ï¼Ÿ
            # æ³¨æ„ï¼šæˆ‘ä»¬è¿™é‡Œä¹Ÿåº”è¯¥ä½¿ç”¨ç»å¯¹å€¼ï¼Œä»¥å®¹çº³åå‘å› å­

            icir_passed = abs(latest_rolling_icir) >= ROLLING_SCREENING_CONFIG["min_rolling_icir_threshold"]

            # åŒæ—¶ï¼Œè¦ç¡®ä¿è¿‘æœŸè¡¨ç°çš„æ–¹å‘ä¸é•¿æœŸæ–¹å‘ä¸€è‡´
            # æˆ‘ä»¬ç”¨å…¨æ ·æœ¬å‡å€¼çš„ç¬¦å·ä»£è¡¨é•¿æœŸæ–¹å‘
            long_term_direction = np.sign(golden_ic_series.mean())
            mean_passed = (latest_rolling_ic_mean * long_term_direction) >= ROLLING_SCREENING_CONFIG[
                "min_rolling_ic_mean_threshold"]

            if icir_passed and mean_passed:
                logger.info(f"  > âœ… å› å­ {factor_name} é€šè¿‡è¿‘æœŸçŠ¶æ€æ£€éªŒ (æ»šåŠ¨ICIR={latest_rolling_icir:.2f})")
                passed_phase2_factors.append(factor_name)
            else:
                logger.info(f"  > âŒ å› å­ {factor_name} æœªé€šè¿‡è¿‘æœŸçŠ¶æ€æ£€éªŒ (æ»šåŠ¨ICIR={latest_rolling_icir:.2f})ï¼Œè¢«å‰”é™¤ã€‚")

        return passed_phase2_factors
    def get_passed_factor_names(self,   need_filter_rencent_bad: bool = False,force_generate:bool=False) -> List[str]:
        passed_factor_names = self.get_base_passed_factors()
        if need_filter_rencent_bad:
            return self.screen_factors_by_recent_rolling_ic(passed_factor_names,force_generate)
        return passed_factor_names

    def select_category_champions(self, passed_factor_stats) -> Dict[str, List[str]]:
        """
        ç±»åˆ«å†…å† å†›é€‰æ‹©
        Args:
            passed_factor_names: é€šè¿‡åŸºæœ¬ç­›é€‰çš„å› å­
             for :nä¸ªç±»åˆ«"
                ç±»å†…æ’åé€»è¾‘ï¼šæŒ‰æ¢æ‰‹ç‡åŠ æƒçš„å‘¨æœŸè¡°å‡æ€»icåˆ†æ•°
                æ¯ä¸ªç±»åˆ«åªè¦2ä¸ª
        Returns:
            Dict[category, List[factor_names]]: å„ç±»åˆ«çš„å† å†›å› å­
        """
        logger.info("å¼€å§‹ç±»åˆ«å†…å† å†›é€‰æ‹©...")
        category_champions = {}
        # æ³¨æ„ éå†çš„æ˜¯ç±»åˆ«ï¼ï¼Œè€Œä¸æ˜¯å› å­ï¼Œæ‰€ä»¥åŠ¡å¿…éœ€è¦ä¿è¯ç±»åˆ«åœ¨configé…ç½®æ–‡ä»¶ï¼
        for category, factor_list in self.factor_categories.items():
            # æ‰¾åˆ°è¯¥ç±»åˆ«ä¸­çš„åˆæ ¼å› å­
            category_factors = {
                name: stats for name, stats in passed_factor_stats.items()
                if name in factor_list
            }

            if not category_factors:
                continue

            # æŒ‰æ¢æ‰‹ç‡è°ƒæ•´åè¯„åˆ†æ’åºï¼ˆå®ç›˜å¯¼å‘ä¼˜åŒ–ï¼‰
            sorted_factors = sorted(
                category_factors.items(),
                key=lambda x: x[1].detail_score_21d['Final_Score'] if self.config.enable_turnover_penalty else x[ #ç¬¬ä¸€ä¸ªfinal_score è®°å¾—åˆ‡æ¢ä¸º æ¢æ‰‹ç‡*åŸå§‹åˆ†æ•° todo
                    1].detail_score_21d['Final_Score'],
                reverse=True
            )

            # é€‰æ‹©å‰Nå
            max_count = min(len(sorted_factors), self.config.max_factors_per_category)
            champions = [name for name, _ in sorted_factors[:max_count]]

            if champions:
                category_champions[category] = champions
                logger.info(f"{category}: {len(champions)} ä¸ªå† å†›")
                for name in champions:
                    stats = passed_factor_stats[name]
                    direction = "+" if np.sign(stats.ic_mean_21d) > 0 else "-"
                    score_used = stats.detail_score_21d if self.config.enable_turnover_penalty else stats.detail_score_21d
                    logger.info(
                        f"  {direction} {name}: è°ƒæ•´å21dè¯„åˆ†={score_used} (top_q_21dæ¢æ‰‹ç‡={stats.top_q_turnover_dict['21d']})")

        return category_champions

    def _generate_selection_report(self, candidate_factors: List[str],
                                   qualified_factors: Dict[str, FactorStats],
                                   category_champions: Dict[str, List[str]],
                                   final_selection: List[str],
                                   correlation_report: Dict[str, Any] = None) -> Dict[str, Any]:
        """ç”Ÿæˆé€‰æ‹©æŠ¥å‘Š"""

        # ç»Ÿè®¡ä¿¡æ¯
        qualified_count = len(qualified_factors)
        champions_count = sum(len(champions) for champions in category_champions.values())
        final_count = len(final_selection)

        # è¯„åˆ†ç»Ÿè®¡
        if qualified_factors:
            scores = [stats.detail_score_21d['Final_Score'] for stats in qualified_factors.values()]
            avg_score = np.mean(scores)
            max_score = np.max(scores)
            min_score = np.min(scores)
        else:
            avg_score = max_score = min_score = 0.0

        # ç±»åˆ«åˆ†å¸ƒ
        category_distribution = {}
        for factor in final_selection:
            for category, factor_list in self.factor_categories.items():
                if factor in factor_list:
                    category_distribution[category] = category_distribution.get(category, 0) + 1
                    break

        # æ„å»ºæŠ¥å‘Š
        report = {
            'selection_config': {
                'snap_config_id': self.snap_config_id,
                'pool_index': self.pool_index,
                'time_range': f"{self.start_date} - {self.end_date}",
                'selection_criteria': {
                    'min_ic_abs_mean': self.config.min_ic_abs_mean,
                    'min_ir_abs_mean': self.config.min_ir_abs_mean,
                    'min_ic_stability': self.config.min_ic_stability,
                    'decay_rate': self.config.decay_rate
                }
            },
            'selection_summary': {
                'candidate_count': len(candidate_factors),
                'qualified_count': qualified_count,
                'champions_count': champions_count,
                'final_count': final_count,
                'pass_rate': qualified_count / len(candidate_factors) if candidate_factors else 0.0
            },
            'score_statistics': {
                'avg_score': avg_score,
                'max_score': max_score,
                'min_score': min_score
            },
            'category_distribution': category_distribution,
            'final_selection': final_selection,
            'factor_details': {
                factor: {
                    'final_score_21d': qualified_factors[factor].detail_score_21d['Final_Score'],
                    # 'avg_ic_abs': qualified_factors[factor].avg_ic_abs,
                    # 'avg_ir_abs': qualified_factors[factor].avg_ir_abs,
                    # 'avg_stability': qualified_factors[factor].avg_stability,
                    # 'snapshot_count': qualified_factors[factor].snapshot_count,
                    # 'time_range': qualified_factors[factor].time_range
                }
                for factor in final_selection if factor in qualified_factors
            }
        }

        # æ·»åŠ ç›¸å…³æ€§æ§åˆ¶æŠ¥å‘Š
        if correlation_report:
            report['correlation_control'] = {
                'enabled': True,
                'philosophy': 'ä¸‰å±‚ç›¸å…³æ€§æ§åˆ¶å“²å­¦',
                'thresholds': correlation_report.get('thresholds', {}),
                'processing_summary': {
                    'input_factors': correlation_report.get('input_count', 0),
                    'final_factors': correlation_report.get('final_count', 0),
                    'orthogonalized_factors': correlation_report.get('orthogonalized_count', 0),
                    'total_decisions': len(correlation_report.get('decisions', []))
                },
                'decisions_breakdown': self._summarize_correlation_decisions(correlation_report.get('decisions', [])),
                'orthogonalized_factors': correlation_report.get('orthogonalized_factors', []),
                'detailed_decisions': correlation_report.get('decisions', [])
            }
        else:
            report['correlation_control'] = {
                'enabled': False,
                'reason': 'ç›¸å…³æ€§æ§åˆ¶è·³è¿‡æˆ–å¤±è´¥'
            }

        return report

    def _summarize_correlation_decisions(self, decisions: List[Dict]) -> Dict[str, int]:
        """æ±‡æ€»ç›¸å…³æ€§å†³ç­–ç»Ÿè®¡"""
        summary = {
            'çº¢è‰²è­¦æŠ¥-äºŒé€‰ä¸€': 0,
            'é»„è‰²é¢„è­¦-æ­£äº¤åŒ–': 0,
            'ç»¿è‰²å®‰å…¨-ç›´æ¥ä¿ç•™': 0
        }

        for decision in decisions:
            decision_type = decision.get('decision', '')
            if decision_type in summary:
                summary[decision_type] += 1

        return summary

    def run_complete_selection(self,pool_index, force_generate: bool = False) -> Tuple[
        List[str], Dict[str, Any]]:
        """
        ç¬¬ä¸€æ­¥ï¼šå…¨æ ·æœ¬â€œç¡¬ç­›é€‰â€ (Phase 1 - çœ‹ç®€å†):

        ï¼ˆå…¨æ ·æœ¬ICå‡å€¼ã€ICIRã€Newey-West Tå€¼ï¼‰ï¼Œå¯¹æ‰€æœ‰å¤‡é€‰å› å­è¿›è¡Œä¸€æ¬¡æ®‹é…·çš„â€œèµ„æ ¼è®¤è¯â€ã€‚

        ç›®çš„ï¼š ç¡®ä¿è¿›å…¥ä¸‹ä¸€è½®çš„ï¼Œéƒ½æ˜¯åœ¨è¿‡å»æ•°å¹´å®Œæ•´å†å²ä¸­ï¼Œè¢«è¯æ˜äº†â€œåŸºå› â€ä¼˜ç§€çš„å› å­ã€‚

        ç¬¬äºŒæ­¥ï¼šæ»šåŠ¨è¡¨ç°â€œä¼˜ä¸­é€‰ä¼˜â€ (Phase 2 - çœ‹çŠ¶æ€):

        åœ¨ç¬¬ä¸€æ­¥ç­›é€‰å‡ºçš„â€œç²¾è‹±æ± â€å†…éƒ¨ï¼Œæˆ‘ä»¬æ‰å¼€å§‹è€ƒå¯Ÿå®ƒä»¬è¿‘æœŸçš„æ»šåŠ¨ICè¡¨ç°ã€‚

        ç›®çš„ï¼š ä»ä¸€ç¾¤â€œåŸºå› â€éƒ½å¾ˆå¥½çš„å› å­ä¸­ï¼ŒæŒ‘é€‰å‡ºé‚£äº›â€œè¿‘æœŸçŠ¶æ€â€ä¹Ÿæ­£ä½³çš„ã€‚

        ç¬¬ä¸‰æ­¥ï¼šç±»åˆ«å†…é€‰æ‹© (Intra-Category Selection):

        æµç¨‹ä¸å˜ã€‚

        ç¬¬å››æ­¥ï¼šç›¸å…³æ€§æ§åˆ¶ (Correlation Control):

        æµç¨‹ä¸å˜ã€‚
        Args:
            factor_names: å€™é€‰å› å­åˆ—è¡¨
        Returns:
            Tuple[List[str], Dict]: (é€‰ä¸­å› å­åˆ—è¡¨, è¯¦ç»†æŠ¥å‘Š)
        """
        logger.info("=" * 60)
        logger.info("å¼€å§‹åŸºäºæ»šåŠ¨ICçš„å®Œæ•´å› å­ç­›é€‰")
        logger.info("=" * 60)

        # ç¬¬ä¸€æ­¥ ç­›é€‰ï¼ˆbase+è¿‘æœŸè¡¨ç°
        passed_factor_names = self.get_passed_factor_names( False, force_generate)
        
        passed_factor_stats = self.build_stats_dict(passed_factor_names)
        if not passed_factor_names:
            logger.warning("è­¦å‘Šï¼šæ²¡æœ‰å› å­é€šè¿‡åŸºç¡€ICç­›é€‰")
            return [], {}

        # ç¬¬äºŒæ­¥ï¼šç±»åˆ«å†…é€‰æ‹©
        category_champions = self.select_category_champions(passed_factor_stats)

        if not category_champions:
            logger.warning("è­¦å‘Šï¼šæ²¡æœ‰ç±»åˆ«å† å†›")
            return [], {}

        # ç¬¬ä¸‰æ­¥ï¼šåˆæ­¥æœ€ç»ˆé€‰æ‹© ï¼ˆåªæ˜¯è¿‡æ»¤æ•°é‡çš„è¿‡æ»¤è€Œå·²ï¼‰ï¼Œé™åˆ¶æœ€å¤šå…«ä¸ª
        preliminary_selection = self.generate_final_selection(category_champions, passed_factor_stats)

        # ç¬¬å››æ­¥ï¼šä¸‰å±‚ç›¸å…³æ€§æ§åˆ¶å“²å­¦
        final_selection, correlation_report = self.apply_correlation_control(  # debug here
            preliminary_selection, passed_factor_stats
        )

        # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
        report = self._generate_selection_report(
            passed_factor_names, passed_factor_stats, category_champions, final_selection, correlation_report
        )

        logger.info("=" * 60)
        logger.info("æ»šåŠ¨ICå› å­ç­›é€‰å®Œæˆï¼")
        logger.info(f"æ¨èç”¨äºICåŠ æƒåˆæˆ: {final_selection}")
        logger.info("=" * 60)

        return final_selection, report

    def generate_final_selection(self, category_champions: Dict[str, List[str]],
                                 qualified_factors: Dict[str, FactorStats]) -> List[str]:
        """
        ç”Ÿæˆæœ€ç»ˆå› å­é€‰æ‹©
        ï¼ˆåªæ˜¯è¿‡æ»¤æ•°é‡çš„è¿‡æ»¤è€Œå·²ï¼‰ï¼Œé™åˆ¶æœ€å¤šå…«ä¸ª
        Args:
            category_champions: å„ç±»åˆ«å† å†›
            qualified_factors: åˆæ ¼å› å­ç»Ÿè®¡

        Returns:
            List[str]: æœ€ç»ˆé€‰æ‹©çš„å› å­åå•
        """
        logger.info("ç”Ÿæˆæœ€ç»ˆå› å­é€‰æ‹©...")

        # æ”¶é›†æ‰€æœ‰å† å†›
        all_champions = []
        for category, champions in category_champions.items():
            for champion in champions:
                if champion in qualified_factors:
                    all_champions.append((champion, qualified_factors[champion]))

        # æŒ‰å¤šå‘¨æœŸè¯„åˆ†æ’åº
        all_champions.sort(key=lambda x: x[1].detail_score_21d['Final_Score'], reverse=True)

        # é€‰æ‹©å‰Nå
        max_selection = min(len(all_champions), self.config.max_final_factors)
        final_selection = [name for name, _ in all_champions[:max_selection]]

        logger.info(f"æœ€ç»ˆé€‰æ‹© {len(final_selection)} ä¸ªå› å­:")
        for i, (name, stats) in enumerate(all_champions[:max_selection], 1):

            logger.info(f"   å› å­:{name}-------------------------------")
            logger.info(f"       è¯„åˆ†: {stats.detail_score_21d['Final_Score']:.1f}")
            logger.info(f"       IC_mean_21d: {stats.ic_mean_21d:.3f}, IC_IR_21d: {stats.ic_ir_21d:.2f}")
            logger.info(f"      ç»†èŠ‚åˆ†æ•°: {stats.detail_score_21d['Final_Score']}")

        return final_selection
    def run_factor_analysis(self, TARGET_STOCK_POOL: str, top_n_final: int = 5, correlation_threshold: float = 0.5,
                            run_version: str = None):
        RESULTS_PATH = workspaces_result_dir

        # --- ç¬¬ä¸€ã€äºŒçº§ç«ç®­: æ„å»ºå¤šå‘¨æœŸå† å†›æ’è¡Œæ¦œ ---
        champion_leaderboard = self.build_champion_leaderboard(
            results_path=RESULTS_PATH,
            target_stock_pool=TARGET_STOCK_POOL,
            run_version=run_version
        )
        print("\n--- å› å­å† å†›æ’è¡Œæ¦œ (å·²é€‰å‡ºæ¯ä¸ªå› å­çš„æœ€ä½³å‘¨æœŸ) ---")

        print(champion_leaderboard.head(10))

        # --- ç¬¬ä¸‰çº§ç«ç®­: ä»å† å†›æ’è¡Œæ¦œä¸­ï¼Œç­›é€‰å‡ºæœ€ç»ˆçš„ã€å¤šæ ·åŒ–çš„é¡¶çº§å› å­ ---
        # top_factors_df = self.get_top_factors(
        #     leaderboard_df=champion_leaderboard,
        #     results_path=RESULTS_PATH,
        #     stock_pool_index=TARGET_STOCK_POOL,
        #     quality_score_threshold=0.0,  # å»ºè®®è®¾ç½®ä¸€ä¸ªæœ‰æ„ä¹‰çš„é—¨æ§›åˆ†ï¼Œæ¯”å¦‚40åˆ†
        #     top_n_final=top_n_final,
        #     correlation_threshold=correlation_threshold
        # )
        print("\n--- æœ€ç»ˆå…¥é€‰çš„é¡¶çº§å› å­è¯¦æƒ… (Diversified Top Factors) ---")
        print(champion_leaderboard)

        # --- åç»­æ­¥éª¤: ä¸ºæœ€ç»ˆå…¥é€‰çš„å› å­ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š ---
        # ... (è¿™é‡Œçš„é€»è¾‘ä¸ä½ ä¹‹å‰çš„ç‰ˆæœ¬ç±»ä¼¼, å¯ä»¥å¤ç”¨)
        logger.info("\n--- å¼€å§‹ä¸ºé¡¶çº§å› å­ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š ---")
        for _, factor_row in champion_leaderboard.iterrows():
            factor_name = factor_row['factor_name']
            best_period = factor_row['best_period']

            print(f"æ­£åœ¨ä¸ºå› å­ '{factor_name}' (æœ€ä½³å‘¨æœŸ: {best_period}) ç”ŸæˆæŠ¥å‘Š...")
            print(f"æ­£åœ¨ä¸ºå› å­ '{factor_name}' ç”ŸæˆæŠ¥å‘Š...")
            # 2. ç”Ÿæˆæ‚¨éœ€è¦çš„æŠ¥å‘Š
            viz_manager = self.visualization_manager
            # --- é€‰é¡¹ Aï¼šç”Ÿæˆæœ€å…¨é¢çš„â€œä¸šç»©æŠ¥å‘Šâ€ ---
            viz_manager.plot_performance_report(
                backtest_base_on_index=TARGET_STOCK_POOL,
                factor_name=factor_name,
                results_path=RESULTS_PATH,
                default_config='o2o',
                run_version='latest'
            )

            # --- é€‰é¡¹ Bï¼šç”Ÿæˆâ€œç‰¹æ€§è¯Šæ–­æŠ¥å‘Šâ€ï¼Œæ·±å…¥äº†è§£å› å­è‡ªèº«å±æ€§ ---
            viz_manager.plot_characteristics_report(
                backtest_base_on_index=TARGET_STOCK_POOL,
                factor_name=factor_name,
                results_path=RESULTS_PATH,
                default_config='o2o',
                run_version='latest'
            )

            # --- é€‰é¡¹ Cï¼šç”Ÿæˆâ€œå½’å› é¢æ¿â€ï¼Œç›´è§‚å¯¹æ¯”é¢„å¤„ç†å‰åçš„æ•ˆæœ ---
            viz_manager.plot_attribution_panel(
                backtest_base_on_index=TARGET_STOCK_POOL,
                factor_name=factor_name,
                results_path=RESULTS_PATH,
                default_config='o2o',
                run_version='latest'
            )

            # --- é€‰é¡¹ Dï¼šç”Ÿæˆâ€œæ ¸å¿ƒæ‘˜è¦â€ï¼Œç”¨äºå¿«é€Ÿæµè§ˆå…³é”®ä¸šç»© ---
            viz_manager.plot_ic_quantile_panel(
                backtest_base_on_index=TARGET_STOCK_POOL,
                factor_name=factor_name,
                results_path=RESULTS_PATH,
                default_config='o2o',
                run_version='latest'
            )
            # # 4.1 ç”Ÿæˆä¸»æŠ¥å‘Š (3x2 ç»Ÿä¸€è¯„ä¼°æŠ¥å‘Š)
            # # ç»˜å›¾å‡½æ•°ç°åœ¨éœ€è¦ä»ç¡¬ç›˜åŠ è½½æ•°æ®ï¼Œæˆ‘ä»¬åªéœ€å‘ŠçŸ¥å…³é”®ä¿¡æ¯
            # self.visualization_manager.plot_unified_factor_report(
            #     backtest_base_on_index=TARGET_STOCK_POOL,
            #     factor_name=factor_name,
            #     results_path=RESULTS_PATH,  # <--- ä¼ å…¥æˆæœåº“çš„æ ¹è·¯å¾„
            #     # ä½ å¯ä»¥å†³å®šä¸»æŠ¥å‘Šé»˜è®¤ä½¿ç”¨C2Cè¿˜æ˜¯O2Cçš„ç»“æœ
            #     default_config='o2o'
            # )
            #
            # # 4.2 è°ƒç”¨æ–°çš„åˆ†å±‚å‡€å€¼æŠ¥å‘Šå‡½æ•°
            # self.visualization_manager.plot_diagnostics_report(
            #     backtest_base_on_index=TARGET_STOCK_POOL,
            #     factor_name=factor_name,
            #     results_path=RESULTS_PATH,
            #     default_config='o2o'
            # )
            # # è°ƒç”¨æ–°çš„å½’å› åˆ†æé¢æ¿å‡½æ•°
            # self.visualization_manager.plot_attribution_panel(
            #     backtest_base_on_index=TARGET_STOCK_POOL,
            #     factor_name=factor_name,
            #     results_path=RESULTS_PATH,
            #     default_config='o2o'
            # )
            #

    def _build_single_period_row(self, factor_dir: Path, period: str, run_version: str) -> Dict | None:
        """ã€è¾…åŠ©å‡½æ•°ã€‘ä¸ºå•ä¸ªå› å­ã€å•ä¸ªå‘¨æœŸæ„å»ºç”¨äºæ‰“åˆ†çš„å®½è¡¨è¡Œ"""

        def _find_and_load_stats(factor_dir: Path, config_name: str, version: str = 'latest') -> Dict | None:
            config_path = factor_dir / config_name
            if not config_path.is_dir(): return None
            version_dirs = [d for d in config_path.iterdir() if d.is_dir()]
            if not version_dirs: return None
            target_version_path = sorted(version_dirs)[-1] if version == 'latest' else config_path / version
            if not target_version_path.exists(): return None
            summary_file = target_version_path / 'summary_stats.json'
            if summary_file.exists():
                with open(summary_file, 'r') as f: return json.load(f)
            return None

        stats_o2o = _find_and_load_stats(factor_dir, 'o2o', run_version)
        if not stats_o2o: return None

        row = {'factor_name': factor_dir.name}
        for r_type, stats_data in [('o2o', stats_o2o)]:
            for d_type in ['raw', 'processed']:
                try:
                    ic_stats = stats_data.get(f'ic_analysis_{d_type}', {}).get(period, {})
                    q_stats = stats_data.get(f'quantile_backtest_{d_type}', {}).get(period, {})
                    if not ic_stats or not q_stats: continue  # å¦‚æœè¯¥å‘¨æœŸæ•°æ®ä¸å®Œæ•´ï¼Œåˆ™è¿”å›None

                    row[f'ic_mean_{d_type}_{r_type}'] = ic_stats.get('ic_mean')
                    row[f'ic_ir_{d_type}_{r_type}'] = ic_stats.get('ic_ir')
                    row[f'ic_t_stat_{d_type}_{r_type}'] = ic_stats.get('ic_t_stat')

                    row[f'tmb_sharpe_{d_type}_{r_type}'] = q_stats['tmb_sharpe']
                    row[f'tmb_max_drawdown_{d_type}_{r_type}'] = q_stats['tmb_max_drawdown']
                    row[f'monotonicity_spearman_{d_type}_{r_type}'] = q_stats['monotonicity_spearman']
                except:
                    continue

            fm_stats = stats_data.get('fama_macbeth', {}).get(period, {})
            row[f'fm_t_statistic_processed_{r_type}'] = fm_stats.get('t_statistic')

        return row

    def build_factor_ic_data(self,
                             run_version: str = 'latest') :
        base_path = workspaces_result_dir / self.resultLoadManager.pool_index
        ret = {}
        for factor_dir in base_path.iterdir():
            if not factor_dir.is_dir(): continue
            one_period = {}
            factor_name = factor_dir.name
            for period in self.ALL_PERIODS:
                # 1. ä¸ºå½“å‰å› å­å’Œå‘¨æœŸæ„å»ºä¸€ä¸ªå®Œæ•´çš„æŒ‡æ ‡è¡Œ
                current_period_row = self._build_single_period_row(factor_dir, period, run_version)
                if current_period_row == None:
                    continue
                ic_ir = current_period_row['ic_ir_processed_o2o']
                ic_mean = current_period_row['ic_mean_processed_o2o']
                ic_t_stat = current_period_row['ic_t_stat_processed_o2o']
                one_period[period] = {'ic_mean':ic_mean,'ic_ir':ic_ir, 'ic_t_stat':ic_t_stat}
            if  len (one_period)!=0:
                ret[factor_name] = one_period
        return ret
    def build_champion_leaderboard(self, results_path: str, target_stock_pool: str,
                                   run_version: str = 'latest') -> pd.DataFrame:
        """
        ã€V4.0-å¤šå‘¨æœŸå† å†›ç‰ˆã€‘ - å®ç°äº†ç¬¬ä¸€å’Œç¬¬äºŒçº§ç«ç®­
        1. æ‰«ææŒ‡å®šè‚¡ç¥¨æ± ä¸‹çš„æ‰€æœ‰å› å­ã€‚
        2. å¯¹æ¯ä¸ªå› å­ï¼Œéå†å…¶æ‰€æœ‰æµ‹è¯•å‘¨æœŸï¼Œæ‰¾åˆ°å¾—åˆ†æœ€é«˜çš„â€œæœ€ä½³å‘¨æœŸâ€ã€‚
        3. å°†æ‰€æœ‰å› å­çš„â€œå† å†›ç‰ˆæœ¬â€æ±‡æ€»æˆä¸€ä¸ªæ’è¡Œæ¦œã€‚
        """
        logger.info(f"æ­£åœ¨ä¸ºè‚¡ç¥¨æ±  [{target_stock_pool}] æ„å»ºå¤šå‘¨æœŸå† å†›æ’è¡Œæ¦œ...")
        champions_data = []
        base_path = Path(results_path) / target_stock_pool

        for factor_dir in base_path.iterdir():
            if not factor_dir.is_dir(): continue
            factor_name = factor_dir.name

            highest_score = -1
            best_period_champion_row = None

            # --- ç¬¬ä¸€çº§ç«ç®­ï¼šå› å­å†…éƒ¨çš„â€œå‘¨æœŸé€‰ç¾â€ ---
            for period in self.ALL_PERIODS:
                # 1. ä¸ºå½“å‰å› å­å’Œå‘¨æœŸæ„å»ºä¸€ä¸ªå®Œæ•´çš„æŒ‡æ ‡è¡Œ
                current_period_row = self._build_single_period_row(factor_dir, period, run_version)
                if current_period_row is None:
                    logger.info(f"  > å› å­ {factor_name} åœ¨å‘¨æœŸ {period} æ•°æ®ä¸å®Œæ•´ï¼Œå·²è·³è¿‡ã€‚")
                    continue

                # 2. ä¸ºè¯¥å‘¨æœŸçš„è¡¨ç°æ‰“åˆ†
                scores = calculate_factor_score_v33(current_period_row)

                # 3. é€‰å‡ºå† å†›
                if scores['Final_Score'] > highest_score:
                    highest_score = scores['Final_Score']
                    # è®°å½•å† å†›ä¿¡æ¯ï¼šåˆå¹¶æŒ‡æ ‡å’Œåˆ†æ•°ï¼Œå¹¶åŠ ä¸Šæœ€ä½³å‘¨æœŸ
                    best_period_champion_row = {
                        **current_period_row,
                        **scores,
                        'best_period': period
                    }

            # é€‰ç¾ç»“æŸåï¼Œè®°å½•å† å†›æ¡£æ¡ˆ
            if best_period_champion_row:
                champions_data.append(best_period_champion_row)
                logger.info(f"âœ“ å› å­ {factor_name} çš„æœ€ä½³å‘¨æœŸä¸º [ {best_period_champion_row['best_period']} ], "
                            f"æœ€é«˜åˆ†: {best_period_champion_row['Final_Score']:.2f}")
            else:
                logger.warning(f"âœ— æœªèƒ½ä¸ºå› å­ {factor_name} åœ¨ä»»ä½•å‘¨æœŸæ‰¾åˆ°å®Œæ•´çš„æµ‹è¯•ç»“æœã€‚")

        # --- ç¬¬äºŒçº§ç«ç®­ï¼šæ„å»ºå† å†›æ’è¡Œæ¦œ ---
        if not champions_data:
            raise ValueError(f"åœ¨è·¯å¾„ {base_path} ä¸‹ï¼Œæ²¡æœ‰æ‰¾åˆ°ä»»ä½•å¯ä»¥ç”Ÿæˆå† å†›æ’è¡Œæ¦œçš„å› å­ã€‚")

        final_leaderboard = pd.DataFrame(champions_data).set_index('factor_name', drop=False)
        ##

        [['Final_Score','ic_mean_processed_o2o', 'ic_ir_processed_o2o', 'tmb_sharpe_processed_o2o',
         'tmb_max_drawdown_processed_o2o', 'monotonicity_spearman_processed_o2o', 'fm_t_statistic_processed_o2o',
         'Prediction_Score', 'Strategy_Score', 'Stability_Score', 'Purity_Score', 'Composability_Score', 'Final_Score','factor_name', 'ic_mean_raw_o2o', 'ic_ir_raw_o2o', 'tmb_sharpe_raw_o2o', 'tmb_max_drawdown_raw_o2o',
         'monotonicity_spearman_raw_o2o',
         'Grade', 'Factor_Direction', 'Composability_Passed', 'best_period']]
        #
        ret  = final_leaderboard.sort_values(by='Final_Score', ascending=False)
        return ret

    def get_top_factors(self, leaderboard_df: pd.DataFrame, results_path: str, stock_pool: str,
                        quality_score_threshold: float, top_n_final: int, correlation_threshold: float,
                        run_version: str = 'latest') -> pd.DataFrame:
        """
        ã€V2.0-å‡çº§ç‰ˆã€‘ä»å† å†›æ’è¡Œæ¦œä¸­ï¼Œç­›é€‰å‡ºæœ€ç»ˆçš„ã€å¤šæ ·åŒ–çš„é¡¶çº§å› å­ã€‚
        """
        logger.info(f"--- ç¬¬ä¸‰çº§ç«ç®­: å¼€å§‹ç­›é€‰å¤šæ ·åŒ–çš„é¡¶çº§å› å­ ---")

        # 1. è´¨é‡ç­›é€‰
        candidate_df = leaderboard_df[leaderboard_df['Final_Score'] >= quality_score_threshold].copy()
        if candidate_df.empty:
            logger.warning(f"æ²¡æœ‰å› å­çš„ç»¼åˆå¾—åˆ†è¶…è¿‡ {quality_score_threshold}ã€‚")
            return pd.DataFrame()
        logger.info(f"é€šè¿‡æœ€ä½åˆ†æ•°é˜ˆå€¼ï¼Œç­›é€‰å‡º {len(candidate_df)} ä¸ªé«˜è´¨é‡å€™é€‰å› å­ã€‚")

        # 2. å¤šæ ·åŒ–ç­›é€‰ (å»ç›¸å…³æ€§)
        # ã€æ ¸å¿ƒå‡çº§ã€‘è°ƒç”¨æ–°ç‰ˆåŠ è½½å‡½æ•°ï¼Œè¯¥å‡½æ•°èƒ½å¤„ç†ä¸åŒçš„æœ€ä½³å‘¨æœŸ
        factor_returns_matrix = self.load_fm_returns_for_champions(
            candidate_df=candidate_df,
            results_path=results_path,
            stock_pool=stock_pool,
            config='o2o',
            run_version=run_version
        )
        correlation_matrix = factor_returns_matrix.corr()

        final_selected_factors = []
        # è´ªå¿ƒç®—æ³•ï¼šä»å¾—åˆ†æœ€é«˜çš„å› å­å¼€å§‹ (candidate_dfå·²æŒ‰åˆ†æ•°æ’åº)
        for factor_name in candidate_df.index:
            if len(final_selected_factors) >= top_n_final: break
            if not final_selected_factors:
                final_selected_factors.append(factor_name)
                continue

            correlations_with_selected = correlation_matrix.loc[factor_name, final_selected_factors].abs()
            if correlations_with_selected.max() < correlation_threshold:
                final_selected_factors.append(factor_name)

        logger.info(f"--- ç­›é€‰å®Œæˆ ---")
        logger.info(f"æœ€ç»ˆé€‰å‡º {len(final_selected_factors)} ä¸ªå¤šæ ·åŒ–é¡¶çº§å› å­ï¼š{final_selected_factors}")

        return leaderboard_df.loc[final_selected_factors]

    def load_fm_returns_for_champions(self, candidate_df: pd.DataFrame, results_path: str, stock_pool: str,
                                      config: str, run_version: str) -> pd.DataFrame:
        """
        ã€V3.0-å‡çº§ç‰ˆã€‘è¾…åŠ©å‡½æ•°ï¼šä¸ºå† å†›å› å­åŠ è½½F-Mæ”¶ç›Šåºåˆ—ï¼Œç”¨äºè®¡ç®—ç›¸å…³æ€§ã€‚
        èƒ½å¤Ÿæ ¹æ®æ¯ä¸ªå› å­çš„ 'best_period' åŠ è½½å¯¹åº”çš„æ”¶ç›Šæ–‡ä»¶ã€‚
        """
        all_returns = {}
        base_results_path = Path(results_path)

        # éå†å† å†›å› å­DataFrameçš„æ¯ä¸€è¡Œ
        for factor_name, row in candidate_df.iterrows():
            period = row['best_period']  # <-- ã€æ ¸å¿ƒã€‘è·å–è¯¥å› å­çš„æœ€ä½³å‘¨æœŸ

            # --- ç‰ˆæœ¬å®šä½é€»è¾‘ ---
            factor_path = base_results_path / stock_pool / factor_name / config
            if not factor_path.is_dir(): continue
            version_dirs = [d for d in factor_path.iterdir() if d.is_dir()]
            if not version_dirs: continue
            target_version_path = sorted(version_dirs)[-1] if run_version == 'latest' else factor_path / run_version
            if not target_version_path.exists(): continue

            # --- ä½¿ç”¨æœ€ä½³å‘¨æœŸæ„å»ºåŠ¨æ€æ–‡ä»¶è·¯å¾„ ---
            file_path = target_version_path / f"fm_returns_series_{period}.parquet"
            if file_path.exists():
                return_series = pd.read_parquet(file_path).squeeze()
                all_returns[factor_name] = return_series
            else:
                logger.warning(f"è­¦å‘Š: æœªæ‰¾åˆ°æ–‡ä»¶: {file_path}")

        if not all_returns:
            logger.error(f"æœªèƒ½ä¸ºä»»ä½•å€™é€‰å› å­åŠ è½½F-Mæ”¶ç›Šåºåˆ—ã€‚")
            return pd.DataFrame()

        return pd.DataFrame(all_returns)

    def build_factor_categorie_maps(self):
        # è¯»å–é…ç½®
        config = _load_local_config_functional()
        factor_definitions = config['factor_definition']

        maps = defaultdict(list)
        for factor in factor_definitions:
            maps[factor['style_category']].append(factor['name'])

        return dict(maps)
    def build_stats_dict(self, factor_names):
        ret = {}
        for f in factor_names:
            ret[f] = self.build_stats(f)
        return ret
    def build_stats(self, factor_name):
        summary_stats = self.resultLoadManager.get_summary_stats(factor_name)
        score = score_factor_from_stats_for_21d(summary_stats)
         # æ„å»ºç»“æœ
        factor_stats = FactorStats(
            factor_name=factor_name,
            ic_mean_21d=summary_stats['ic_analysis_processed']['21d']['ic_mean'],
            ic_ir_21d=summary_stats['ic_analysis_processed']['21d']['ic_ir'],
            detail_score_21d=score,
            top_q_turnover_dict=summary_stats['turnover'],#todo è®°å¾—åç»­æ”¹æˆï¼štop_q_turnover
            # periods_data=aggregated_periods,
            # avg_ic_with_sign=avg_ic_with_sign,
            # avg_ir_ir_with_sign=avg_ic_ir_with_sign,
            # avg_ic_abs=avg_ic_abs,
            # avg_ir_abs=avg_ir_abs,
            # best_period_ic_ir=best_period_ic_ir,
            # nw_t_stat_series_mean=nw_t_stat_series_mean,
            # avg_stability=np.mean(all_stabilities) if all_stabilities else 0.0,
            # avg_ic_volatility=np.mean(all_ic_stds) if all_ic_stds else 0.0,
            # detail_score_21d=detail_score_21d,
            # snapshot_count=len(dates_range),
            # time_range=(min(dates_range), max(dates_range)) if dates_range else ('', ''),
            # # å°†ä¸‰ä¸ªæ ¸å¿ƒæ¢æ‰‹ç‡æŒ‡æ ‡å¡«å…¥è¿”å›ç»“æ„
            # daily_rank_change_mean=final_turnover_stats['avg_daily_rank_change'],
            # daily_turnover_trend=final_turnover_stats['daily_turnover_trend'],
            # daily_turnover_volatility=final_turnover_stats['daily_turnover_volatility'],
            # turnover_adjusted_score=turnover_adjusted_score
        )
        return factor_stats


# ç»´æŒé…ç½®ä¸å˜ï¼Œå› ä¸ºæˆ‘ä»¬ä¼šåœ¨ä»£ç ä¸­å¤„ç†æ–¹å‘
PHASE1_SCREENING_CONFIG = {
    'min_full_sample_icir_abs': 0.4,   # ä¿®æ­£ï¼šæˆ‘ä»¬ç°åœ¨å…³å¿ƒICIRçš„ç»å¯¹å€¼
    'min_full_sample_ic_mean_abs': 0.02, # ä¿®æ­£ï¼šICå‡å€¼çš„ç»å¯¹å€¼ä¹Ÿåº”è¾¾æ ‡
    'min_newey_west_t_stat_abs': 1.96, # Tå€¼çš„ç»å¯¹å€¼è¦æ˜¾è‘— (95%ç½®ä¿¡åº¦)
    'min_win_rate': 0.55               # èƒœç‡ä¾ç„¶é‡è¦
}


def screen_factor_phase1(
        summary_row: Union[pd.Series, dict],
        config: Dict = None
) -> Tuple[bool, Dict]:
    """
    ã€V2ç‰ˆï¼šå› å­å‡†å…¥ç­›é€‰å‡½æ•° - æ–¹å‘ä¸­æ€§ã€‘
    æ­¤ç‰ˆæœ¬åŸºäºICIRçš„ã€ç»å¯¹å€¼ã€‘è¿›è¡Œç­›é€‰ï¼Œèƒ½åŒæ—¶è¯†åˆ«æ­£å‘å’Œåå‘çš„æœ‰æ•ˆå› å­ã€‚

    Returns:
        Tuple[bool, Dict]:
        - is_passed (bool): æ˜¯å¦é€šè¿‡ç­›é€‰ã€‚
        - screening_results (Dict): åŒ…å«æ ¸å¿ƒæŒ‡æ ‡å’Œã€å› å­æ–¹å‘ã€‘çš„å­—å…¸ã€‚
    """
    if config is None:
        config = PHASE1_SCREENING_CONFIG

    ic_mean = summary_row.get('full_sample_ic_mean', 0)
    ic_ir = summary_row.get('full_sample_icir', 0)
    nw_t_stat = summary_row.get('full_sample_nw_t_stat', 0)
    win_rate = summary_row.get('full_sample_win_rate', 0)

    # --- æ ¸å¿ƒä¿®æ­£ 1ï¼šåˆ¤æ–­å› å­æ–¹å‘ ---
    # np.sign()ä¼šè¿”å›1, -1,æˆ–0ã€‚å¦‚æœic_meanæ¥è¿‘0ï¼Œæˆ‘ä»¬é»˜è®¤ä¸ºæ­£å‘1ã€‚
    factor_direction = np.sign(ic_mean) if abs(ic_mean) > 1e-6 else 1

    # --- æ ¸å¿ƒä¿®æ­£ 2ï¼šåŸºäºç»å¯¹å€¼è¿›è¡Œç­›é€‰ ---
    ic_mean_abs = abs(ic_mean)
    ic_ir_abs = abs(ic_ir)
    nw_t_stat_abs = abs(nw_t_stat)

    # èƒœç‡éœ€è¦æ ¹æ®æ–¹å‘é‡æ–°è®¡ç®—ï¼š(IC * æ–¹å‘) > 0 çš„æ¯”ä¾‹
    # å‡è®¾ summary_row é‡Œçš„ win_rate æ˜¯åŸºäºic_meanæ–¹å‘ç®—çš„ï¼Œè¿™é‡Œç›´æ¥ç”¨

    screening_results = {
        'IC Mean': ic_mean,
        'ICIR': ic_ir,
        'NW T-stat': nw_t_stat,
        'Win Rate': win_rate,
        'Factor Direction': int(factor_direction)  # æ–°å¢ï¼šè¾“å‡ºå› å­æ–¹å‘
    }

    if ic_ir_abs < config['min_full_sample_icir_abs']:
        screening_results['failure_reason'] = f"|ICIR| < {config['min_full_sample_icir_abs']}"
        return False, screening_results

    if ic_mean_abs < config['min_full_sample_ic_mean_abs']:
        screening_results['failure_reason'] = f"|IC Mean| < {config['min_full_sample_ic_mean_abs']}"
        return False, screening_results

    if nw_t_stat_abs < config['min_newey_west_t_stat_abs']:
        screening_results['failure_reason'] = f"|NW T-stat| < {config['min_newey_west_t_stat_abs']}"
        return False, screening_results

    if win_rate < config['min_win_rate']:
        screening_results['failure_reason'] = f"Win Rate < {config['min_win_rate']}"
        return False, screening_results

    return True, screening_results


def _generate_factor_profile_v4(
        factor_name: str,
        factor_stats: Dict[str, Dict]
) -> Dict:
    """
    ã€V4 æœ€ç»ˆç‰ˆè¾…åŠ©å‡½æ•°ã€‘ä¸ºä¸€ä¸ªé€šè¿‡ç­›é€‰çš„å› å­ç”Ÿæˆæ·±åº¦ç”»åƒå’Œè¯Šæ–­ç»“è®ºã€‚

    æ ¸å¿ƒæ”¹è¿›ï¼š
    1. å°† 10d å‘¨æœŸçš„æ•°æ®æ•´åˆè¿›çŸ­æœŸæ•ˆåº”çš„è¯Šæ–­é€»è¾‘ä¸­ã€‚
    2. æä¾›æ›´ä¸°å¯Œã€æ›´ç»†è‡´çš„çŸ­æœŸæ•ˆåº”ç”»åƒï¼ˆå¦‚â€œç»å…¸åè½¬åèµ°å¼ºâ€ï¼‰ã€‚
    """
    profile = {
        "å› å­åç§°": factor_name,
        "å†³ç­–æŒ‡æ ‡ (21d)": {},
        "è¾…åŠ©è¯Šæ–­": {},
        "æœ€ç»ˆç”»åƒç»“è®º": "æœ‰å¾…è¯„ä¼°"
    }

    # --- 1. æå–æ‰€æœ‰å‘¨æœŸçš„å…³é”®æŒ‡æ ‡ ---
    icir_dict = {
        p: factor_stats.get(f'{p}d', {}).get("ic_ir", 0)
        for p in [1, 5, 10, 21, 40, 60, 120]
    }

    icir_21d = icir_dict[21]
    profile["å†³ç­–æŒ‡æ ‡ (21d)"]["21d å…¨æ ·æœ¬ICIR"] = f"{icir_21d:.4f} (âœ… å†³ç­–é€šè¿‡)"

    # --- 2. ã€V4ä¿®æ­£ã€‘è¯Šæ–­çŸ­æœŸæ•ˆåº” (Short-term Effect), å¼•å…¥10dæ•°æ® ---
    icir_1d = icir_dict[1]
    icir_5d = icir_dict[5]
    icir_10d = icir_dict[10]

    short_term_diagnosis_text = f"ICIR_1d={icir_1d:.2f}, ICIR_5d={icir_5d:.2f}, ICIR_10d={icir_10d:.2f}"

    # å»ºç«‹æ›´ç²¾ç»†çš„åˆ¤æ–­é€»è¾‘
    if icir_1d < -0.05 and icir_10d > 0.02:
        short_term_conclusion = " (è¯Šæ–­ï¼šç»å…¸çš„çŸ­æœŸåè½¬åèµ°å¼ºï¼Œå½¢æ€éå¸¸å¥åº·)"
    elif icir_1d > 0.1 and icir_5d > 0.1 and icir_10d > 0.05:
        short_term_conclusion = " (âš ï¸ è­¦å‘Šï¼šå­˜åœ¨æŒç»­çš„å¼ºçŸ­æœŸåŠ¨é‡ï¼Œé«˜åº¦ç–‘ä¼¼è¿½é«˜å‹å› å­)"
    elif icir_1d < -0.1 and icir_10d < -0.05:
        short_term_conclusion = " (âš ï¸ è­¦å‘Šï¼šçŸ­æœŸåè½¬æ•ˆåº”è¿‡å¼ºä¸”æŒç»­ï¼Œå¯èƒ½ä¾µèš€ä¸­æœŸä¿¡å·)"
    else:
        short_term_conclusion = " (è¯Šæ–­ï¼šçŸ­æœŸæ•ˆåº”ä¸æ˜æ˜¾æˆ–å½¢æ€ä¸å…¸å‹)"
    profile["è¾…åŠ©è¯Šæ–­"]["çŸ­æœŸæ•ˆåº” (1d, 5d, 10d)"] = short_term_diagnosis_text + short_term_conclusion

    # --- 3. è¯Šæ–­ä¿¡å·æŒä¹…æ€§ (IC Decay) ---
    abs_icir_21d = abs(icir_21d)
    benchmark_icir = abs_icir_21d if abs_icir_21d > 1e-6 else 0.01

    decay_ratio_40d = abs(icir_dict[40]) / benchmark_icir
    decay_ratio_60d = abs(icir_dict[60]) / benchmark_icir
    decay_ratio_120d = abs(icir_dict[120]) / benchmark_icir

    persistence_diagnosis_text = (f"ICIR_40d={icir_dict[40]:.2f}, "
                                  f"ICIR_60d={icir_dict[60]:.2f}, "
                                  f"ICIR_120d={icir_dict[120]:.2f}")

    if decay_ratio_120d > 0.6:
        persistence_conclusion = " (è¯Šæ–­ï¼šä¿¡å·éå¸¸æŒä¹…ï¼Œè¡°å‡ææ…¢ï¼Œé¡¶çº§é•¿æ•ˆå› å­)"
    elif decay_ratio_60d < 0.3:
        persistence_conclusion = " (è¯Šæ–­ï¼šä¿¡å·åœ¨ä¸­æœŸ(60d)è¡°å‡ä¸¥é‡ï¼Œä¸é€‚åˆé•¿å‘¨æœŸæŒæœ‰)"
    elif decay_ratio_40d < 0.5:
        persistence_conclusion = " (è¯Šæ–­ï¼šä¿¡å·åœ¨åˆæœŸ(40d)è¡°å‡è¾ƒå¿«ï¼Œåå‘ä¸­çŸ­å‘¨æœŸ)"
    else:
        persistence_conclusion = " (è¯Šæ–­ï¼šä¿¡å·æ­£å¸¸è¡°å‡ï¼Œç¬¦åˆä¸­é•¿æœŸå› å­ç‰¹å¾)"
    profile["è¾…åŠ©è¯Šæ–­"]["ä¿¡å·æŒä¹…æ€§ (40d, 60d, 120d)"] = persistence_diagnosis_text + persistence_conclusion

    # --- 4. ã€V4ä¿®æ­£ã€‘å½¢æˆæœ€ç»ˆç»“è®º ---
    final_conclusion = "è¡¨ç°åˆæ ¼çš„ä¸­é•¿æœŸå› å­ï¼Œå¯ä½œä¸ºå¤‡é€‰çº³å…¥åˆæˆæ± ã€‚"  # é»˜è®¤ç»“è®º

    if "é¡¶çº§é•¿æ•ˆå› å­" in persistence_conclusion and "ç»å…¸" in short_term_conclusion:
        final_conclusion = "é¡¶çº§é•¿æ•ˆå› å­ã€‚ä¿¡å·æŒä¹…ä¸”å‘ˆç°å¥åº·çš„â€˜åè½¬åèµ°å¼ºâ€™å½¢æ€ï¼ŒAlphaæ¥æºå¹²å‡€ã€‚å¼ºçƒˆå»ºè®®ä½œä¸ºæ ¸å¿ƒåŸºçŸ³ã€‚"
    elif "è¡°å‡ä¸¥é‡" in persistence_conclusion or "è¡°å‡è¾ƒå¿«" in persistence_conclusion:
        final_conclusion = "ä¸­çŸ­å‘¨æœŸå› å­ã€‚è™½ç„¶é€šè¿‡äº†21dç­›é€‰ï¼Œä½†å…¶é•¿æœŸæœ‰æ•ˆæ€§å­˜ç–‘ï¼Œåœ¨æœˆåº¦è°ƒä»“ç­–ç•¥ä¸­éœ€è°¨æ…ä½¿ç”¨æˆ–ä½é…ã€‚"
    elif "è­¦å‘Šï¼šå­˜åœ¨æŒç»­çš„å¼ºçŸ­æœŸåŠ¨é‡" in short_term_conclusion:
        final_conclusion = "å¯èƒ½è¢«åŠ¨é‡æ±¡æŸ“çš„å› å­ã€‚å…¶Alphaæ¥æºä¸çº¯ç²¹ï¼Œç¨³å®šæ€§é£é™©è¾ƒé«˜ï¼Œå»ºè®®è¿›ä¸€æ­¥åšå‰¥ç¦»åˆ†ææˆ–ç›´æ¥æ”¾å¼ƒã€‚"
    elif "è­¦å‘Šï¼šçŸ­æœŸåè½¬æ•ˆåº”è¿‡å¼ºä¸”æŒç»­" in short_term_conclusion:
        final_conclusion = "çŸ­æœŸåè½¬ç‰¹å¾è¿‡å¼ºï¼Œé£é™©è¾ƒé«˜ã€‚è™½ç„¶21dè¡¨ç°åˆæ ¼ï¼Œä½†å¯èƒ½ä¾µèš€äº†éƒ¨åˆ†ä¸­æœŸæ”¶ç›Šï¼Œéœ€è°¨æ…è¯„ä¼°ã€‚"

    profile["æœ€ç»ˆç”»åƒç»“è®º"] = final_conclusion
    profile['ic_ä¸åŒå‘¨æœŸè¡¨ç°'] = show_diff_period_ic(factor_stats)

    return profile

def show_diff_period_ic(factor_stats):
    ic_dict = {
        p: round(factor_stats.get(f'{p}d', {}).get("ic_mean", 0), 3)
        for p in [1, 5, 10, 21, 40, 60, 120]
    }
    return ic_dict

# --- 1. å®šä¹‰ä¸€ä¸ªæ›´å…¨é¢çš„ã€å¤šç»´åº¦çš„ç­›é€‰é…ç½® ---
PHASE1_CONFIG_V3 = {
    'decision_period': 21,
    'min_icir_abs': 0.32,
    'min_ic_mean_abs': 0.02,
    'min_nw_t_stat_abs': 1.96
}


def profile_elite_factors(
        all_factors_summary: Dict[str, Dict],
        config: Dict = None
) -> Dict[str, Dict]:
    """
    """
    if config is None:
        config = PHASE1_CONFIG_V3
    decision_period_str = f"{config['decision_period']}d"

    print(
        f"ç­›é€‰æ ‡å‡†: |ICIR| >= {config['min_icir_abs']}, |IC Mean| >= {config['min_ic_mean_abs']}, |T-stat| >= {config['min_nw_t_stat_abs']}")

    factor_profiles = {}

    for factor_name, factor_stats in all_factors_summary.items():
        print(f"\næ­£åœ¨è¯„ä¼°å› å­: {factor_name}...")

        stats_for_decision = factor_stats.get(decision_period_str)

        if not stats_for_decision:
            print(f"  > âŒ ç­›é€‰å¤±è´¥: ç¼ºå°‘å†³ç­–å‘¨æœŸ {decision_period_str} çš„ç»Ÿè®¡æ•°æ®ã€‚")
            continue

        ic_mean = stats_for_decision.get('ic_mean', 0)
        ic_ir = stats_for_decision.get('ic_ir', 0)
        nw_t_stat = stats_for_decision.get('ic_t_stat', 0)#ä¸‹æ­¤åˆ‡æ¢ä»ic_nw_t_stat todo

        # --- æ‰§è¡Œâ€œä¸‰é“é˜²ç«å¢™â€æ£€éªŒ ---
        passed_effectiveness = abs(ic_mean) >= config['min_ic_mean_abs']
        passed_stability = abs(ic_ir) >= config['min_icir_abs']
        passed_significance = abs(nw_t_stat) >= config['min_nw_t_stat_abs']

        if passed_effectiveness and passed_stability and passed_significance:
            print(
                f"  > âœ… é€šè¿‡æ‰€æœ‰ç­›é€‰ (|IC Mean|={abs(ic_mean):.4f}, |ICIR|={abs(ic_ir):.4f}, |T-stat|={abs(nw_t_stat):.2f})")

            # å¯¹é€šè¿‡çš„å› å­è¿›è¡Œæ·±åº¦ç”»åƒ
            profile = _generate_factor_profile_v4(factor_name, factor_stats)
            factor_profiles[factor_name] = profile
        else:
            # æä¾›æ›´è¯¦ç»†çš„å¤±è´¥åŸå› 
            failure_reasons = []
            if not passed_effectiveness: failure_reasons.append(f"æœ‰æ•ˆæ€§ä¸è¶³(|IC Mean|={abs(ic_mean):.4f})")
            if not passed_stability: failure_reasons.append(f"ç¨³å®šæ€§ä¸è¶³(|ICIR|={abs(ic_ir):.4f})")
            if not passed_significance: failure_reasons.append(f"æ˜¾è‘—æ€§ä¸è¶³(|T-stat|={abs(nw_t_stat):.2f})")
            print(f"  > âŒ ç­›é€‰å¤±è´¥: {', '.join(failure_reasons)}ã€‚")

    print("\n" + "=" * 50)
    print(f"ç­›é€‰å®Œæˆ! å…± {len(factor_profiles)} ä¸ªå› å­è¿›å…¥ç²¾è‹±æ± ã€‚")
    print("=" * 50)
    return factor_profiles


#
# def profile_elite_factors(
#         all_factors_summary: Dict[str, Dict],
#         decision_period: int = 21,
#         icir_threshold: float = 0.4
# ) -> Dict[str, Dict]:
#     """
#     ã€V2ä¿®æ­£ç‰ˆä¸»å‡½æ•°ã€‘æ‰§è¡Œä¸¤æ­¥èµ°çš„å› å­ç­›é€‰å’Œç”»åƒæµç¨‹ã€‚
#
#     æ ¸å¿ƒä¿®æ­£ï¼š
#     1. åŸºäºICIRçš„ã€ç»å¯¹å€¼ã€‘è¿›è¡Œç¡¬æ€§é—¨æ§›ç­›é€‰ï¼Œä»¥è¯†åˆ«æ­£å‘å’Œåå‘å› å­ã€‚
#     2. ç¡®ä¿ä»åµŒå¥—å­—å…¸ä¸­æ­£ç¡®æå–ic_irå€¼ã€‚
#     """
#     print(f"--- å¼€å§‹å› å­ç­›é€‰ä¸ç”»åƒ (V2ç‰ˆ-æ–¹å‘ä¸­æ€§) | å†³ç­–å‘¨æœŸ: {decision_period}d | |ICIR|é—¨æ§›: {icir_threshold} ---")
#     factor_profiles = {}
#
#     for factor_name, factor_stats in all_factors_summary.items():
#         print(f"\næ­£åœ¨è¯„ä¼°å› å­: {factor_name}...")
#
#         # --- ç¬¬ä¸€æ­¥ï¼šç¡¬æ€§é—¨æ§›ç­›é€‰ ---
#         decision_key = f'{decision_period}d'
#         icir_for_decision = factor_stats.get(decision_key)
#
#         if not icir_for_decision:
#             print(f"  > âŒ ç­›é€‰å¤±è´¥: ç¼ºå°‘å†³ç­–å‘¨æœŸ {decision_key} çš„ç»Ÿè®¡æ•°æ®ã€‚")
#             continue
#
#
#         # --- ã€æ ¸å¿ƒé€»è¾‘ä¿®æ­£ã€‘ ---
#         # åŸºäºICIRçš„ç»å¯¹å€¼è¿›è¡Œåˆ¤æ–­
#         if abs(icir_for_decision) >= icir_threshold:
#             print(f"  > âœ… é€šè¿‡ç¡¬æ€§ç­›é€‰ (|ICIR|={abs(icir_for_decision):.4f})")
#
#             # --- ç¬¬äºŒæ­¥ï¼šå¯¹é€šè¿‡ç­›é€‰çš„å› å­ï¼Œè¿›è¡Œâ€œæ·±åº¦ç”»åƒâ€ ---
#             # _generate_factor_profile_v2 å‡½æ•°èƒ½æ­£ç¡®å¤„ç†æ­£è´ŸICIRå¹¶ç»™å‡ºç”»åƒ
#             profile = _generate_factor_profile_v2(factor_name, factor_stats)
#             factor_profiles[factor_name] = profile
#         else:
#             print(f"  > âŒ ç­›é€‰å¤±è´¥: |{decision_key} ICIR| ({abs(icir_for_decision):.4f}) æœªè¾¾åˆ°é—¨æ§› {icir_threshold}ã€‚")
#
#     print("\n" + "=" * 50)
#     print(f"ç­›é€‰å®Œæˆ! å…± {len(factor_profiles)} ä¸ªå› å­è¿›å…¥ç²¾è‹±æ± ã€‚")
#     print("=" * 50)
#     return factor_profiles


def _normalize_score(value: float, worse_val: float, best_val: float) -> float:
    """ä¸€ä¸ªç®€å•çš„çº¿æ€§è¯„åˆ†å‡½æ•°ï¼Œå°†æŒ‡æ ‡å€¼æ˜ å°„åˆ°50-100åˆ†ã€‚"""
    # å¤„ç†åå‘æŒ‡æ ‡çš„æƒ…å†µï¼Œç¡®ä¿ best_val æ€»æ˜¯è¾ƒå¤§çš„é‚£ä¸ª
    if worse_val > best_val:
        value, worse_val, best_val = -value, -worse_val, -best_val

    if value >= best_val: return 100.0
    if value <= worse_val: return 50.0
    return 50 + 50 * (value - worse_val) / (best_val - worse_val)


def score_factor_from_stats_for_21d(
        factor_data: Dict,
        config: Dict = None
) -> Dict:
    """
    ã€å®šåˆ¶ç‰ˆå› å­æ‰“åˆ†å‡½æ•°ã€‘
    æ ¹æ®ä½ æä¾›çš„ç‰¹å®šæ•°æ®ç»“æ„ï¼Œå¯¹ä¸€ä¸ªå·²ç»é€šè¿‡ç¡¬æ€§ç­›é€‰çš„ç²¾è‹±å› å­è¿›è¡Œå¤šç»´åº¦æ‰“åˆ†ã€‚
    """
    # --- 1. å®šä¹‰è¯„åˆ†æ ‡å‡†å’Œæƒé‡ ---
    if config is None:
        config = {
            'power': {'worse': 0.02, 'best': 0.06},
            'stability_icir': {'worse': 0.3, 'best': 0.8},
            'stability_tstat': {'worse': 1.96, 'best': 3.0},
            'character_decay': {'worse': 0.2, 'best': 0.8},
            'cost_turnover': {'worse': 1.5, 'best': 0.5},  # ä¿®æ­£ï¼šturnover_mean çš„é‡çº§æ˜¯1.5å·¦å³
            'weights': {'power': 0.4, 'stability': 0.3, 'character': 0.15, 'cost': 0.15},
            'character_weights': {'decay': 0.7, 'reversal': 0.3}
        }

    # --- 2. ä»ä½ çš„æ•°æ®ç»“æ„ä¸­ï¼Œå®‰å…¨åœ°æå–æ‰€æœ‰éœ€è¦çš„åŸå§‹æŒ‡æ ‡ ---
    ic_stats = factor_data.get('ic_analysis_processed', {})
    turnover_stats = factor_data.get('turnover', {})

    stats_1d = ic_stats.get('1d', {})
    stats_21d = ic_stats.get('21d', {})
    stats_120d = ic_stats.get('120d', {})
    turnover_21d = turnover_stats.get('21d', {})

    ic_mean_21d = stats_21d.get('ic_mean', 0)
    ic_ir_21d = stats_21d.get('ic_ir', 0)
    # é€‚é…ï¼šä½¿ç”¨ä½ æ•°æ®ä¸­å·²æœ‰çš„ ic_t_stat
    t_stat_21d = stats_21d.get('ic_t_stat', 0)

    ic_ir_1d = stats_1d.get('ic_ir', 0)
    ic_ir_120d = stats_120d.get('ic_ir', 0)

    # é€‚é…ï¼šä½¿ç”¨21då‘¨æœŸçš„ turnover_mean
    avg_monthly_turnover = 0.5 # todo æ³¨æ„åç»­é‡è·‘æ•°æ® æ³¨é‡Šå–æ¶ˆæ‰ turnover_21d.get('turnover_mean', 1.0)  # å¦‚æœç¼ºå¤±ï¼Œç»™ä¸€ä¸ªä¸­æ€§åé«˜çš„æƒ©ç½šå€¼

    # --- 3. è®¡ç®—å„ç»´åº¦å¾—åˆ† ---

    # ç»´åº¦ä¸€ï¼šæ ¸å¿ƒé¢„æµ‹èƒ½åŠ›
    power_score = _normalize_score(abs(ic_mean_21d), config['power']['worse'], config['power']['best'])

    # ç»´åº¦äºŒï¼šä¿¡å·ç¨³å®šæ€§
    stability_icir_score = _normalize_score(abs(ic_ir_21d), config['stability_icir']['worse'],
                                            config['stability_icir']['best'])
    stability_tstat_score = _normalize_score(abs(t_stat_21d), config['stability_tstat']['worse'],
                                             config['stability_tstat']['best'])
    stability_score = (stability_icir_score + stability_tstat_score) / 2

    # ç»´åº¦ä¸‰ï¼šå› å­â€œå“æ ¼â€
    benchmark_icir_abs = abs(ic_ir_21d) if abs(ic_ir_21d) > 1e-6 else 1.0
    decay_ratio = abs(ic_ir_120d) / benchmark_icir_abs
    decay_score = _normalize_score(decay_ratio, config['character_decay']['worse'], config['character_decay']['best'])

    reversal_score = 90.0 if ic_ir_1d < 0 else (70.0 if ic_ir_1d <= 0.1 else 30.0)
    character_score = (decay_score * config['character_weights']['decay'] +
                       reversal_score * config['character_weights']['reversal'])

    # ç»´åº¦å››ï¼šäº¤æ˜“æˆæœ¬ (åå‘æŒ‡æ ‡)
    cost_score = _normalize_score(avg_monthly_turnover, config['cost_turnover']['worse'],
                                  config['cost_turnover']['best'])

    # --- 4. è®¡ç®—æœ€ç»ˆæ€»åˆ† ---
    final_score = (power_score * config['weights']['power'] +
                   stability_score * config['weights']['stability'] +
                   character_score * config['weights']['character'] +
                   cost_score * config['weights']['cost'])

    return {
        'Final_Score': round(final_score, 2),
        'Power_Score': round(power_score, 2),
        'Stability_Score': round(stability_score, 2),
        'Character_Score': round(character_score, 2),
        'Cost_Score': round(cost_score, 2)
    }


if __name__ == '__main__':
    config = SelectionConfig() #è‡ªå·±æ­é…ï¼
    x = FactorSelector('20250906_045625_05e460ab',config)
    print(1)
    #
    #
    TARGET_UNIVERSE = INDEX_CODES['ZZ800']  # ä»¥ä¸­è¯300ä¸ºä¸»æˆ˜åœº
    # TARGET_UNIVERSE = INDEX_CODES['ZZ500']  # ä»¥ä¸­è¯1000ä¸ºä¸»æˆ˜åœº
    # TARGET_UNIVERSE = INDEX_CODES['ZZ800']  # ä»¥ä¸­è¯1000ä¸ºä¸»æˆ˜åœº
    #
    # selector.run_factor_analysis(
    #     TARGET_STOCK_POOL=TARGET_UNIVERSE,
    #     top_n_final=400,
    #     correlation_threshold=0.0,
    #     run_version='latest'
    # )
    # get_base_passed_factors(TARGET_UNIVERSE)

    x.run_complete_selection(TARGET_UNIVERSE)