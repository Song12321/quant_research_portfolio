"""
å¹¶å‘æ‰§è¡Œå™¨ - ç”¨äºæ‰¹é‡å› å­è®¡ç®—çš„é«˜æ•ˆå¹¶å‘å¤„ç†

æ”¯æŒåŠŸèƒ½ï¼š
1. å¤šçº¿ç¨‹å¹¶å‘æ‰§è¡Œ
2. è¿›åº¦ç›‘æ§å’Œæ—¥å¿—è®°å½•
3. å¼‚å¸¸å¤„ç†å’Œé‡è¯•æœºåˆ¶
4. èµ„æºç®¡ç†å’Œå†…å­˜æ§åˆ¶
5. å¯é…ç½®çš„å¹¶å‘å‚æ•°
"""

import threading
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Callable, Any, Dict, Optional, Tuple
from dataclasses import dataclass
from pathlib import Path
import logging
from quant_lib.config.logger_config import setup_logger

logger = setup_logger(__name__)


@dataclass
class ConcurrentConfig:
    """å¹¶å‘é…ç½®"""
    max_workers: int = 4  # æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°
    chunk_size: int = 1  # æ¯ä¸ªä»»åŠ¡çš„æ‰¹æ¬¡å¤§å°
    timeout: Optional[float] = 60000  # å•ä¸ªä»»åŠ¡è¶…æ—¶æ—¶é—´(ç§’)
    retry_count: int = 1  # å¤±è´¥é‡è¯•æ¬¡æ•°
    log_interval: int = 10  # è¿›åº¦æ—¥å¿—é—´éš”(ç§’)


class ConcurrentExecutor:
    """é€šç”¨å¹¶å‘æ‰§è¡Œå™¨"""
    
    def __init__(self, config: Optional[ConcurrentConfig] = None):
        self.config = config or ConcurrentConfig()
        self._completed_count = 0
        self._failed_count = 0
        self._total_count = 0
        self._lock = threading.Lock()
        self._start_time = None
        
    def execute_batch(
        self,
        target_function: Callable,
        task_list: List[Any],
        task_name: str = "æ‰¹é‡ä»»åŠ¡",
        **kwargs
    ) -> Tuple[List[Any], List[Tuple[Any, Exception]]]:
        """
        æ‰¹é‡å¹¶å‘æ‰§è¡Œä»»åŠ¡
        
        Args:
            target_function: ç›®æ ‡æ‰§è¡Œå‡½æ•°
            task_list: ä»»åŠ¡å‚æ•°åˆ—è¡¨
            task_name: ä»»åŠ¡åç§°(ç”¨äºæ—¥å¿—)
            **kwargs: ä¼ é€’ç»™ç›®æ ‡å‡½æ•°çš„é¢å¤–å‚æ•°
            
        Returns:
            (successful_results, failed_tasks): æˆåŠŸç»“æœåˆ—è¡¨å’Œå¤±è´¥ä»»åŠ¡åˆ—è¡¨
        """
        self._total_count = len(task_list)
        self._completed_count = 0
        self._failed_count = 0
        self._start_time = time.time()
        
        logger.info(f"ğŸš€ å¼€å§‹{task_name}ï¼Œå…±{self._total_count}ä¸ªä»»åŠ¡ï¼Œå¹¶å‘åº¦{self.config.max_workers}")
        
        successful_results = []
        failed_tasks = []
        
        # å¯åŠ¨è¿›åº¦ç›‘æ§çº¿ç¨‹
        progress_thread = threading.Thread(target=self._progress_monitor, daemon=True)
        progress_thread.start()
        
        try:
            with ThreadPoolExecutor(max_workers=self.config.max_workers) as executor:
                # æäº¤æ‰€æœ‰ä»»åŠ¡
                future_to_task = {
                    executor.submit(self._execute_with_retry, target_function, task, **kwargs): task
                    for task in task_list
                }
                
                # æ”¶é›†ç»“æœ
                for future in as_completed(future_to_task, timeout=self.config.timeout * len(task_list)):
                    task = future_to_task[future]
                    
                    try:
                        result = future.result()
                        successful_results.append(result)
                        
                        with self._lock:
                            self._completed_count += 1
                            
                    except Exception as e:
                        failed_tasks.append((task, e))
                        logger.error(f"ä»»åŠ¡å¤±è´¥: {task} - {e}")
                        
                        with self._lock:
                            self._failed_count += 1
                            
        except Exception as e:
            logger.error(f"æ‰¹é‡æ‰§è¡Œå¼‚å¸¸: {e}")
            
        finally:
            # æœ€ç»ˆç»Ÿè®¡
            elapsed_time = time.time() - self._start_time
            success_rate = (self._completed_count / self._total_count) * 100 if self._total_count > 0 else 0
            
            logger.info(f"âœ… {task_name}å®Œæˆ")
            logger.info(f"ğŸ“Š æˆåŠŸ: {self._completed_count}, å¤±è´¥: {self._failed_count}, æˆåŠŸç‡: {success_rate:.1f}%")
            logger.info(f"â±ï¸ æ€»è€—æ—¶: {elapsed_time:.1f}ç§’, å¹³å‡æ¯ä»»åŠ¡: {elapsed_time/self._total_count:.1f}ç§’")
            
        return successful_results, failed_tasks
    
    def _execute_with_retry(self, target_function: Callable, task: Any, **kwargs) -> Any:
        """å¸¦é‡è¯•æœºåˆ¶çš„ä»»åŠ¡æ‰§è¡Œ"""
        last_exception = None
        
        for attempt in range(self.config.retry_count + 1):
            try:
                return target_function(task, **kwargs)
                
            except Exception as e:
                last_exception = e
                if attempt < self.config.retry_count:
                    logger.warning(f"ä»»åŠ¡ {task} ç¬¬{attempt+1}æ¬¡å°è¯•å¤±è´¥ï¼Œå‡†å¤‡é‡è¯•: {e}")
                    time.sleep(0.5 * (attempt + 1))  # é€’å¢å»¶è¿Ÿ
                    
        raise last_exception
    
    def _progress_monitor(self):
        """è¿›åº¦ç›‘æ§çº¿ç¨‹"""
        while self._completed_count + self._failed_count < self._total_count:
            time.sleep(self.config.log_interval)
            
            with self._lock:
                completed = self._completed_count
                failed = self._failed_count
                progress = ((completed + failed) / self._total_count) * 100
                elapsed = time.time() - self._start_time
                
            logger.info(f"ğŸ“ˆ è¿›åº¦: {completed}/{self._total_count} ({progress:.1f}%), "
                       f"å¤±è´¥: {failed}, è€—æ—¶: {elapsed:.0f}ç§’")


class FactorCalculationExecutor(ConcurrentExecutor):
    """å› å­è®¡ç®—ä¸“ç”¨å¹¶å‘æ‰§è¡Œå™¨"""
    
    def __init__(self, config: Optional[ConcurrentConfig] = None):
        # å› å­è®¡ç®—é€šå¸¸CPUå¯†é›†ï¼Œé€‚å½“å‡å°‘å¹¶å‘æ•°
        if config is None:
            config = ConcurrentConfig(
                max_workers=3,  # å› å­è®¡ç®—CPUå¯†é›†ï¼Œä¸å®œè¿‡å¤šçº¿ç¨‹
                timeout=12000,   # å› å­è®¡ç®—å¯èƒ½è¾ƒè€—æ—¶
                retry_count=2   # å¢åŠ é‡è¯•æ¬¡æ•°
            )
        super().__init__(config)
    
    def execute_factor_batch(
        self,
        factor_names: List[str],
        snapshot_config_id: str,
        target_function: Callable = None
    ) -> Tuple[List[Any], List[Tuple[str, Exception]]]:
        """
        æ‰¹é‡æ‰§è¡Œå› å­è®¡ç®—
        
        Args:
            factor_names: å› å­åç§°åˆ—è¡¨
            snapshot_config_id: å¿«ç…§é…ç½®ID
            target_function: ç›®æ ‡å‡½æ•°(é»˜è®¤ä¸ºrolling_icè®¡ç®—)
            
        Returns:
            (successful_results, failed_factors): æˆåŠŸç»“æœå’Œå¤±è´¥å› å­
        """
        if target_function is None:
            from projects._03_factor_selection.factor_manager.ic_manager.rolling_ic_manager import \
                run_cal_and_save_rolling_ic_by_snapshot_config_id
            target_function = run_cal_and_save_rolling_ic_by_snapshot_config_id
        
        logger.info(f"ğŸ”¬ å¼€å§‹æ‰¹é‡å› å­è®¡ç®—: {len(factor_names)}ä¸ªå› å­")
        logger.info(f"ğŸ“‹ å¿«ç…§é…ç½®ID: {snapshot_config_id}")
        
        # åŒ…è£…æ‰§è¡Œå‡½æ•°
        def execute_single_factor(factor_name: str):
            logger.info(f"ğŸ§® å¼€å§‹è®¡ç®—å› å­: {factor_name}")
            result = target_function(snapshot_config_id, [factor_name])
            logger.info(f"âœ… å› å­ {factor_name} è®¡ç®—å®Œæˆ")
            return result
        
        return self.execute_batch(
            target_function=execute_single_factor,
            task_list=factor_names,
            task_name="å› å­ICè®¡ç®—"
        )
    
    def execute_chunked_factors(
        self,
        factor_names: List[str],
        snapshot_config_id: str,
        chunk_size: int = 3,
        target_function: Callable = None
    ) -> Tuple[List[Any], List[Tuple[List[str], Exception]]]:
        """
        åˆ†ç»„å¹¶å‘æ‰§è¡Œå› å­è®¡ç®—(é€‚åˆå†…å­˜å¯†é›†å‹ä»»åŠ¡)
        
        Args:
            factor_names: å› å­åç§°åˆ—è¡¨
            snapshot_config_id: å¿«ç…§é…ç½®ID
            chunk_size: æ¯ç»„å› å­æ•°é‡
            target_function: ç›®æ ‡å‡½æ•°
            
        Returns:
            (successful_results, failed_chunks): æˆåŠŸç»“æœå’Œå¤±è´¥åˆ†ç»„
        """
        if target_function is None:
            from projects._03_factor_selection.factor_manager.ic_manager.rolling_ic_manager import \
                run_cal_and_save_rolling_ic_by_snapshot_config_id
            target_function = run_cal_and_save_rolling_ic_by_snapshot_config_id
        
        # åˆ†ç»„
        factor_chunks = [
            factor_names[i:i + chunk_size]
            for i in range(0, len(factor_names), chunk_size)
        ]
        
        logger.info(f"ğŸ“¦ å› å­åˆ†ç»„: {len(factor_chunks)}ç»„ï¼Œæ¯ç»„æœ€å¤š{chunk_size}ä¸ªå› å­")
        
        # åŒ…è£…æ‰§è¡Œå‡½æ•°
        def execute_factor_chunk(factor_chunk: List[str]):
            logger.info(f"ğŸ¯ å¼€å§‹è®¡ç®—å› å­ç»„: {factor_chunk}")
            result = target_function(snapshot_config_id, factor_chunk)
            logger.info(f"âœ… å› å­ç»„è®¡ç®—å®Œæˆ: {len(factor_chunk)}ä¸ªå› å­")
            return result
        
        return self.execute_batch(
            target_function=execute_factor_chunk,
            task_list=factor_chunks,
            task_name="åˆ†ç»„å› å­ICè®¡ç®—"
        )


# ä¾¿æ·å‡½æ•°
def run_concurrent_factors(
    factor_names: List[str],
    snapshot_config_id: str,
    max_workers: int = 3,
    execution_mode: str = "single"  # "single" æˆ– "chunked"
) -> Tuple[List[Any], List[Tuple[Any, Exception]]]:
    """
    ä¾¿æ·çš„å¹¶å‘å› å­è®¡ç®—å‡½æ•°
    
    Args:
        factor_names: å› å­åç§°åˆ—è¡¨
        snapshot_config_id: å¿«ç…§é…ç½®ID
        max_workers: æœ€å¤§å¹¶å‘æ•°
        execution_mode: æ‰§è¡Œæ¨¡å¼ ("single": å•å› å­å¹¶å‘, "chunked": åˆ†ç»„å¹¶å‘)
        
    Returns:
        (successful_results, failed_tasks): æˆåŠŸç»“æœå’Œå¤±è´¥ä»»åŠ¡
    """
    config = ConcurrentConfig(max_workers=max_workers)
    executor = FactorCalculationExecutor(config)
    
    if execution_mode == "single":
        return executor.execute_factor_batch(factor_names, snapshot_config_id)
    elif execution_mode == "chunked":
        return executor.execute_chunked_factors(factor_names, snapshot_config_id, chunk_size=3)
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ‰§è¡Œæ¨¡å¼: {execution_mode}")


if __name__ == "__main__":
    # ç¤ºä¾‹ç”¨æ³•
    import pandas as pd
    
    # è¯»å–å› å­åˆ—è¡¨
    # df = pd.read_csv(r'D:\lqs\codeAbout\py\Quantitative\quant_research_portfolio\projects\_03_factor_selection\factor_manager\selector\o2o_v3.csv')
    # factor_names = df['factor_name'].unique().tolist()
    factor_names = ['vwap_deviation_20d','roe_change_q','roa_ttm','turnover_rate',  'large_trade_ratio_10d','beta','revenue_growth_ttm']
    #
    snapshot_config_id = '20250906_045625_05e460ab'
    
    # # æ–¹å¼1: å•å› å­å¹¶å‘
    # logger.info("=== å•å› å­å¹¶å‘æ¨¡å¼ ===")
    # successful, failed = run_concurrent_factors(
    #     factor_names=factor_names[:6],  # æµ‹è¯•å‰10ä¸ªå› å­
    #     snapshot_config_id=snapshot_config_id,
    #     max_workers=6,
    #     execution_mode="single"
    # )
    
    # æ–¹å¼2: åˆ†ç»„å¹¶å‘
    logger.info("=== åˆ†ç»„å¹¶å‘æ¨¡å¼ ===")
    successful, failed = run_concurrent_factors(
        factor_names=factor_names,
        snapshot_config_id=snapshot_config_id,
        max_workers=3,
        execution_mode="chunked"
    )