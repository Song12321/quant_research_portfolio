"""
ICåŠ æƒå› å­åˆæˆå™¨ - ä¸“ä¸šçº§å› å­åˆæˆå¼•æ“

æ ¸å¿ƒåŠŸèƒ½ï¼š
1. åŸºäºå†å²ICè¡¨ç°çš„æ™ºèƒ½æƒé‡åˆ†é…
2. å¤šç»´åº¦å› å­ç­›é€‰æœºåˆ¶
3. åŠ¨æ€æƒé‡è°ƒæ•´
4. é£é™©æ§åˆ¶å’Œç¨³å¥æ€§æ£€éªŒ

è®¾è®¡ç†å¿µï¼š
- ä»¥å®ç›˜ç›ˆåˆ©ä¸ºç»ˆæç›®æ ‡
- å¹³è¡¡å› å­é¢„æµ‹èƒ½åŠ›ä¸ç¨³å®šæ€§
- é˜²æ­¢è¿‡æ‹Ÿåˆï¼Œæ³¨é‡æ³›åŒ–èƒ½åŠ›
"""

import pandas as pd
from numpy.linalg import LinAlgError
import numpy as np
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass
from pathlib import Path
import json
import statsmodels.api as sm
from sklearn.linear_model import LinearRegression

from projects._03_factor_selection.config_manager.factor_direction_config import get_new_factor_direction
from projects._03_factor_selection.data_manager.data_manager import DataManager
from projects._03_factor_selection.factor_manager.factor_manager import FactorManager
from projects._03_factor_selection.factor_manager.selector.factor_selector import FactorSelector
from projects._03_factor_selection.factor_manager.storage.result_load_manager import ResultLoadManager
from projects._03_factor_selection.factor_manager.ic_manager.rolling_ic_manager import (
    ICCalculationConfig, run_cal_and_save_rolling_ic_by_snapshot_config_id
)
from projects._03_factor_selection.factor_manager.selector.rolling_ic_factor_selector import (
    RollingICFactorSelector, RollingICSelectionConfig
)
from projects._03_factor_selection.config_manager.config_snapshot.config_snapshot_manager import ConfigSnapshotManager
from projects._03_factor_selection.utils.factor_processor import FactorProcessor
from quant_lib.config.logger_config import setup_logger

logger = setup_logger(__name__)


@dataclass
class FactorWeightingConfig:
    """å› å­æƒé‡é…ç½®"""
    # ICç­›é€‰æ ‡å‡†
    min_ic_mean: float = 0.015  # æœ€å°ICå‡å€¼é˜ˆå€¼
    min_ic_ir: float = 0.183  # æœ€å°ICä¿¡æ¯æ¯”ç‡é˜ˆå€¼
    min_ic_win_rate: float = 0.52  # æœ€å°ICèƒœç‡é˜ˆå€¼
    max_ic_p_value: float = 0.10  # æœ€å¤§ICæ˜¾è‘—æ€§på€¼

    # æƒé‡è®¡ç®—å‚æ•°
    ic_decay_halflife: int = 60  # ICæƒé‡è¡°å‡åŠè¡°æœŸ(å¤©)
    max_single_weight: float = 0.5  # å•ä¸ªå› å­æœ€å¤§æƒé‡
    min_single_weight: float = 0.05  # å•ä¸ªå› å­æœ€å°æƒé‡

    # é£é™©æ§åˆ¶
    max_factors_count: int = 8  # æœ€å¤§å› å­æ•°é‡
    correlation_threshold: float = 0.70  # å› å­é—´ç›¸å…³æ€§é˜ˆå€¼

    # å›çœ‹æœŸè®¾ç½®
    lookback_periods: List[str] = None  # ICè®¡ç®—å‘¨æœŸ
    # æ­£äº¤åŒ–æµç¨‹çš„ç¨³å¥æ€§æ§åˆ¶å‚æ•°
    min_orthogonalization_obs: int = 30  # æ­£äº¤åŒ–å›å½’æ—¶è¦æ±‚çš„æœ€å°æ ·æœ¬é‡
    orthogonalization_x_std_eps: float = 1e-6  # åˆ¤æ–­åŸºå‡†å› å­æ ‡å‡†å·®æ˜¯å¦è¿‡å°çš„é˜ˆå€¼
    #ä½¿ç”¨åœºæ™¯:_adjust_ic_stats_by_r_squared ç”¨äºè°ƒæ•´æ­£äº¤åŒ– ic
    main_evaluation_period = '5'

    def __post_init__(self):
        if self.lookback_periods is None:
            self.lookback_periods = ['5d', '21d']


@dataclass
class FactorQualityReport:
    """å› å­è´¨é‡æŠ¥å‘Š"""
    factor_name: str
    ic_stats: Dict[str, float]
    weight: float
    selection_reason: str
    risk_flags: List[str]


class ICWeightCalculator:
    """ICæƒé‡è®¡ç®—å™¨ - æ ¸å¿ƒç®—æ³•å¼•æ“"""

    def __init__(self, config: FactorWeightingConfig):
        self.config = config

    def calculate_ic_based_weights(
            self,
            factor_ic_stats: Dict[str, Dict[str, float]]
    ) -> Dict[str, float]:
        """
        åŸºäºICç»Ÿè®¡çš„æ™ºèƒ½æƒé‡åˆ†é…
        
        Args:
            factor_ic_stats: {factor_name: {period: ic_stats_dict}}
            
        Returns:
            Dict[factor_name, weight]: æ ‡å‡†åŒ–åçš„æƒé‡åˆ†é…
        """
        logger.info("ğŸ§® å¼€å§‹è®¡ç®—ICåŠ æƒæƒé‡...")

        # 1. è®¡ç®—ç»¼åˆICå¾—åˆ†
        factor_scores = {}
        for factor_name, periods_stats in factor_ic_stats.items():
            normalize_stats = self._normalize_factor_stats_direction(periods_stats,factor_name)
            score = self._calculate_composite_ic_score(normalize_stats)
            factor_scores[factor_name] = score
            logger.debug(f"  {factor_name}: ç»¼åˆICå¾—åˆ† = {score:.4f}")

        # 2. åŸºäºå¾—åˆ†è®¡ç®—åŸå§‹æƒé‡
        raw_weights = self._convert_scores_to_weights(factor_scores)

        # 3. åº”ç”¨çº¦æŸå’Œæ ‡å‡†åŒ–
        final_weights = self._apply_weight_constraints(raw_weights)

        logger.info(f"âœ… ICæƒé‡è®¡ç®—å®Œæˆï¼Œå…±{len(final_weights)}ä¸ªå› å­è¢«åˆ†é…æƒé‡")
        return final_weights

    def _normalize_factor_stats_direction(self,
                                          raw_stats: Dict,
                                          factor_name: str) -> Dict:
        """
        æ ¹æ®é¢„å®šä¹‰çš„å› å­æ–¹å‘ï¼Œè§„èŒƒåŒ–ICç»Ÿè®¡æ•°æ®ã€‚
        ç¡®ä¿æ‰€æœ‰æŒ‡æ ‡éƒ½å¤„ç†æˆâ€œæ­£å‘â€ï¼šå€¼è¶Šå¤§è¶Šå¥½ã€‚
        """

        direction = get_new_factor_direction(factor_name)

        # å¦‚æœæ˜¯æ­£å‘å› å­ï¼Œæ— éœ€ä»»ä½•æ“ä½œ
        if direction == 1:
            return raw_stats
        logger.debug(f"æ£€æµ‹åˆ°å› å­ {factor_name} ä¸ºè´Ÿå‘å› å­(direction={direction})ï¼Œå¼€å§‹è¿›è¡Œæ–¹å‘è§„èŒƒåŒ–...")

        normalized_stats = {}
        for period, period_stats in raw_stats.items():
            # å¤åˆ¶ä»¥é¿å…ä¿®æ”¹åŸå§‹æ•°æ®
            stats = period_stats.copy()

            # --- æ ¸å¿ƒæ‰­è½¬é€»è¾‘ ---
            # 1. å‡å€¼ç±»æŒ‡æ ‡ï¼šç›´æ¥ä¹˜ä»¥æ–¹å‘ (-1)
            if 'ic_mean' in stats:
                stats['ic_mean'] *= direction
            if 'ic_ir' in stats:
                # IRçš„ç¬¦å·ç”±ICå‡å€¼å†³å®šï¼Œä¹Ÿç›´æ¥æ‰­è½¬
                stats['ic_ir'] *= direction
            if 'ic_t_stat' in stats:
                stats['ic_t_stat'] *= direction

            # 3. ç»å¯¹å€¼/æ–¹å·®ç±»æŒ‡æ ‡ï¼šä¿æŒä¸å˜
            #    ic_std, ic_p_value, ic_count, ic_mean_std, ic_ir_std ç­‰éƒ½æ˜¯è¡¡é‡æ³¢åŠ¨æˆ–ç»Ÿè®¡é‡çš„ï¼Œæ— éœ€æ”¹å˜ã€‚

            normalized_stats[period] = stats

        return normalized_stats
    #ok
    def _calculate_composite_ic_score(self, periods_stats: Dict[str, Dict]) -> float:
        """è®¡ç®—å› å­çš„ç»¼åˆICå¾—åˆ†"""
        period_scores = []

        for period, stats in periods_stats.items():
            if not stats or 'ic_mean' not in stats:
                continue

            # æå–å…³é”®æŒ‡æ ‡
            ic_mean = abs(stats.get('ic_mean', 0))  # ä½¿ç”¨ç»å¯¹å€¼
            ic_ir = abs(stats.get('ic_ir', 0))
            ic_win_rate = stats.get('ic_win_rate', 0.5)
            ic_t_stat = abs(stats.get('ic_t_stat', 0))

            # å¤šç»´åº¦è¯„åˆ†æ¨¡å‹
            # 1. ICå‡å€¼å¾—åˆ† (40%æƒé‡)
            ic_mean_score = min(ic_mean / 0.05, 1.0) * 0.4

            # 2. ICç¨³å®šæ€§å¾—åˆ† (30%æƒé‡) 
            ic_stability_score = min(ic_ir / 0.5, 1.0) * 0.3

            # 3. ICèƒœç‡å¾—åˆ† (20%æƒé‡)
            ic_win_score = max(0, (ic_win_rate - 0.5) * 2) * 0.2

            # 4. ICæ˜¾è‘—æ€§å¾—åˆ† (10%æƒé‡)
            ic_sig_score = min(ic_t_stat / 2.0, 1.0) * 0.1

            period_score = ic_mean_score + ic_stability_score + ic_win_score + ic_sig_score
            period_scores.append(period_score)

        # å¤šå‘¨æœŸå¹³å‡ï¼Œç»™çŸ­æœŸç¨é«˜æƒé‡
        if not period_scores:
            return 0.0

        if len(period_scores) == 1:
            return period_scores[0]
        else:
            # --- æ”¹è¿›çš„åŠ æƒæ–¹æ¡ˆ ---
            # ä½¿ç”¨æŒ‡æ•°è¡°å‡æƒé‡ï¼Œç»™çŸ­æœŸæ›´é«˜æƒé‡ï¼Œä½†ä¾ç„¶è€ƒè™‘æ‰€æœ‰å‘¨æœŸ
            # decay_rate è¶Šå°ï¼Œæƒé‡è¡°å‡è¶Šæ…¢
            decay_rate = 0.75
            weights = np.array([decay_rate ** i for i in range(len(period_scores))])
            weights /= weights.sum()  # æƒé‡å½’ä¸€åŒ–

            logger.debug(f"  å¤šå‘¨æœŸæƒé‡ (ä»1dåˆ°120d): {[f'{w:.2f}' for w in weights]}")
            return np.average(period_scores, weights=weights)

    def _convert_scores_to_weights(self, factor_scores: Dict[str, float]) -> Dict[str, float]:
        """å°†å¾—åˆ†è½¬æ¢ä¸ºæƒé‡"""
        if not factor_scores:
            return {}

        # ä½¿ç”¨ softmax å‡½æ•°å°†å¾—åˆ†è½¬æ¢ä¸ºæƒé‡ï¼Œå¢å¼ºåŒºåˆ†åº¦
        scores = np.array(list(factor_scores.values()))

        # è¿‡æ»¤æ‰å¾—åˆ†è¿‡ä½çš„å› å­
        valid_mask = scores > 0.1
        if not valid_mask.any():
            logger.warning("âš ï¸ æ‰€æœ‰å› å­å¾—åˆ†éƒ½è¿‡ä½ï¼Œä½¿ç”¨ç­‰æƒé‡")
            return {name: 1.0 / len(factor_scores) for name in factor_scores.keys()}

        # å¯¹æœ‰æ•ˆå› å­åº”ç”¨ softmax
        valid_scores = scores[valid_mask]
        valid_names = [name for i, name in enumerate(factor_scores.keys()) if valid_mask[i]]

        # æ¸©åº¦å‚æ•°æ§åˆ¶æƒé‡é›†ä¸­åº¦ï¼Œæ¸©åº¦è¶Šé«˜è¶Šå¹³å‡
        temperature = 2.0
        exp_scores = np.exp(valid_scores / temperature)
        softmax_weights = exp_scores / exp_scores.sum()

        return dict(zip(valid_names, softmax_weights))

    def _apply_weight_constraints(self, raw_weights: Dict[str, float]) -> Dict[str, float]:
        """åº”ç”¨æƒé‡çº¦æŸ"""
        if not raw_weights:
            return {}

        weights = raw_weights.copy()

        # 1. åº”ç”¨å•å› å­æƒé‡ä¸Šä¸‹é™
        for name in weights:
            weights[name] = np.clip(weights[name],
                                    self.config.min_single_weight,
                                    self.config.max_single_weight)

        # 2. é‡æ–°æ ‡å‡†åŒ–
        total_weight = sum(weights.values())
        if total_weight > 0:
            weights = {name: w / total_weight for name, w in weights.items()}

        return weights


class FactorQualityFilter:
    """å› å­è´¨é‡ç­›é€‰å™¨"""

    def __init__(self, config: FactorWeightingConfig):
        self.config = config

    def filter_factors_by_quality(
            self,
            factor_ic_stats: Dict[str, Dict[str, Dict]]
    ) -> Tuple[Dict[str, Dict[str, Dict]], List[FactorQualityReport]]:
        """
        åŸºäºå¤šç»´åº¦è´¨é‡æŒ‡æ ‡ç­›é€‰å› å­
        
        Returns:
            (filtered_factor_stats, quality_reports)
        """
        logger.info("ğŸ” å¼€å§‹å› å­è´¨é‡ç­›é€‰...")

        filtered_stats = {}
        quality_reports = []

        for factor_name, periods_stats in factor_ic_stats.items():
            report = self._evaluate_single_factor(factor_name, periods_stats)
            quality_reports.append(report)

            if self._passes_quality_filter(report):
                filtered_stats[factor_name] = periods_stats
                logger.info(f"âœ… {factor_name}: é€šè¿‡ç­›é€‰ (æƒé‡={report.weight:.3f}) - {report.selection_reason}")
            else:
                logger.info(f"âŒ {factor_name}: æœªé€šè¿‡ç­›é€‰ - {report.selection_reason}")

        logger.info(f"ğŸ“Š ç­›é€‰ç»“æœ: {len(filtered_stats)}/{len(factor_ic_stats)} ä¸ªå› å­é€šè¿‡è´¨é‡æ£€éªŒ")
        return filtered_stats, quality_reports

    def _evaluate_single_factor(self, factor_name: str, periods_stats: Dict) -> FactorQualityReport:
        """è¯„ä¼°å•ä¸ªå› å­è´¨é‡"""
        # è®¡ç®—å…³é”®æŒ‡æ ‡çš„ç»¼åˆè¡¨ç°
        ic_means = []
        ic_irs = []
        ic_win_rates = []
        ic_p_values = []

        for period, stats in periods_stats.items():
            if stats and 'ic_mean' in stats:
                ic_means.append(abs(stats.get('ic_mean', 0)))
                ic_irs.append(stats.get('ic_ir', 0))
                ic_win_rates.append(stats.get('ic_win_rate', 0.5))
                ic_p_values.append(stats.get('ic_p_value', 1.0))

        if not ic_means:
            return FactorQualityReport(
                factor_name=factor_name,
                ic_stats={},
                weight=0.0,
                selection_reason="ç¼ºå°‘æœ‰æ•ˆçš„ICç»Ÿè®¡æ•°æ®",
                risk_flags=["æ•°æ®ä¸è¶³"]
            )

        # ç»¼åˆç»Ÿè®¡ ä¹‹å‰å¯¹ æ¯ä¸ªperiod ç”¨ä¸åŒçš„æ—¶é—´è¿›è¡Œaverï¼Œç°åœ¨å¯¹ä¸åŒçš„periodè¿›è¡Œaver
        avg_ic_mean = np.mean(ic_means)
        avg_ic_ir = np.mean(ic_irs)
        avg_win_rate = np.mean(ic_win_rates)
        min_p_value = min(ic_p_values)

        ic_stats = {
            'avg_ic_mean': avg_ic_mean,
            'avg_ic_ir': avg_ic_ir,
            'avg_win_rate': avg_win_rate,
            'min_p_value': min_p_value
        }

        # è´¨é‡è¯„ä¼°
        risk_flags = []
        selection_reason = ""
        if avg_ic_mean < self.config.min_ic_mean:
            risk_flags.append(f"ICå‡å€¼è¿‡ä½({avg_ic_mean:.3f})")
        if avg_ic_ir < self.config.min_ic_ir:
            risk_flags.append(f"ICä¿¡æ¯æ¯”ç‡è¿‡ä½({avg_ic_ir:.3f})")
        if avg_win_rate < self.config.min_ic_win_rate:
            risk_flags.append(f"ICèƒœç‡è¿‡ä½({avg_win_rate:.3f})")
        if min_p_value > self.config.max_ic_p_value:
            risk_flags.append(f"ICæ˜¾è‘—æ€§ä¸è¶³(p={min_p_value:.3f})")

        if not risk_flags:
            selection_reason = f"é«˜è´¨é‡å› å­ (IC={avg_ic_mean:.3f}, IR={avg_ic_ir:.2f}, èƒœç‡={avg_win_rate:.1%})"
            weight = self._calculate_factor_weight(ic_stats)
        else:
            selection_reason = "; ".join(risk_flags)
            weight = 0.0

        return FactorQualityReport(
            factor_name=factor_name,
            ic_stats=ic_stats,
            weight=weight,
            selection_reason=selection_reason,
            risk_flags=risk_flags
        )

    def _passes_quality_filter(self, report: FactorQualityReport) -> bool:
        """åˆ¤æ–­å› å­æ˜¯å¦é€šè¿‡è´¨é‡ç­›é€‰"""
        return len(report.risk_flags) == 0 and report.weight > 0

    def _calculate_factor_weight(self, ic_stats: Dict[str, float]) -> float:
        """åŸºäºICç»Ÿè®¡è®¡ç®—å› å­åˆæ­¥æƒé‡"""
        # è¿™é‡Œå¯ä»¥å®ç°æ›´å¤æ‚çš„æƒé‡é€»è¾‘
        # ç›®å‰ä½¿ç”¨ç®€å•çš„ç»¼åˆå¾—åˆ†
        ic_mean = ic_stats.get('avg_ic_mean', 0)
        ic_ir = ic_stats.get('avg_ic_ir', 0)
        win_rate = ic_stats.get('avg_win_rate', 0.5)

        # ç»¼åˆå¾—åˆ†
        score = ic_mean * 0.5 + ic_ir * 0.3 + max(0, win_rate - 0.5) * 0.2
        return min(score, 1.0)

from projects._03_factor_selection.factor_manager.factor_composite.factor_synthesizer import FactorSynthesizer
class ICWeightedSynthesizer(FactorSynthesizer):
    """ICåŠ æƒå› å­åˆæˆå™¨ - ç»§æ‰¿å¹¶æ‰©å±•ç°æœ‰åŠŸèƒ½"""

    def __init__(self, factor_manager, factor_analyzer, factor_processor,
                 config: Optional[FactorWeightingConfig] = None, 
                 selector_config: Optional[RollingICSelectionConfig] = None):
        super().__init__(factor_manager, factor_analyzer, factor_processor)

        self.config = config or FactorWeightingConfig()
        self.weight_calculator = ICWeightCalculator(self.config)
        self.quality_filter = FactorQualityFilter(self.config)

        # è®¾ç½®å·¥ä½œè·¯å¾„
        self.main_work_path = Path(r"D:\lqs\codeAbout\py\Quantitative\import_file\quant_research_portfolio\workspace\result")

        # æ»šåŠ¨ICç®¡ç†å™¨ - æ ¸å¿ƒæ”¹è¿›
        rolling_ic_config = ICCalculationConfig(
            lookback_months=12,
            forward_periods=self.config.lookback_periods,
            calculation_frequency='M'
        )

        # é›†æˆä¸“ä¸šçš„æ»šåŠ¨ICå› å­ç­›é€‰å™¨
        self.selector_config = selector_config or RollingICSelectionConfig()
        self.factor_selector = None  # å»¶è¿Ÿåˆå§‹åŒ–ï¼Œéœ€è¦snap_config_id

        # ç¼“å­˜ICç»Ÿè®¡æ•°æ®ï¼Œé¿å…é‡å¤è®¡ç®—
        self._ic_stats_cache = {}
    #è¿™ä¸ªå‡½æ•°æ²¡æœ‰æ­£äº¤åŒ–åˆæˆï¼
    def synthesize_ic_weighted_factor(
            self,
            composite_factor_name: str,
            stock_pool_index: str,
            candidate_factor_names: List[str],
            force_recalculate_ic: bool = False,
            snap_config_id: str = None

    ) -> Tuple[pd.DataFrame, Dict]:
        """
        ICåŠ æƒå› å­åˆæˆä¸»æµç¨‹
        
        Args:
            composite_factor_name: å¤åˆå› å­åç§°
            stock_pool_index: è‚¡ç¥¨æ± åç§°
            candidate_factor_names: å€™é€‰å› å­åˆ—è¡¨
            force_recalculate_ic: æ˜¯å¦å¼ºåˆ¶é‡æ–°è®¡ç®—IC
            
        Returns:
            (composite_factor_df, synthesis_report)
        """
        logger.info(f"\nğŸš€ å¼€å§‹ICåŠ æƒå› å­åˆæˆ: {composite_factor_name}")
        logger.info(f"ğŸ“Š å€™é€‰å› å­æ•°é‡: {len(candidate_factor_names)}")
        logger.info(f"ğŸ“ˆ ç›®æ ‡è‚¡ç¥¨æ± INDEX: {stock_pool_index}")

        # ç¬¬ä¸€æ­¥ï¼šæ”¶é›†å€™é€‰å› å­çš„ICç»Ÿè®¡æ•°æ®
        factor_ic_stats = self._collect_factor_ic_stats(
            candidate_factor_names,
            stock_pool_index,
            force_recalculate_ic,
            snap_config_id

        )

        # ç¬¬äºŒæ­¥ï¼šè´¨é‡ç­›é€‰
        qualified_factor_stats, quality_reports = self.quality_filter.filter_factors_by_quality(
            factor_ic_stats
        )

        if not qualified_factor_stats:
            raise ValueError("âŒ æ²¡æœ‰å› å­é€šè¿‡è´¨é‡ç­›é€‰ï¼Œæ— æ³•è¿›è¡Œåˆæˆ")

        # ç¬¬ä¸‰æ­¥ï¼šè®¡ç®—æƒé‡
        factor_weights = self.weight_calculator.calculate_ic_based_weights(
            qualified_factor_stats
        )

        # ç¬¬å››æ­¥ï¼šæ‰§è¡ŒåŠ æƒåˆæˆ
        composite_factor_df = self._execute_weighted_synthesis(
            composite_factor_name,
            stock_pool_index,
            factor_weights,
            snap_config_id
        )

        # ç”ŸæˆåˆæˆæŠ¥å‘Š
        synthesis_report = self._generate_synthesis_report(
            composite_factor_name,
            candidate_factor_names,
            factor_weights,
            quality_reports
        )


        logger.info(f"âœ… ICåŠ æƒå› å­åˆæˆå®Œæˆ: {composite_factor_name}")
        return composite_factor_df, synthesis_report

    def calculate_rolling_weights(
            self,
            candidate_factor_names: List[str],
            stock_pool_index_name: str,
            calculation_date: str,
            resultLoadManager: ResultLoadManager
    ) -> Dict[str, float]:
        """
        æ»šåŠ¨æƒé‡è®¡ç®— - æ ¸å¿ƒæ”¹è¿›ï¼šå®Œå…¨é¿å…å‰è§†åå·®
        
        Args:
            candidate_factor_names: å€™é€‰å› å­åˆ—è¡¨
            stock_pool_index_name: è‚¡ç¥¨æ± åç§°
            calculation_date: æƒé‡è®¡ç®—æ—¶ç‚¹ï¼ˆä¸¥æ ¼ä¸ä½¿ç”¨æ­¤æ—¶ç‚¹ä¹‹åçš„æ•°æ®ï¼‰
            factor_data_source: å› å­æ•°æ®æº
            return_data_source: æ”¶ç›Šæ•°æ®æº
            
        Returns:
            Dict[factor_name, weight]: åŸºäºå†å²ICçš„æƒé‡åˆ†é…
        """
        logger.info(f"ğŸ”„ å¼€å§‹æ»šåŠ¨æƒé‡è®¡ç®— @ {calculation_date}")
        logger.info(f"ğŸ“Š å€™é€‰å› å­: {len(candidate_factor_names)} ä¸ª")

        # ç¬¬ä¸€æ­¥ï¼šè·å–æˆªæ­¢åˆ°calculation_dateçš„å†å²ICæ•°æ®
        historical_ic_stats = {}

        for factor_name in candidate_factor_names:
            try:
                # ä»æ»šåŠ¨ICç®¡ç†å™¨è·å–å†å²ICå¿«ç…§
                latest_snapshot = self.rolling_ic_manager.get_ic_at_timepoint(
                    factor_name, stock_pool_index_name, calculation_date
                )

                if latest_snapshot and latest_snapshot.stats:
                    historical_ic_stats[factor_name] = latest_snapshot.stats
                    logger.debug(f"  âœ… {factor_name}: è·å–å†å²IC @ {calculation_date}")
                else:
                    snapshot = self.rolling_ic_manager._calculate_ic_snapshot(
                        factor_name, stock_pool_index_name, calculation_date,
                        resultLoadManager
                    )
                    if snapshot and snapshot.stats:
                        historical_ic_stats[factor_name] = snapshot.stats
                        # ä¿å­˜å¿«ç…§ä»¥ä¾›åç»­ä½¿ç”¨
                        self.rolling_ic_manager._save_snapshot(snapshot)
                        logger.debug(f"  ğŸ”„ {factor_name}: å®æ—¶è®¡ç®—IC @ {calculation_date}")
                    else:
                        logger.warning(f"  âŒ {factor_name}: æ— æ³•è®¡ç®—å†å²IC--æ­£å¸¸ï¼šå› ä¸ºä¸æ»¡è¶³120ä¸ªè§‚æµ‹ç‚¹ï¼")
            except Exception as e:
                logger.error(f"  âŒ {factor_name}: ICè·å–å¤±è´¥ - {e}")
                continue

        if not historical_ic_stats:
            logger.error("âŒ æ— ä»»ä½•å› å­çš„å†å²ICæ•°æ®ï¼Œæ— æ³•è®¡ç®—æƒé‡")
            return {}

        logger.info(f"ğŸ“Š æˆåŠŸè·å– {len(historical_ic_stats)} ä¸ªå› å­çš„å†å²ICæ•°æ®")

        # ç¬¬äºŒæ­¥ï¼šåŸºäºå†å²ICè¿›è¡Œè´¨é‡ç­›é€‰
        qualified_factor_stats, quality_reports = self.quality_filter.filter_factors_by_quality(
            historical_ic_stats
        )

        if not qualified_factor_stats:
            logger.warning("âš ï¸ æ— å› å­é€šè¿‡è´¨é‡ç­›é€‰ï¼Œè¿”å›ç­‰æƒé‡")
            equal_weight = 1.0 / len(candidate_factor_names)
            return {name: equal_weight for name in candidate_factor_names}

        # ç¬¬ä¸‰æ­¥ï¼šè®¡ç®—æƒé‡ï¼ˆä»…åŸºäºå†å²ICè¡¨ç°ï¼‰
        factor_weights = self.weight_calculator.calculate_ic_based_weights(
            qualified_factor_stats
        )

        # ç¬¬å››æ­¥ï¼šä¸ºæœªé€šè¿‡ç­›é€‰çš„å› å­åˆ†é…0æƒé‡
        final_weights = {}
        for factor_name in candidate_factor_names:
            final_weights[factor_name] = factor_weights.get(factor_name, 0.0)

        # æ—¥å¿—è®°å½•
        selected_factors = [name for name, weight in final_weights.items() if weight > 0]
        logger.info(f"âœ… æ»šåŠ¨æƒé‡è®¡ç®—å®Œæˆ @ {calculation_date}")
        logger.info(f"ğŸ“Š é€‰ä¸­å› å­: {len(selected_factors)}/{len(candidate_factor_names)}")

        for factor_name, weight in sorted(final_weights.items(), key=lambda x: x[1], reverse=True):
            if weight > 0:
                logger.info(f"  ğŸ¯ {factor_name}: {weight:.1%}")

        return final_weights

    # æ·±å…¥å‰–æ
    def _collect_factor_ic_stats(
            self,
            factor_names: List[str],
            stock_pool_index: str,
            force_recalculate: bool = False,
            snap_config_id: str = None
    ) -> Dict[str, Dict[str, Dict]]:
        """æ”¶é›†å› å­ICç»Ÿè®¡æ•°æ®"""
        logger.info("ğŸ“Š æ­£åœ¨æ”¶é›†å› å­ICç»Ÿè®¡æ•°æ®...")

        factor_ic_stats = {}

        for factor_name in factor_names:
            cache_key = f"{factor_name}_{stock_pool_index}"

            if not force_recalculate and cache_key in self._ic_stats_cache:
                factor_ic_stats[factor_name] = self._ic_stats_cache[cache_key]
                logger.debug(f"  ğŸ“¥ {factor_name}: ä½¿ç”¨ç¼“å­˜æ•°æ®")
                continue

            try:
                # ä»å·²ä¿å­˜çš„æµ‹è¯•ç»“æœä¸­è¯»å–ICç»Ÿè®¡
                ic_stats = self._load_factor_ic_stats(factor_name=factor_name,stock_pool_index= stock_pool_index, snap_config_id=snap_config_id)

                if ic_stats:
                    factor_ic_stats[factor_name] = ic_stats
                    self._ic_stats_cache[cache_key] = ic_stats
                    logger.debug(f"  âœ… {factor_name}: ICæ•°æ®åŠ è½½æˆåŠŸ")
                else:
                    logger.warning(f"  âš ï¸ {factor_name}: æœªæ‰¾åˆ°ICç»Ÿè®¡æ•°æ®ï¼Œè·³è¿‡")

            except Exception as e:
                raise ValueError(f"  âŒ {factor_name}: åŠ è½½ICæ•°æ®å¤±è´¥ - {e}")

        logger.info(f"ğŸ“Š ICæ•°æ®æ”¶é›†å®Œæˆ: {len(factor_ic_stats)}/{len(factor_names)} ä¸ªå› å­")
        return factor_ic_stats

    def _load_factor_ic_stats(self, factor_name: str, stock_pool_index: str, calcu_type='o2o', snap_config_id: str = None) -> Optional[Dict]:
        """
        ä»æ»šåŠ¨ICå­˜å‚¨ä¸­æå–å› å­çš„ICç»Ÿè®¡æ•°æ®
        Args:
            factor_name: å› å­åç§°
            stock_pool_index: è‚¡ç¥¨æ± ç´¢å¼•
            calcu_type: æ”¶ç›Šè®¡ç®—ç±»å‹ï¼Œé»˜è®¤'o2o'
            snap_config_id: é…ç½®å¿«ç…§IDï¼Œç”¨äºç¡®å®šç‰ˆæœ¬
            
        Returns:
            Dict[period, ic_stats]: å„å‘¨æœŸçš„ICç»Ÿè®¡æ•°æ®ï¼Œæ ¼å¼ä¸RollingICManagerä¸€è‡´
        """
        try:
            if snap_config_id is None:
                logger.warning(f"æœªæä¾›snap_config_idï¼Œæ— æ³•ç¡®å®šæ•°æ®ç‰ˆæœ¬")
                return None
                
            # 1. ä»é…ç½®å¿«ç…§è·å–ç‰ˆæœ¬ä¿¡æ¯
            config_manager = ConfigSnapshotManager()
            pool_index, start_date, end_date, config_evaluation = config_manager.get_snapshot_config_content_details(snap_config_id)
            version = f"{start_date}_{end_date}"
            
            # 2. æ„å»ºæ»šåŠ¨ICæ–‡ä»¶è·¯å¾„
            rolling_ic_dir = (self.main_work_path / stock_pool_index / factor_name / 
                             calcu_type / version / 'rolling_ic')
            
            if not rolling_ic_dir.exists():
                # å°±åœ°ç”ŸæˆICæ•°æ®å¹¶ä¿å­˜åˆ°æœ¬åœ°
                logger.info(f"æ»šåŠ¨ICç›®å½•ä¸å­˜åœ¨ï¼Œå¼€å§‹å°±åœ°ç”Ÿæˆ: {factor_name}")
                try:
                    # è°ƒç”¨ç”Ÿæˆå‡½æ•°ä¸ºå½“å‰å› å­ç”ŸæˆICæ•°æ®
                    run_cal_and_save_rolling_ic_by_snapshot_config_id(snap_config_id, [factor_name])
                    logger.info(f"âœ… æˆåŠŸç”Ÿæˆæ»šåŠ¨ICæ•°æ®: {factor_name}")
                    
                    # é‡æ–°æ£€æŸ¥ç›®å½•æ˜¯å¦å­˜åœ¨
                    if not rolling_ic_dir.exists():
                        raise ValueError(f"for-{factor_name} ç”ŸæˆICæ•°æ®åç›®å½•ä»ä¸å­˜åœ¨: {rolling_ic_dir}")
                except Exception as e:
                    raise ValueError(f"ç”Ÿæˆæ»šåŠ¨ICæ•°æ®å¤±è´¥ {factor_name}: {e}")

            # 3. æŸ¥æ‰¾æ‰€æœ‰ICå¿«ç…§æ–‡ä»¶
            ic_files = list(rolling_ic_dir.glob("ic_snapshot_*.json"))
            if not ic_files:
                # å¦‚æœç›®å½•å­˜åœ¨ä½†æ— æ–‡ä»¶ï¼Œå¯èƒ½æ˜¯ICç”Ÿæˆä¸å®Œæ•´ï¼Œå°è¯•é‡æ–°ç”Ÿæˆ
                logger.warning(f"ICç›®å½•å­˜åœ¨ä½†æ— å¿«ç…§æ–‡ä»¶ï¼Œå°è¯•é‡æ–°ç”Ÿæˆ: {factor_name}")
                try:
                    run_cal_and_save_rolling_ic_by_snapshot_config_id(snap_config_id, [factor_name])
                    
                    # é‡æ–°æŸ¥æ‰¾æ–‡ä»¶
                    ic_files = list(rolling_ic_dir.glob("ic_snapshot_*.json"))
                    if not ic_files:
                        raise ValueError(f"é‡æ–°ç”Ÿæˆåä»æ— ICå¿«ç…§æ–‡ä»¶: {rolling_ic_dir}")
                    logger.info(f"âœ… é‡æ–°ç”ŸæˆICæ•°æ®æˆåŠŸ: {factor_name}")
                except Exception as e:
                    raise ValueError(f"é‡æ–°ç”ŸæˆICæ•°æ®å¤±è´¥ {factor_name}: {e}")

            logger.debug(f"æ‰¾åˆ° {len(ic_files)} ä¸ªICå¿«ç…§æ–‡ä»¶ for {factor_name}")
            
            # 4. åŠ è½½å¹¶èšåˆICç»Ÿè®¡æ•°æ®
            all_periods_stats = {}
            
            for ic_file in ic_files:
                try:
                    with open(ic_file, 'r', encoding='utf-8') as f:
                        snapshot_data = json.load(f)
                    
                    # æå–ic_statså­—æ®µ
                    ic_stats = snapshot_data.get('stats', {})
                    
                    # èšåˆå„å‘¨æœŸçš„ç»Ÿè®¡æ•°æ®
                    for period, period_stats in ic_stats.items():
                        if period not in all_periods_stats:
                            all_periods_stats[period] = []
                        all_periods_stats[period].append(period_stats)
                        
                except Exception as e:
                    logger.warning(f"è¯»å–ICæ–‡ä»¶å¤±è´¥ {ic_file}: {e}")
                    continue
            
            if not all_periods_stats:
                logger.debug(f"æœªæ‰¾åˆ°æœ‰æ•ˆçš„ICç»Ÿè®¡æ•°æ®")
                return None
            
            # 5. è®¡ç®—èšåˆç»Ÿè®¡æŒ‡æ ‡
            aggregated_stats = {}
            for period, stats_list in all_periods_stats.items():#stat_listï¼šæ‰€æœ‰å¿«ç…§
                if not stats_list:
                    continue
                    
                # è®¡ç®—æ—¶é—´åºåˆ—çš„å¹³å‡æŒ‡æ ‡
                ic_means = [s.get('ic_mean', 0) for s in stats_list if s.get('ic_mean') is not None]
                ic_stds = [s.get('ic_std', 0) for s in stats_list if s.get('ic_std') is not None]
                ic_irs = [s.get('ic_ir', 0) for s in stats_list if s.get('ic_ir') is not None]
                ic_win_rates = [s.get('ic_win_rate', 0.5) for s in stats_list if s.get('ic_win_rate') is not None]
                ic_p_values = [s.get('ic_p_value', 1.0) for s in stats_list if s.get('ic_p_value') is not None]
                ic_t_stats = [s.get('ic_t_stat', 0) for s in stats_list if s.get('ic_t_stat') is not None]
                
                if not ic_means:
                    continue
                
                # èšåˆç»Ÿè®¡
                aggregated_stats[period] = {
                    'ic_mean': np.mean(ic_means),
                    'ic_std': np.mean(ic_stds) if ic_stds else 0,
                    'ic_ir': np.mean(ic_irs) if ic_irs else 0,
                    'ic_win_rate': np.mean(ic_win_rates) if ic_win_rates else 0.5,
                    'ic_p_value': np.mean(ic_p_values) if ic_p_values else 1.0,
                    'ic_t_stat': np.mean(ic_t_stats) if ic_t_stats else 0,
                    'ic_count': len(ic_means),
                    'snapshot_count': len(stats_list),
                    'ic_mean_std': np.std(ic_means) if len(ic_means) > 1 else 0,  # ICå‡å€¼çš„ç¨³å®šæ€§
                    'ic_ir_std': np.std(ic_irs) if len(ic_irs) > 1 else 0  # IRçš„ç¨³å®šæ€§
                }
            
            if not aggregated_stats:
                logger.debug(f"èšåˆåæ— æœ‰æ•ˆç»Ÿè®¡æ•°æ®")
                return None
                
            logger.debug(f"æˆåŠŸæå–å› å­ {factor_name} çš„ICç»Ÿè®¡: {list(aggregated_stats.keys())} å‘¨æœŸ")
            return aggregated_stats
            
        except Exception as e:
            logger.error(f"åŠ è½½å› å­ICç»Ÿè®¡å¤±è´¥ {factor_name}: {e}")
            return None

    def _execute_weighted_synthesis(
            self,
            composite_factor_name: str,
            stock_pool_index_name: str,
            factor_weights: Dict[str, float],
            snap_config_id:str
    ) -> pd.DataFrame:
        """æ‰§è¡ŒåŠ æƒå› å­åˆæˆ"""
        logger.info(f"âš–ï¸ å¼€å§‹æ‰§è¡ŒåŠ æƒåˆæˆï¼Œä½¿ç”¨{len(factor_weights)}ä¸ªå› å­")

        processed_factors = []
        weights_list = []

        for factor_name, weight in factor_weights.items():
            logger.info(f"  ğŸ”„ å¤„ç†å› å­: {factor_name} (æƒé‡: {weight:.3f})")

            # å¤„ç†å•ä¸ªå› å­
            processed_df = self.get_sub_factor_df_from_local(factor_name, stock_pool_index_name,snap_config_id)

            processed_factors.append(processed_df)
            weights_list.append(weight)

        if not processed_factors:
            raise ValueError("æ²¡æœ‰ä»»ä½•å› å­è¢«æˆåŠŸå¤„ç†")

        # åŠ æƒåˆæˆ
        composite_factor_df = self._weighted_combine_factors(processed_factors, weights_list)

        # æœ€ç»ˆæ ‡å‡†åŒ–
        composite_factor_df = self.processor._standardize_robust(composite_factor_df)

        logger.info(f"âœ… åŠ æƒåˆæˆå®Œæˆ: {composite_factor_name}")
        return composite_factor_df

    def _weighted_combine_factors(
            self,
            factor_dfs: List[pd.DataFrame],
            weights: List[float]
    ) -> pd.DataFrame:
        """åŠ æƒåˆå¹¶å› å­æ•°æ®æ¡†"""
        if len(factor_dfs) != len(weights):
            raise ValueError("å› å­æ•°é‡ä¸æƒé‡æ•°é‡ä¸åŒ¹é…")

        # ç¡®ä¿æƒé‡å½’ä¸€åŒ–
        weights = np.array(weights)
        weights = weights / weights.sum()

        # åŠ æƒåˆå¹¶
        result_df = None
        for i, (df, weight) in enumerate(zip(factor_dfs, weights)):
            weighted_df = df * weight

            if result_df is None:
                result_df = weighted_df
            else:
                result_df = result_df.add(weighted_df, fill_value=0)

        return result_df

    def _generate_synthesis_report(
            self,
            composite_factor_name: str,
            candidate_factors: List[str],
            final_weights: Dict[str, float],
            quality_reports: List[FactorQualityReport]
    ) -> Dict:
        """ç”Ÿæˆå› å­åˆæˆæŠ¥å‘Š"""

        # æŒ‰æƒé‡æ’åº
        sorted_weights = sorted(final_weights.items(), key=lambda x: x[1], reverse=True)

        report = {
            'composite_factor_name': composite_factor_name,
            'synthesis_timestamp': pd.Timestamp.now(),
            'candidate_factors_count': len(candidate_factors),
            'qualified_factors_count': len(final_weights),
            'final_weights': dict(sorted_weights),
            'top_3_factors': sorted_weights[:3],
            'config_used': {
                'min_ic_mean': self.config.min_ic_mean,
                'min_ic_ir': self.config.min_ic_ir,
                'min_ic_win_rate': self.config.min_ic_win_rate,
                'max_single_weight': self.config.max_single_weight
            },
            'quality_summary': {
                'passed': len([r for r in quality_reports if not r.risk_flags]),
                'failed': len([r for r in quality_reports if r.risk_flags]),
                'main_failure_reasons': self._summarize_failure_reasons(quality_reports)
            }
        }

        return report

    def _summarize_failure_reasons(self, quality_reports: List[FactorQualityReport]) -> Dict[str, int]:
        """æ±‡æ€»å¤±è´¥åŸå› ç»Ÿè®¡"""
        failure_counts = {}

        for report in quality_reports:
            for flag in report.risk_flags:
                reason = flag.split('(')[0]  # æå–åŸå› ä¸»ä½“
                failure_counts[reason] = failure_counts.get(reason, 0) + 1

        return failure_counts
    
    def _generate_comprehensive_report(
            self,
            composite_factor_name: str,
            candidate_factors: List[str],
            selected_factors: List[str],
            final_weights: Dict[str, float],
            selection_report: Dict
    ) -> Dict:
        """ç”ŸæˆåŒ…å«ç­›é€‰å’Œåˆæˆä¿¡æ¯çš„ç»¼åˆæŠ¥å‘Š"""
        
        # åŸºç¡€åˆæˆæŠ¥å‘Š
        base_report = self._generate_synthesis_report(
            composite_factor_name, candidate_factors, final_weights, []
        )
        
        # æ·»åŠ ä¸“ä¸šç­›é€‰ä¿¡æ¯
        comprehensive_report = {
            **base_report,
            'professional_selection': {
                'selection_method': 'RollingIC-based Professional Selection',
                'candidate_count': len(candidate_factors),
                'selected_count': len(selected_factors),
                'selection_rate': len(selected_factors) / len(candidate_factors) if candidate_factors else 0,
                'selected_factors': selected_factors,
                'selection_report': selection_report
            }
        }
        
        return comprehensive_report

    def print_synthesis_report(self, report: Dict):
        """æ‰“å°åˆæˆæŠ¥å‘Šï¼ˆæ”¯æŒä¸“ä¸šç­›é€‰å’Œä¼ ç»Ÿç­›é€‰ä¸¤ç§æ ¼å¼ï¼‰"""
        print(f"\n{'=' * 80}")
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯ä¸“ä¸šç­›é€‰æŠ¥å‘Š
        if 'professional_selection' in report:
            print(f"ğŸ“Š ä¸“ä¸šæ»šåŠ¨ICç­›é€‰+ICåŠ æƒåˆæˆæŠ¥å‘Š")
            print(f"{'=' * 80}")
            print(f"ğŸ¯ åˆæˆå› å­åç§°: {report['composite_factor_name']}")
            print(f"â° åˆæˆæ—¶é—´: {report['synthesis_timestamp'].strftime('%Y-%m-%d %H:%M:%S')}")
            
            # ä¸“ä¸šç­›é€‰ä¿¡æ¯
            prof_sel = report['professional_selection']
            print(f"\nğŸ” ä¸“ä¸šç­›é€‰ç»“æœ:")
            print(f"  ğŸ“ˆ å€™é€‰å› å­æ•°é‡: {prof_sel['candidate_count']}")
            print(f"  âœ… ç­›é€‰é€šè¿‡æ•°é‡: {prof_sel['selected_count']}")
            print(f"  ğŸ“Š ç­›é€‰é€šè¿‡ç‡: {prof_sel['selection_rate']:.1%}")
            print(f"  ğŸ† ç­›é€‰æ–¹æ³•: {prof_sel['selection_method']}")
            
            print(f"\nğŸ¯ æœ€ç»ˆé€‰ä¸­å› å­:")
            for i, factor in enumerate(prof_sel['selected_factors'], 1):
                weight = report['final_weights'].get(factor, 0)
                print(f"  {i:2d}. {factor:25s}: {weight:6.1%}")
                
        else:
            print(f"ğŸ“Š ICåŠ æƒå› å­åˆæˆæŠ¥å‘Š")
            print(f"{'=' * 80}")
            print(f"ğŸ¯ åˆæˆå› å­åç§°: {report['composite_factor_name']}")
            print(f"â° åˆæˆæ—¶é—´: {report['synthesis_timestamp'].strftime('%Y-%m-%d %H:%M:%S')}")
            print(f"ğŸ“ˆ å€™é€‰å› å­æ•°é‡: {report['candidate_factors_count']}")
            print(f"âœ… é€šè¿‡ç­›é€‰æ•°é‡: {report['qualified_factors_count']}")

            print(f"\nğŸ† æœ€ç»ˆæƒé‡åˆ†é…:")
            for factor_name, weight in report['final_weights'].items():
                print(f"  {factor_name:25s}: {weight:6.1%}")

        print(f"\nğŸ¥‡ æƒé‡å‰ä¸‰å:")
        for i, (factor_name, weight) in enumerate(report.get('top_3_factors', []), 1):
            print(f"  {i}. {factor_name}: {weight:.1%}")

        # è´¨é‡ç­›é€‰ä¿¡æ¯ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if 'quality_summary' in report:
            quality_summary = report['quality_summary']
            print(f"\nğŸ“‹ è´¨é‡ç­›é€‰æ±‡æ€»:")
            print(f"  âœ… é€šè¿‡: {quality_summary['passed']} ä¸ª")
            print(f"  âŒ å¤±è´¥: {quality_summary['failed']} ä¸ª")

            if quality_summary['main_failure_reasons']:
                print(f"  ä¸»è¦å¤±è´¥åŸå› :")
                for reason, count in quality_summary['main_failure_reasons'].items():
                    print(f"    - {reason}: {count} ä¸ªå› å­")

        print(f"{'=' * 80}")

    def execute_orthogonalization_plan(
            self,
            orthogonalization_plan: List[Dict],
            stock_pool_index: str,
            snap_config_id: str
    ) -> Dict[str, pd.DataFrame]:
        """
        æ‰§è¡Œæ­£äº¤åŒ–æ”¹é€ è®¡åˆ’ - æ ¸å¿ƒåŠŸèƒ½ï¼šæˆªé¢çº¿æ€§å›å½’æ®‹å·®æå–
        
        Args:
            orthogonalization_plan: æ­£äº¤åŒ–è®¡åˆ’åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«ï¼š
                - original_factor: ç›®æ ‡å› å­åç§°
                - base_factor: åŸºå‡†å› å­åç§°
                - orthogonal_name: æ­£äº¤åŒ–åçš„æ–°å› å­åç§°
                - correlation: åŸå§‹ç›¸å…³æ€§
                - base_score: åŸºå‡†å› å­è¯„åˆ†
                - target_score: ç›®æ ‡å› å­è¯„åˆ†
            stock_pool_index: è‚¡ç¥¨æ± åç§°
            snap_config_id: é…ç½®å¿«ç…§ID
            
        Returns:
            Dict[orthogonal_name, orthogonal_factor_df]: æ­£äº¤åŒ–åçš„å› å­æ•°æ®
        """
        if not orthogonalization_plan:
            logger.info("âšª æ— æ­£äº¤åŒ–è®¡åˆ’ï¼Œè·³è¿‡æ‰§è¡Œ")
            return {}
            
        logger.info(f"ğŸ”§ å¼€å§‹æ‰§è¡Œæ­£äº¤åŒ–è®¡åˆ’ï¼Œå…± {len(orthogonalization_plan)} é¡¹")
        
        orthogonal_factors = {}
        
        for plan_item in orthogonalization_plan:
            try:
                orthogonal_factor_df, avg_r_squared = self._execute_single_orthogonalization(
                    plan_item, stock_pool_index, snap_config_id
                )
                
                if orthogonal_factor_df is not None:
                    orthogonal_factors[plan_item['orthogonal_name']] = orthogonal_factor_df
                    logger.info(f"âœ… æˆåŠŸç”Ÿæˆæ­£äº¤åŒ–å› å­: {plan_item['orthogonal_name']} (RÂ²={avg_r_squared:.3f})")
                else:
                    logger.warning(f"âš ï¸ æ­£äº¤åŒ–å¤±è´¥: {plan_item['orthogonal_name']}")
                    
            except Exception as e:
                logger.error(f"âŒ æ­£äº¤åŒ–æ‰§è¡Œå¼‚å¸¸ {plan_item['orthogonal_name']}: {e}")
                continue
        
        logger.info(f"ğŸ¯ æ­£äº¤åŒ–æ‰§è¡Œå®Œæˆï¼ŒæˆåŠŸç”Ÿæˆ {len(orthogonal_factors)} ä¸ªæ­£äº¤åŒ–å› å­")
        return orthogonal_factors

    def _execute_single_orthogonalization(
            self,
            plan_item: Dict,
            stock_pool_index: str,
            snap_config_id: str
    ) -> Tuple[Optional[pd.DataFrame], float]:
        """
        æ‰§è¡Œå•ä¸ªæ­£äº¤åŒ–æ”¹é€  - é€æ—¥æˆªé¢OLSå›å½’
        
        æ ¸å¿ƒé€»è¾‘ï¼š
        1. åŠ è½½ç›®æ ‡å› å­å’ŒåŸºå‡†å› å­æ•°æ®
        2. é€æ—¥è¿›è¡Œæˆªé¢çº¿æ€§å›å½’ï¼štarget_factor = Î± + Î² * base_factor + Îµ
        3. æå–æ®‹å·®Îµä½œä¸ºæ­£äº¤åŒ–åçš„å› å­å€¼
        
        Args:
            plan_item: å•ä¸ªæ­£äº¤åŒ–è®¡åˆ’
            stock_pool_index: è‚¡ç¥¨æ± åç§°
            snap_config_id: é…ç½®å¿«ç…§ID
            
        Returns:
            (æ­£äº¤åŒ–åçš„å› å­DataFrame, å¹³å‡RÂ²): ç”¨äºICè°ƒæ•´
        """
        target_factor = plan_item['original_factor']
        base_factor = plan_item['base_factor']
        orthogonal_name = plan_item['orthogonal_name']
        
        logger.debug(f"  ğŸ”„ æ‰§è¡Œæ­£äº¤åŒ–: {target_factor} vs {base_factor} -> {orthogonal_name}")
        
        try:
            # 1. åŠ è½½å› å­æ•°æ®
            target_df = self.get_sub_factor_df_from_local(target_factor, stock_pool_index, snap_config_id)
            base_df = self.get_sub_factor_df_from_local(base_factor, stock_pool_index, snap_config_id)
            
            if target_df is None or base_df is None:
                raise ValueError(f"  âŒ æ— æ³•åŠ è½½å› å­æ•°æ®: target={target_df is not None}, base={base_df is not None}")

            # 2. æ•°æ®å¯¹é½å’Œé¢„å¤„ç†
            aligned_target, aligned_base = self._align_factor_data(target_df, base_df)
            
            if aligned_target.empty or aligned_base.empty:
                logger.error("  âŒ å› å­æ•°æ®å¯¹é½åä¸ºç©º")
                return None, 0.0
            
            # 3. é€æ—¥æˆªé¢å›å½’ï¼Œè·å–RÂ²ç”¨äºICè°ƒæ•´
            orthogonal_df, avg_r_squared = self._daily_cross_sectional_orthogonalization(
                aligned_target, aligned_base, orthogonal_name
            )
            
            # è®°å½•RÂ²ä¿¡æ¯ç”¨äºåç»­ICè°ƒæ•´
            plan_item['avg_r_squared'] = avg_r_squared
            
            return orthogonal_df, avg_r_squared
            
        except Exception as e:
            logger.error(f"  âŒ å•é¡¹æ­£äº¤åŒ–å¤±è´¥ {orthogonal_name}: {e}")
            return None, 0.0

    def _align_factor_data(
            self,
            target_df: pd.DataFrame,
            base_df: pd.DataFrame
    ) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """
        å› å­æ•°æ®å¯¹é½ - ç¡®ä¿æ—¶é—´å’Œè‚¡ç¥¨ç»´åº¦ä¸€è‡´
        
        Args:
            target_df: ç›®æ ‡å› å­æ•°æ®
            base_df: åŸºå‡†å› å­æ•°æ®
            
        Returns:
            (aligned_target, aligned_base): å¯¹é½åçš„æ•°æ®
        """
        # æ‰¾åˆ°å…±åŒçš„æ—¶é—´å’Œè‚¡ç¥¨
        common_dates = target_df.index.intersection(base_df.index)
        common_stocks = target_df.columns.intersection(base_df.columns)
        
        if len(common_dates) == 0 or len(common_stocks) == 0:
            logger.error(f"  âŒ æ— å…±åŒæ—¶é—´ç‚¹æˆ–è‚¡ç¥¨ï¼šæ—¥æœŸ={len(common_dates)}, è‚¡ç¥¨={len(common_stocks)}")
            return pd.DataFrame(), pd.DataFrame()
        
        # æ•°æ®å¯¹é½
        aligned_target = target_df.loc[common_dates, common_stocks]
        aligned_base = base_df.loc[common_dates, common_stocks]
        
        logger.debug(f"  ğŸ“Š æ•°æ®å¯¹é½å®Œæˆï¼š{len(common_dates)}ä¸ªäº¤æ˜“æ—¥, {len(common_stocks)}åªè‚¡ç¥¨")
        
        return aligned_target, aligned_base

    def _daily_cross_sectional_orthogonalization(
            self,
            target_df: pd.DataFrame,
            base_df: pd.DataFrame,
            orthogonal_name: str
    ) -> Tuple[pd.DataFrame, Dict]:
        """
        é€æ—¥æˆªé¢æ­£äº¤åŒ– (V2 - æˆ˜æ–—åŠ å›ºç‰ˆ)
        - å¼ºåŒ–1: ä½¿ç”¨ align ç¡®ä¿ç´¢å¼•ä¸¥æ ¼å¯¹é½
        - å¼ºåŒ–2: ç»Ÿä¸€æœ€å°æ ·æœ¬é‡å‚æ•°
        - å¼ºåŒ–3: æ ‡å‡†åŒ–æ—¶ä½¿ç”¨ ddof=0 å¹¶åŠ  epsilon
        - å¼ºåŒ–4: è¿”å›åŒ…å«æˆåŠŸç‡ã€å¤±è´¥æ—¥æœŸç­‰ä¿¡æ¯çš„å…ƒæ•°æ®å­—å…¸
        """
        logger.debug(f"  ğŸ§® V2: å¼€å§‹é€æ—¥æˆªé¢å›å½’ï¼Œå…±{len(target_df)}ä¸ªäº¤æ˜“æ—¥")

        orthogonal_df = pd.DataFrame(index=target_df.index, columns=target_df.columns, dtype=np.float64)

        r_squared_list = []
        failed_dates = []
        epsilon = 1e-8

        for date in target_df.index:
            try:
                y_cross = target_df.loc[date]
                x_cross = base_df.loc[date]

                # ã€å¼ºåŒ–ã€‘ä½¿ç”¨ align ä¸¥æ ¼å¯¹é½ y å’Œ xï¼Œå¹¶ç§»é™¤å…±åŒçš„NaN
                y_aligned, x_aligned = y_cross.align(x_cross, join='inner')
                valid_mask = (~y_aligned.isna()) & (~x_aligned.isna())
                y_valid = y_aligned[valid_mask]
                x_valid = x_aligned[valid_mask]

                # ã€å¼ºåŒ–ã€‘ç»Ÿä¸€æœ€å°æ ·æœ¬é‡æ£€æŸ¥
                if len(y_valid) < self.config.min_orthogonalization_obs:
                    logger.debug(f"    âš ï¸ {date}: æœ‰æ•ˆè§‚æµ‹ä¸è¶³({len(y_valid)}ä¸ª)ï¼Œè·³è¿‡")
                    failed_dates.append(date.strftime('%Y-%m-%d'))
                    continue

                residuals, r_squared = self._perform_cross_sectional_ols(y_valid, x_valid, date)

                if residuals is not None and r_squared is not None:
                    # ã€å¼ºåŒ–ã€‘æ ‡å‡†åŒ–æ—¶ä½¿ç”¨ ddof=0 (æ€»ä½“æ ‡å‡†å·®)
                    std_val = residuals.std(ddof=0)

                    if std_val > epsilon:
                        mean_val = residuals.mean()  # OLSæ®‹å·®å‡å€¼ç†è®ºä¸Šä¸º0
                        standardized_residuals = (residuals - mean_val) / std_val
                        orthogonal_df.loc[date, standardized_residuals.index] = standardized_residuals.values
                        r_squared_list.append(r_squared)
                    else:
                        # å¦‚æœæ ‡å‡†å·®ä¸º0ï¼Œè¯´æ˜æ®‹å·®ä¸ºå¸¸æ•°ï¼Œå¡«å……ä¸º0
                        orthogonal_df.loc[date, residuals.index] = 0.0
                        r_squared_list.append(r_squared)
                else:
                    failed_dates.append(date.strftime('%Y-%m-%d'))

            except Exception as e:
                logger.debug(f"    âŒ {date}: å›å½’å¾ªç¯å¤±è´¥ - {e}")
                failed_dates.append(date.strftime('%Y-%m-%d'))
                continue

        successful_regressions = len(r_squared_list)
        total_days = len(target_df)

        # ã€å¼ºåŒ–ã€‘æ„å»ºåŒ…å«ä¸°å¯Œä¿¡æ¯çš„å…ƒæ•°æ®å­—å…¸
        meta = {
            'successful_regressions': successful_regressions,
            'total_days': total_days,
            'success_rate': successful_regressions / total_days if total_days > 0 else 0.0,
            'avg_r_squared': np.mean(r_squared_list) if r_squared_list else 0.0,
            'failed_dates': failed_dates
        }

        if meta['success_rate'] < 0.9:  # å¦‚æœæˆåŠŸç‡ä½äº90%ï¼Œåˆ™æå‡æ—¥å¿—çº§åˆ«ä¸ºè­¦å‘Š
            raise ValueError(f"  âš ï¸ {orthogonal_name}: æˆªé¢å›å½’æˆåŠŸç‡è¾ƒä½: {meta['success_rate']:.1%}")
        else:
            logger.debug(f"  âœ… {orthogonal_name}: æˆªé¢å›å½’å®Œæˆ, æˆåŠŸç‡ {meta['success_rate']:.1%}")

        return orthogonal_df, meta['avg_r_squared']

    def _perform_cross_sectional_ols(
            self,
            y: pd.Series,
            x: pd.Series,
            date: str = None
    ) -> Tuple[Optional[pd.Series], Optional[float]]:
        """
        æ‰§è¡Œå•æ—¥æˆªé¢OLSå›å½’å¹¶æå–æ®‹å·® (V2 - æˆ˜æ–—åŠ å›ºç‰ˆ)
        - å¼ºåŒ–1: å¯¹é«˜RÂ²åªå‘Šè­¦ä¸ä¸­æ–­
        - å¼ºåŒ–2: æ£€æŸ¥è‡ªå˜é‡xçš„æ–¹å·®ï¼Œé¿å…é€€åŒ–å›å½’
        - å¼ºåŒ–3: æ•è· statsmodels å†…éƒ¨çš„æ‹Ÿåˆé”™è¯¯
        """
        try:
            # ã€å¼ºåŒ–ã€‘æ£€æŸ¥è‡ªå˜é‡æ ‡å‡†å·®ï¼Œå¦‚æœè¿‡å°ï¼ˆæ¥è¿‘å¸¸æ•°ï¼‰ï¼Œåˆ™å›å½’æ— æ„ä¹‰
            if x.std(ddof=0) < self.config.orthogonalization_x_std_eps:
                logger.debug(f"    âš ï¸ {date}: åŸºå‡†å› å­æ ‡å‡†å·®è¿‡ä½({x.std(ddof=0):.2e})ï¼Œè·³è¿‡å›å½’")
                return None, None

            # ä½¿ç”¨ has_constant='add' æ›´ç¨³å¥ï¼Œé¿å…é‡å¤æ·»åŠ å¸¸æ•°é¡¹
            X_with_const = sm.add_constant(x, has_constant='add')

            # ã€å¼ºåŒ–ã€‘å•ç‹¬æ•è·æ¨¡å‹æ‹Ÿåˆæ—¶çš„çº¿æ€§ä»£æ•°é”™è¯¯ï¼ˆå¦‚å¥‡å¼‚çŸ©é˜µï¼‰
            try:
                model = sm.OLS(y, X_with_const).fit()
            except LinAlgError as e:
                raise ValueError(f"    âŒ {date}: OLSæ‹Ÿåˆå¤±è´¥ - çŸ©é˜µé”™è¯¯: {e}")

            residuals = model.resid
            r_squared = model.rsquared

            # ã€å¼ºåŒ–ã€‘å¯¹äºå¼‚å¸¸é«˜çš„RÂ²ï¼Œåªè®°å½•è­¦å‘Šï¼Œä¸ä¸­æ–­æµç¨‹
            if r_squared > 0.95:
                logger.warning(f"    âš ï¸ {date}: RÂ²å¼‚å¸¸é«˜({r_squared:.3f})ï¼Œå¯èƒ½å­˜åœ¨æ•°æ®å…±çº¿æ€§é—®é¢˜")

            return residuals, r_squared

        except Exception as e:
            raise ValueError(f"    âŒ {date}: _perform_cross_sectional_ols_v2 å‘ç”Ÿæ„å¤–é”™è¯¯: {e}")

    def _perform_ols_sklearn_fallback(
            self,
            y: pd.Series,
            x: pd.Series
    ) -> Tuple[Optional[pd.Series], Optional[float]]:
        """
        sklearnå›å½’å¤‡ç”¨æ–¹æ¡ˆ
        
        Args:
            y: å› å˜é‡
            x: è‡ªå˜é‡
            
        Returns:
            (æ®‹å·®åºåˆ—, RÂ²å€¼): æ®‹å·®å’Œæ‹Ÿåˆåº¦
        """
        try:
            # sklearnä¼šè‡ªåŠ¨æ·»åŠ æˆªè·é¡¹ï¼ˆå¦‚æœfit_intercept=Trueï¼‰
            reg = LinearRegression(fit_intercept=True)
            
            # reshapeæ•°æ®
            X = x.values.reshape(-1, 1)
            y_values = y.values
            
            # æ‹Ÿåˆæ¨¡å‹
            reg.fit(X, y_values)
            
            # è®¡ç®—é¢„æµ‹å€¼å’Œæ®‹å·®
            y_pred = reg.predict(X)
            residuals = y_values - y_pred
            
            # è®¡ç®—RÂ²
            r_squared = reg.score(X, y_values)
            
            # è¿”å›pandas Seriesæ ¼å¼å’ŒRÂ²
            residuals_series = pd.Series(residuals, index=y.index)
            return residuals_series, r_squared
            
        except Exception as e:
            logger.debug(f"    âŒ sklearnå›å½’å¤±è´¥: {e}")
            return None, None

    def _standardize_orthogonal_factor(
            self,
            orthogonal_df: pd.DataFrame
    ) -> pd.DataFrame:
        """
        æ ‡å‡†åŒ–æ­£äº¤åŒ–å› å­
        
        åº”ç”¨æˆªé¢æ ‡å‡†åŒ–ï¼šæ¯ä¸ªäº¤æ˜“æ—¥å†…ï¼Œå› å­å€¼æ ‡å‡†åŒ–ä¸ºå‡å€¼0ã€æ ‡å‡†å·®1
        
        Args:
            orthogonal_df: åŸå§‹æ­£äº¤åŒ–å› å­
            
        Returns:
            æ ‡å‡†åŒ–åçš„æ­£äº¤åŒ–å› å­
        """
        if orthogonal_df.empty:
            return orthogonal_df
        
        logger.debug("  ğŸ“ å¼€å§‹å› å­æ ‡å‡†åŒ–")
        
        standardized_df = orthogonal_df.copy()
        
        # é€æ—¥æ ‡å‡†åŒ–
        for date in orthogonal_df.index:
            date_values = orthogonal_df.loc[date]
            valid_values = date_values.dropna()
            
            if len(valid_values) < 5:  # è‡³å°‘éœ€è¦5ä¸ªæœ‰æ•ˆå€¼
                continue
            
            # Z-Scoreæ ‡å‡†åŒ–
            mean_val = valid_values.mean()
            std_val = valid_values.std()
            
            if std_val > 1e-8:  # é¿å…é™¤é›¶
                standardized_values = (valid_values - mean_val) / std_val
                standardized_df.loc[date, valid_values.index] = standardized_values
        
        logger.debug("  âœ… å› å­æ ‡å‡†åŒ–å®Œæˆ")
        return standardized_df

    def _adjust_ic_stats_by_r_squared(
            self,
            original_ic_stats: Dict[str, Dict],
            avg_r_squared: float,
            orthogonal_factor_name: str
    ) -> Dict[str, Dict]:
        """
        åŸºäºRÂ²è°ƒæ•´æ­£äº¤åŒ–å› å­çš„ICç»Ÿè®¡ (V2 - æœ€ç»ˆç”Ÿäº§ç‰ˆ)

        æ ¸å¿ƒç†è®ºï¼šæ ‡å‡†å·®/ç›¸å…³æ€§åˆ†è§£ (Std Dev / Correlation Decomposition)
        æ­¤ç‰ˆæœ¬ç»è¿‡ä¸¥æ ¼å®¡æŸ¥å’ŒåŠ å›ºï¼Œè§£å†³äº†IC IRä¸å˜æ€§ã€p-valueé™¤é›¶ä¿æŠ¤ç­‰é—®é¢˜ã€‚

        è°ƒæ•´å…¬å¼ï¼š
        - æ ¸å¿ƒè°ƒæ•´ç³»æ•°: adjustment_factor = sqrt(1 - RÂ²)
        - IC/Std/T-statç±»æŒ‡æ ‡: adjusted = original * adjustment_factor
        - IC IR: ç†è®ºä¸Šä¸å˜ (å› å­è¢«æŠµæ¶ˆ)
        - èƒœç‡è°ƒæ•´: 0.5 + (original_rate - 0.5) * adjustment_factor
        - på€¼è°ƒæ•´: min(1.0, p_value / (adjustment_factor + eps))

        Args:
            original_ic_stats: åŸå§‹å› å­çš„ICç»Ÿè®¡æ•°æ®
            avg_r_squared: å¹³å‡RÂ²å€¼ï¼ˆæ¥è‡ªé€æ—¥å›å½’ï¼‰
            orthogonal_factor_name: æ­£äº¤åŒ–å› å­åç§°ï¼ˆç”¨äºæ—¥å¿—ï¼‰

        Returns:
            è°ƒæ•´åçš„ICç»Ÿè®¡æ•°æ®
        """
        epsilon = 1e-8  # ç”¨äºé˜²æ­¢é™¤é›¶é”™è¯¯

        # --- 1. å¥å…¨æ€§æ£€æŸ¥ ---
        if not (0 < avg_r_squared < 1):
            raise ValueError(
                f"  âš ï¸ {orthogonal_factor_name}: å¼‚å¸¸æˆ–æ— æ•ˆçš„RÂ²å€¼({avg_r_squared:.3f})ï¼Œæ— æ³•è¿›è¡Œè°ƒæ•´ï¼Œå°†ä½¿ç”¨åŸå§‹ICç»Ÿè®¡ã€‚")

        # --- 2. è®¡ç®—æ ¸å¿ƒè°ƒæ•´ç³»æ•° ---
        adjustment_factor = np.sqrt(1 - avg_r_squared)
        logger.debug(f"  ğŸ“Š {orthogonal_factor_name}: RÂ²={avg_r_squared:.3f}, ç»Ÿä¸€è°ƒæ•´ç³»æ•°={adjustment_factor:.3f}")

        adjusted_ic_stats = {}

        # --- 3. é€ä¸ªé¢„æµ‹å‘¨æœŸè¿›è¡Œè°ƒæ•´ ---
        for period, period_stats in original_ic_stats.items():
            # ä½¿ç”¨ .copy() æ˜¯ä¸€ä¸ªå¥½ä¹ æƒ¯ï¼Œé¿å…æ„å¤–ä¿®æ”¹åŸå§‹è¾“å…¥å­—å…¸
            adjusted_period_stats = period_stats.copy()

            # 3.1: ç›´æ¥ä¹˜ä»¥è°ƒæ•´ç³»æ•°çš„æŒ‡æ ‡ (çº¿æ€§ç›¸å…³ç±»)
            keys_to_scale = [
                'ic_mean', 'ic_std', 'ic_volatility',
                'ic_t_stat', 'ic_nw_t_stat'
            ]
            # è‡ªåŠ¨åŒ…å«æ‰€æœ‰ä»¥ _score ç»“å°¾çš„è¯„åˆ†é¡¹
            score_keys = [k for k in adjusted_period_stats if k.endswith('_score')]
            keys_to_scale.extend(score_keys)

            for key in keys_to_scale:
                if key in adjusted_period_stats and isinstance(adjusted_period_stats[key], (int, float)):
                    adjusted_period_stats[key] *= adjustment_factor

            # 3.2: å…·æœ‰ç‰¹æ®Šè°ƒæ•´é€»è¾‘çš„æŒ‡æ ‡
            # èƒœç‡(Win Rate): å‘50%åŸºå‡†è¿›è¡Œæ”¶ç¼©
            if 'ic_win_rate' in adjusted_period_stats:
                original_win_rate = adjusted_period_stats['ic_win_rate']
                adjusted_period_stats['ic_win_rate'] = 0.5 + (original_win_rate - 0.5) * adjustment_factor

            # På€¼(P-values): åå‘è°ƒæ•´ï¼Œå¹¶ç”¨epsilonè¿›è¡Œé™¤é›¶ä¿æŠ¤
            p_value_keys = ['ic_p_value', 'ic_nw_p_value']
            for key in p_value_keys:
                if key in adjusted_period_stats:
                    original_p_value = adjusted_period_stats[key]
                    adjusted_period_stats[key] = min(1.0, original_p_value / (adjustment_factor + epsilon))

            # 3.3: ç†è®ºä¸Šä¿æŒä¸å˜çš„æŒ‡æ ‡
            # IC IR = ICå‡å€¼ / ICæ ‡å‡†å·®ã€‚è°ƒæ•´å› å­åœ¨åˆ†å­åˆ†æ¯ä¸Šè¢«æŠµæ¶ˆï¼Œæ•…IRç†è®ºä¸Šä¸å˜ã€‚
            # å…¶ä»–å¦‚æ ·æœ¬æ•°ã€æœ€å¤§/æœ€å°å€¼ç­‰ç»Ÿè®¡é‡ä¹Ÿä¿æŒä¸å˜ã€‚
            keys_to_keep = ['ic_ir', 'ic_count', 'ic_max', 'ic_min']
            # (æˆ‘ä»¬å› ä¸ºä½¿ç”¨äº†.copy()ï¼Œæ‰€ä»¥æ— éœ€é¢å¤–ä»£ç ï¼Œè¿™äº›å€¼å·²è‡ªåŠ¨ä¿ç•™)

            adjusted_ic_stats[period] = adjusted_period_stats

        # --- 4. æ—¥å¿—è®°å½•è°ƒæ•´æ•ˆæœ ---
        # ä½¿ç”¨ .get() é“¾å¼è°ƒç”¨æ¥å®‰å…¨åœ°è·å–å€¼ï¼Œé¿å…KeyError
        original_main_ic = original_ic_stats.get(self.config.main_evaluation_period, {}).get('ic_mean', 0)
        adjusted_main_ic = adjusted_ic_stats.get(self.config.main_evaluation_period, {}).get('ic_mean', 0)

        adjustment_magnitude = (1 - adjustment_factor) * 100

        logger.info(f"  ğŸ”„ {orthogonal_factor_name}: ä¸»å‘¨æœŸICè°ƒæ•´ {original_main_ic:.4f} -> {adjusted_main_ic:.4f}")
        logger.info(
            f"      ç†è®ºä¾æ®: sqrt(1-RÂ²)={adjustment_factor:.3f}, ICé¢„æµ‹åŠ›ä¸‹é™å¹…åº¦: {adjustment_magnitude:.1f}%")

        return adjusted_ic_stats


    #todo çœ‹è¿™é‡Œ
    def synthesize_with_orthogonalization(
            self,
            composite_factor_name: str,
            candidate_factor_names: List[str],
            snap_config_id: str,
            force_generate_ic: bool = False
    ) -> Tuple[pd.DataFrame, Dict]:
        """
        å¸¦æ­£äº¤åŒ–çš„ä¸“ä¸šå› å­åˆæˆæµç¨‹
        
        å®Œæ•´æµç¨‹ï¼š
        1. ä¸“ä¸šç­›é€‰ï¼ˆçº¢è‰²åŒºåŸŸæ·˜æ±° + é»„è‰²åŒºåŸŸæ­£äº¤åŒ–è®¡åˆ’ï¼‰
        2. æ‰§è¡Œæ­£äº¤åŒ–æ”¹é€ è®¡åˆ’
        3. åŸºäºå¤„ç†åçš„å› å­è¿›è¡ŒICåŠ æƒåˆæˆ
        
        Args:
            composite_factor_name: å¤åˆå› å­åç§°
            candidate_factor_names: å€™é€‰å› å­åˆ—è¡¨
            snap_config_id: é…ç½®å¿«ç…§ID
            force_generate_ic: æ˜¯å¦å¼ºåˆ¶é‡æ–°ç”ŸæˆICæ•°æ®
            
        Returns:
            (composite_factor_df, synthesis_report)
        """
        logger.info(f"\nğŸš€ å¯åŠ¨å¸¦æ­£äº¤åŒ–çš„ä¸“ä¸šå› å­åˆæˆ: {composite_factor_name}")
        logger.info(f"ğŸ“Š å€™é€‰å› å­æ•°é‡: {len(candidate_factor_names)}")
        
        # 1. åˆå§‹åŒ–ä¸“ä¸šç­›é€‰å™¨
        if self.factor_selector is None:
            self.factor_selector = FactorSelector(snap_config_id, self.selector_config)
            logger.info("âœ… æ»šåŠ¨ICå› å­ç­›é€‰å™¨åˆå§‹åŒ–å®Œæˆ")
        
        # 2. æ‰§è¡Œå®Œæ•´çš„ä¸“ä¸šç­›é€‰æµç¨‹ï¼ˆåŒ…å«æ­£äº¤åŒ–è®¡åˆ’ç”Ÿæˆï¼‰
        selected_factors, selection_report = self.factor_selector.run_complete_selection(
            factor_names = candidate_factor_names, force_generate = force_generate_ic
        )
        
        if not selected_factors:
            raise ValueError("âŒ ä¸“ä¸šç­›é€‰æœªé€‰å‡ºä»»ä½•å› å­ï¼Œæ— æ³•è¿›è¡Œåˆæˆ")
        
        # 3. è·å–æ­£äº¤åŒ–è®¡åˆ’
        orthogonalization_plan = selection_report.get('correlation_control').get('orthogonalized_factors',[])
        logger.info(f"ğŸ“‹ è·å–åˆ° {len(orthogonalization_plan)} é¡¹æ­£äº¤åŒ–è®¡åˆ’")
        
        # 4. æ‰§è¡Œæ­£äº¤åŒ–æ”¹é€ 
        config_manager = ConfigSnapshotManager()
        pool_index, start_date, end_date, config_evaluation = config_manager.get_snapshot_config_content_details(snap_config_id)
        
        orthogonal_factors = {}
        if orthogonalization_plan:
            orthogonal_factors = self.execute_orthogonalization_plan(
                orthogonalization_plan, pool_index, snap_config_id
            )
        
        # 5. æ„å»ºæœ€ç»ˆå› å­åˆ—è¡¨ï¼ˆåŸå§‹ç­›é€‰å› å­ + æ­£äº¤åŒ–å› å­ï¼‰
        final_factor_list = selected_factors.copy()
        
        # æ›¿æ¢è¢«æ­£äº¤åŒ–çš„å› å­
        for plan_item in orthogonalization_plan:
            original_factor = plan_item['original_factor']
            orthogonal_name = plan_item['orthogonal_name']
            
            if original_factor in final_factor_list and orthogonal_name in orthogonal_factors:
                final_factor_list.remove(original_factor)
                final_factor_list.append(orthogonal_name)
                logger.info(f"ğŸ”„ å› å­æ›¿æ¢: {original_factor} -> {orthogonal_name}")
        
        logger.info(f"ğŸ¯ æœ€ç»ˆå› å­åˆ—è¡¨: {len(final_factor_list)} ä¸ªå› å­")
        
        # 6. åŸºäºæœ€ç»ˆå› å­åˆ—è¡¨è®¡ç®—ICæƒé‡ï¼ˆä¿®æ­£åçš„é€»è¾‘ï¼‰
        factor_ic_stats = {}
        for factor_name in final_factor_list:
            try:
                # æ£€æŸ¥æ˜¯å¦ä¸ºæ­£äº¤åŒ–å› å­
                is_orthogonal_factor = False
                original_factor = factor_name
                avg_r_squared = 0.0
                
                # æŸ¥æ‰¾å¯¹åº”çš„æ­£äº¤åŒ–è®¡åˆ’é¡¹
                for plan_item in orthogonalization_plan:
                    if plan_item['orthogonal_name'] == factor_name:
                        is_orthogonal_factor = True
                        original_factor = plan_item['original_factor']
                        avg_r_squared = plan_item.get('avg_r_squared', 0.0)
                        break
                
                # åŠ è½½åŸå§‹å› å­çš„ICç»Ÿè®¡
                ic_stats = self._load_factor_ic_stats(
                    original_factor, pool_index, snap_config_id=snap_config_id
                )
                
                if ic_stats:
                    if is_orthogonal_factor and avg_r_squared > 0:
                        # ğŸ¯ æ ¸å¿ƒä¿®æ­£ï¼šåŸºäºRÂ²è°ƒæ•´æ­£äº¤åŒ–å› å­çš„ICç»Ÿè®¡
                        logger.info(f"  ğŸ”§ æ­£äº¤åŒ–å› å­ICè°ƒæ•´: {factor_name}")
                        adjusted_ic_stats = self._adjust_ic_stats_by_r_squared(
                            ic_stats, avg_r_squared, factor_name
                        )
                        factor_ic_stats[factor_name] = adjusted_ic_stats
                    else:
                        # åŸå§‹å› å­ç›´æ¥ä½¿ç”¨
                        factor_ic_stats[factor_name] = ic_stats
                        logger.debug(f"  ğŸ“Š åŸå§‹å› å­: {factor_name}")
                    
            except Exception as e:
                logger.error(f"  âŒ {factor_name}: ICç»Ÿè®¡å¤„ç†å¼‚å¸¸ - {e}")
        
        # 7. è®¡ç®—æœ€ç»ˆæƒé‡
        if factor_ic_stats:
            factor_weights = self.weight_calculator.calculate_ic_based_weights(factor_ic_stats)
        else:
            raise ValueError("âš ï¸ æ— æ³•è·å–ICç»Ÿè®¡ï¼Œä½¿ç”¨ç­‰æƒé‡åˆæˆ")
            # equal_weight = 1.0 / len(final_factor_list)
            # factor_weights = {name: equal_weight for name in final_factor_list}
        
        # 8. æ‰§è¡ŒåŠ æƒåˆæˆï¼ˆæ”¯æŒæ­£äº¤åŒ–å› å­ï¼‰
        composite_factor_df = self._execute_weighted_synthesis_with_orthogonal(
            composite_factor_name, pool_index, factor_weights, orthogonal_factors, snap_config_id
        )
        
        # 9. ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        synthesis_report = self._generate_orthogonalization_report(
            composite_factor_name,
            candidate_factor_names,
            selected_factors,
            final_factor_list,
            factor_weights,
            orthogonalization_plan,
            selection_report
        )
        
        logger.info(f"âœ… å¸¦æ­£äº¤åŒ–çš„ä¸“ä¸šå› å­åˆæˆå®Œæˆ: {composite_factor_name}")
        return composite_factor_df, synthesis_report

    def _execute_weighted_synthesis_with_orthogonal(
            self,
            composite_factor_name: str,
            stock_pool_index_name: str,
            factor_weights: Dict[str, float],
            orthogonal_factors: Dict[str, pd.DataFrame],
            snap_config_id: str
    ) -> pd.DataFrame:
        """
        æ”¯æŒæ­£äº¤åŒ–å› å­çš„åŠ æƒåˆæˆ
        
        Args:
            composite_factor_name: å¤åˆå› å­åç§°
            stock_pool_index_name: è‚¡ç¥¨æ± åç§°
            factor_weights: å› å­æƒé‡
            orthogonal_factors: æ­£äº¤åŒ–å› å­æ•°æ® {name: df}
            snap_config_id: é…ç½®å¿«ç…§ID
            
        Returns:
            åˆæˆåçš„å› å­DataFrame
        """
        logger.info(f"âš–ï¸ å¼€å§‹æ‰§è¡Œæ”¯æŒæ­£äº¤åŒ–çš„åŠ æƒåˆæˆï¼Œä½¿ç”¨{len(factor_weights)}ä¸ªå› å­")
        
        processed_factors = []
        weights_list = []
        
        for factor_name, weight in factor_weights.items():
            logger.info(f"  ğŸ”„ å¤„ç†å› å­: {factor_name} (æƒé‡: {weight:.3f})")
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºæ­£äº¤åŒ–å› å­
            if factor_name in orthogonal_factors:
                logger.debug(f"    ğŸ“ ä½¿ç”¨æ­£äº¤åŒ–å› å­æ•°æ®: {factor_name}")
                processed_df = orthogonal_factors[factor_name]
            else:
                logger.debug(f"    ğŸ“Š ä»æœ¬åœ°åŠ è½½åŸå§‹å› å­: {factor_name}")
                processed_df = self.get_sub_factor_df_from_local(factor_name, stock_pool_index_name, snap_config_id)
            
            if processed_df is not None and not processed_df.empty:
                # --- æ ¸å¿ƒé€»è¾‘ï¼šå› å­æ–¹å‘ç»Ÿä¸€ ---
                # ä»ICç»Ÿè®¡æ•°æ®ä¸­è·å–è¯¥å› å­çš„ICå‡å€¼
                direction= get_new_factor_direction(factor_name)
                if direction < 0:
                    logger.info(f"    â¬‡ï¸  æ£€æµ‹åˆ°è´Ÿå‘å› å­ {factor_name}ï¼Œè¿›è¡Œæ–¹å‘ç¿»è½¬ * -1")
                    processed_df *= -1
                else:
                    logger.debug(f"    â¬†ï¸  æ­£å‘å› å­{factor_name}ï¼Œæ— éœ€å¤„ç†")
                # --- æ–¹å‘ç»Ÿä¸€ç»“æŸ ---
                processed_factors.append(processed_df)
                weights_list.append(weight)
            else:
                logger.warning(f"    âš ï¸ å› å­æ•°æ®æ— æ•ˆï¼Œè·³è¿‡: {factor_name}")
        
        if not processed_factors:
            raise ValueError("æ²¡æœ‰ä»»ä½•å› å­è¢«æˆåŠŸå¤„ç†")
        
        # åŠ æƒåˆæˆ
        composite_factor_df = self._weighted_combine_factors(processed_factors, weights_list)
        
        # æœ€ç»ˆæ ‡å‡†åŒ–
        composite_factor_df = self.processor._standardize_robust(composite_factor_df)
        
        logger.info(f"âœ… æ”¯æŒæ­£äº¤åŒ–çš„åŠ æƒåˆæˆå®Œæˆ: {composite_factor_name}")
        return composite_factor_df

    def _generate_orthogonalization_report(
            self,
            composite_factor_name: str,
            candidate_factors: List[str],
            selected_factors: List[str],
            final_factor_list: List[str],
            factor_weights: Dict[str, float],
            orthogonalization_plan: List[Dict],
            selection_report: Dict
    ) -> Dict:
        """
        ç”ŸæˆåŒ…å«æ­£äº¤åŒ–ä¿¡æ¯çš„ç»¼åˆæŠ¥å‘Š
        
        Args:
            composite_factor_name: å¤åˆå› å­åç§°
            candidate_factors: å€™é€‰å› å­åˆ—è¡¨
            selected_factors: åˆæ­¥ç­›é€‰å› å­åˆ—è¡¨
            final_factor_list: æœ€ç»ˆå› å­åˆ—è¡¨ï¼ˆç»æ­£äº¤åŒ–å¤„ç†åï¼‰
            factor_weights: æœ€ç»ˆæƒé‡
            orthogonalization_plan: æ­£äº¤åŒ–è®¡åˆ’
            selection_report: ç­›é€‰æŠ¥å‘Š
            
        Returns:
            ç»¼åˆæŠ¥å‘Š
        """
        # åŸºç¡€æŠ¥å‘Š
        base_report = self._generate_comprehensive_report(
            composite_factor_name, candidate_factors, final_factor_list, factor_weights, selection_report
        )
        
        # æ·»åŠ æ­£äº¤åŒ–ä¿¡æ¯
        orthogonalization_info = {
            'orthogonalization_enabled': True,
            'orthogonalization_plan_count': len(orthogonalization_plan),
            'orthogonalization_details': []
        }
        
        for plan_item in orthogonalization_plan:
            orthogonalization_info['orthogonalization_details'].append({
                'original_factor': plan_item['original_factor'],
                'base_factor': plan_item['base_factor'],
                'orthogonal_name': plan_item['orthogonal_name'],
                'original_correlation': plan_item.get('correlation', 0),
                'base_score': plan_item.get('base_score', 0),
                'target_score': plan_item.get('target_score', 0)
            })
        
        # åˆå¹¶æŠ¥å‘Š
        comprehensive_report = {
            **base_report,
            'orthogonalization': orthogonalization_info,
            'factor_transformation_summary': {
                'initial_selected_count': len(selected_factors),
                'orthogonalized_count': len(orthogonalization_plan),
                'final_factor_count': len(final_factor_list),
                'replacement_mapping': {
                    plan['original_factor']: plan['orthogonal_name']
                    for plan in orthogonalization_plan
                }
            }
        }
        
        return comprehensive_report

if __name__ == '__main__':
    data_manager = DataManager()
    factor_manager= FactorManager(data_manager)
    from projects._03_factor_selection.factor_manager.factor_analyzer.factor_analyzer import FactorAnalyzer

    factor_analyzer = FactorAnalyzer(factor_manager)
    factor_processor = FactorProcessor(factor_manager.data_manager.config)
    iCWeightedSynthesizer = (ICWeightedSynthesizer(factor_manager, factor_analyzer, factor_processor))
    ret  = {
        'log_circ_mv': iCWeightedSynthesizer._load_factor_ic_stats('log_circ_mv', '000906',snap_config_id = '20250906_045625_05e460ab')
    }
    iCWeightedSynthesizer.weight_calculator.calculate_ic_based_weights(ret)

    # (ICWeightedSynthesizer(factor_manager, factor_analyzer, factor_processor).synthesize_with_orthogonalization
    #  (composite_factor_name='composite_factor_name',candidate_factor_names=['volatility_40d','sp_ratio','earnings_stability','cfp_ratio','ep_ratio']
    #   ,snap_config_id= '20250825_091622_98ed2d08',force_generate_ic=False))

    ##todo åˆæˆå¥½çš„å› å­åœ¨è¿›å…¥ icæµ‹è¯•!! ç›´æ¥ç”¨æœ¬åœ°çš„closeæ•°æ®å°±è¡Œ