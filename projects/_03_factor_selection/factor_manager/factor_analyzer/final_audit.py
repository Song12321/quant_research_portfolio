# final_audit.py
import numpy as np
import pandas as pd


def final_audit(factor_data_path):
    """åœ¨æ— èŒç¯å¢ƒä¸­ï¼Œå¯¹æˆªè·çš„æ•°æ®è¿›è¡Œæœ€ç»ˆæ£€éªŒ"""

    # --- 1. åŠ è½½"è¯ç‰©" ---
    print("--- æ­£åœ¨åŠ è½½æ•°æ®å¿«ç…§ ---")
    factor_df = pd.read_parquet(factor_data_path)
    price_df = pd.read_parquet(
        'D:\lqs\codeAbout\py\Quantitative\quant_research_portfolio\projects\_03_factor_selection\debug_snapshot/price_for_returns.parquet')

    # --- 2. ç”¨"è¯ç‰©"ä¸­çš„ä»·æ ¼ï¼Œé‡æ–°è®¡ç®—æ”¶ç›Šç‡ ---
    #    å®Œå…¨å¤åˆ» calcu_forward_returns_close_close çš„é€»è¾‘
    print("--- æ­£åœ¨é‡æ–°è®¡ç®—20æ—¥ C2C æœªæ¥æ”¶ç›Š ---")
    period = 20

    # 1. å®šä¹‰èµ·ç‚¹å’Œç»ˆç‚¹ä»·æ ¼ (ä¸¥æ ¼éµå¾ªT-1åŸåˆ™)
    start_price = price_df.shift(1)
    end_price = price_df.shift(1 - period)

    # 2. åˆ›å»º"æœªæ¥å­˜ç»­"æ©ç  (ç¡®ä¿åœ¨æŒæœ‰æœŸé¦–å°¾è‚¡ä»·éƒ½å­˜åœ¨)
    survived_mask = start_price.notna() & end_price.notna()

    # 3. è®¡ç®—åŸå§‹æ”¶ç›Šç‡ï¼Œå¹¶åº”ç”¨æ©ç è¿‡æ»¤
    forward_returns_raw = end_price / start_price - 1
    forward_returns = forward_returns_raw.where(survived_mask)

    # 4. clip æ“ä½œåº”è¯¥åœ¨æ‰€æœ‰è®¡ç®—å’Œè¿‡æ»¤å®Œæˆåè¿›è¡Œ
    forward_returns = forward_returns.clip(-0.15, 0.15)

    # --- 3. æœ€ç»ˆå¯¹å†³ï¼šç›´æ¥è®¡ç®—æˆªé¢ Spearman ç›¸å…³æ€§ ---
    print("--- æ­£åœ¨è®¡ç®—å› å­ä¸æœªæ¥æ”¶ç›Šçš„å•è°ƒæ€§ ---")
    daily_corr = factor_df.corrwith(forward_returns, axis=1, method='spearman')

    # --- 4. æœ€ç»ˆå®¡åˆ¤ ---
    avg_monotonicity = daily_corr.mean()

    print("\n" + "=" * 30 + " ã€æœ€ç»ˆå®¡åˆ¤ç»“æœã€‘ " + "=" * 30)
    print(f"åœ¨éš”ç¦»çš„æ— èŒç¯å¢ƒä¸­ï¼Œå› å­ä¸æœªæ¥æ”¶ç›Šçš„å¹³å‡å•è°ƒæ€§ä¸º: {avg_monotonicity:.6f}")
    print("=" * 80)

    if abs(avg_monotonicity) > 0.5:
        print(
            "âœ— ç»“è®ºï¼šå¹½çµä¾ç„¶å­˜åœ¨ï¼è¿™æ„å‘³ç€ä½ ä¼ å…¥`comprehensive_test`çš„`factor_df`æˆ–`returns_calculator`åœ¨åˆ›å»ºæ—¶å°±å·²ç»è¢«æ±¡æŸ“ï¼")
        print("  è¯·å›å¤´æ£€æŸ¥ä½ çš„`FactorCalculator`å’Œ`prepare_date_for_entity_service`ï¼")
    else:
        print(
            "âœ“ ç»“è®ºï¼šå¹½çµæ¶ˆå¤±äº†ï¼è¿™æ„å‘³ç€ä½ çš„ä¸Šæ¸¸æ•°æ®å’Œè®¡ç®—å…¨éƒ¨æ­£ç¡®ï¼Œbugè—åœ¨ä½ `core_three_test`ä¸‹æ¸¸çš„æŸä¸ªå…·ä½“æµ‹è¯•å‡½æ•°ï¼ˆå¦‚`calculate_quantile_returns`ï¼‰çš„å®ç°ä¸­ï¼")

    # --- 5. é¢å¤–è¯Šæ–­ä¿¡æ¯ ---
    print(f"\nğŸ” è¯Šæ–­ä¿¡æ¯:")
    print(f"  å› å­æ•°æ®å½¢çŠ¶: {factor_df.shape}")
    print(f"  æ”¶ç›Šç‡æ•°æ®å½¢çŠ¶: {forward_returns.shape}")
    print(f"  æœ‰æ•ˆç›¸å…³æ€§æ•°é‡: {daily_corr.notna().sum()}")
    print(f"  ç›¸å…³æ€§æ ‡å‡†å·®: {daily_corr.std():.6f}")


def debug_spearman_calculation(quantile_means):
    """è°ƒè¯•Spearmanè®¡ç®—"""
    from scipy.stats import spearmanr

    # å®é™…çš„åˆ†ä½æ•°å‡å€¼
    quantile_ranks = list(range(1, 6))  # [1, 2, 3, 4, 5]

    print("ğŸ” æ‰‹åŠ¨éªŒè¯Spearmanè®¡ç®—:")
    print(f"  åˆ†ä½æ•°åºå·: {quantile_ranks}")
    print(f"  åˆ†ä½æ•°å‡å€¼: {quantile_means}")

    # æ‰‹åŠ¨è®¡ç®—Spearman
    corr, p_value = spearmanr(quantile_ranks, quantile_means)
    print(f"  Spearmanç›¸å…³æ€§: {corr:.6f}")

    # æ£€æŸ¥æ’å
    import numpy as np
    means_ranks = np.argsort(np.argsort(quantile_means)) + 1
    print(f"  å‡å€¼çš„æ’å: {means_ranks}")
    print(f"  åˆ†ä½æ•°æ’å: {quantile_ranks}")

    # æ‰‹åŠ¨è®¡ç®—æ’åç›¸å…³æ€§
    rank_corr = np.corrcoef(quantile_ranks, means_ranks)[0, 1]
    print(f"  æ‰‹åŠ¨æ’åç›¸å…³æ€§: {rank_corr:.6f}")


def compare_daily_correlations(factor_df, forward_returns, n_dates=10):
    """æ¯”è¾ƒå¤šä¸ªæ—¥æœŸçš„ç›´æ¥ç›¸å…³æ€§å’Œåˆ†å±‚å•è°ƒæ€§"""

    print("ğŸ” æ¯”è¾ƒå¤šä¸ªæ—¥æœŸçš„ç›¸å…³æ€§æ¨¡å¼:")

    common_dates = factor_df.index.intersection(forward_returns.index)[:n_dates]

    for i, date in enumerate(common_dates):
        factor_values = factor_df.loc[date].dropna()
        return_values = forward_returns.loc[date].dropna()

        common_stocks = factor_values.index.intersection(return_values.index)
        if len(common_stocks) < 100:
            continue

        factor_common = factor_values[common_stocks]
        return_common = return_values[common_stocks]

        # ç›´æ¥ç›¸å…³æ€§
        direct_corr = factor_common.corr(return_common, method='spearman')

        # åˆ†å±‚å•è°ƒæ€§
        try:
            quantiles = pd.qcut(factor_common, 5, labels=False, duplicates='drop') + 1
            df_temp = pd.DataFrame({'factor': factor_common, 'return': return_common, 'quantile': quantiles})
            group_means = df_temp.groupby('quantile')['return'].mean()

            from scipy.stats import spearmanr
            layer_mono, _ = spearmanr(range(len(group_means)), group_means.values)

            print(
                f"  ğŸ“… {date.strftime('%Y-%m-%d')}: ç›´æ¥={direct_corr:.4f}, åˆ†å±‚={layer_mono:.4f}, å·®å¼‚={abs(direct_corr - layer_mono):.4f}")

        except Exception as e:
            print(f"  âŒ {date}: è®¡ç®—å¤±è´¥ - {e}")


def debug_perfect_monotonicity(factor_df, forward_returns):
    """è°ƒè¯•å®Œç¾å•è°ƒæ€§é—®é¢˜"""

    print("ğŸš¨ è°ƒè¯•å®Œç¾å•è°ƒæ€§å¼‚å¸¸...")

    # é€‰æ‹©ç¬¬ä¸€ä¸ªæ—¥æœŸè¿›è¡Œè¯¦ç»†åˆ†æ
    test_date = factor_df.index[2]
    factor_values = factor_df.loc[test_date].dropna()
    return_values = forward_returns.loc[test_date].dropna()

    common_stocks = factor_values.index.intersection(return_values.index)
    factor_common = factor_values[common_stocks]
    return_common = return_values[common_stocks]

    print(f"ğŸ” è°ƒè¯•æ—¥æœŸ: {test_date}")
    print(f"  è‚¡ç¥¨æ•°é‡: {len(common_stocks)}")

    # åˆ†ç»„åˆ†æ
    try:
        quantiles = pd.qcut(factor_common, 5, labels=False, duplicates='drop') + 1
        df_temp = pd.DataFrame({
            'factor': factor_common,
            'return': return_common,
            'quantile': quantiles
        })

        # æ£€æŸ¥æ¯ä¸ªåˆ†ä½æ•°çš„è¯¦ç»†ä¿¡æ¯
        print(f"  ğŸ“Š å„åˆ†ä½æ•°è¯¦ç»†ä¿¡æ¯:")
        group_stats = df_temp.groupby('quantile').agg({
            'factor': ['count', 'mean', 'std', 'min', 'max'],
            'return': ['mean', 'std', 'min', 'max']
        }).round(6)

        print(group_stats)

        # è®¡ç®—ç»„å‡å€¼
        group_means = df_temp.groupby('quantile')['return'].mean()
        print(f"  ğŸ¯ å„ç»„å¹³å‡æ”¶ç›Š: {group_means.values}")

        # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰ç»„æ”¶ç›Šéƒ½ç›¸åŒ
        unique_means = len(group_means.unique())
        print(f"  ğŸš¨ å”¯ä¸€å‡å€¼æ•°é‡: {unique_means}")

        if unique_means == 1:
            print(f"  âŒ æ‰€æœ‰ç»„æ”¶ç›Šå®Œå…¨ç›¸åŒ: {group_means.iloc[0]}")
            print(f"  ğŸ” è¿™è§£é‡Šäº†ä¸ºä»€ä¹ˆå•è°ƒæ€§è®¡ç®—å¼‚å¸¸ï¼")

            # è¿›ä¸€æ­¥æ£€æŸ¥åŸå§‹æ”¶ç›Šæ•°æ®
            print(f"  ğŸ“ˆ åŸå§‹æ”¶ç›Šç»Ÿè®¡:")
            print(f"    å”¯ä¸€æ”¶ç›Šå€¼æ•°é‡: {return_common.nunique()}")
            print(f"    æ”¶ç›Šå€¼èŒƒå›´: [{return_common.min():.8f}, {return_common.max():.8f}]")
            print(f"    æ˜¯å¦æ‰€æœ‰æ”¶ç›Šéƒ½ç›¸åŒ: {return_common.nunique() == 1}")

        # æ‰‹åŠ¨è®¡ç®—Spearman
        from scipy.stats import spearmanr
        mono_corr, _ = spearmanr(range(len(group_means)), group_means.values)
        print(f"  ğŸ“Š æ‰‹åŠ¨è®¡ç®—å•è°ƒæ€§: {mono_corr}")

    except Exception as e:
        print(f"  âŒ åˆ†ç»„å¤±è´¥: {e}")


def debug_returns_calculation_detailed(price_df, period=20):
    """è¯¦ç»†è°ƒè¯•æ”¶ç›Šç‡è®¡ç®—"""

    print(f"ğŸ” è¯¦ç»†è°ƒè¯• {period} æ—¥æ”¶ç›Šç‡è®¡ç®—...")

    # å¤åˆ»ä½ çš„è®¡ç®—é€»è¾‘
    start_price = price_df.shift(1)
    end_price = price_df.shift(1 - period)

    # æ£€æŸ¥åŸå§‹ä»·æ ¼æ•°æ®
    print(f"ğŸ“Š ä»·æ ¼æ•°æ®æ£€æŸ¥:")
    print(f"  price_df shape: {price_df.shape}")
    print(f"  ä»·æ ¼èŒƒå›´: [{price_df.min().min():.2f}, {price_df.max().max():.2f}]")

    # é€‰æ‹©ä¸€ä¸ªæµ‹è¯•æ—¥æœŸå’Œè‚¡ç¥¨
    test_date = price_df.index[50]  # é€‰æ‹©ä¸­é—´çš„æ—¥æœŸ
    test_stock = price_df.columns[0]  # é€‰æ‹©ç¬¬ä¸€åªè‚¡ç¥¨

    print(f"\nğŸ” æµ‹è¯•è‚¡ç¥¨ {test_stock} åœ¨æ—¥æœŸ {test_date}:")

    # è·å–å…·ä½“çš„ä»·æ ¼å€¼
    start_price_val = start_price.loc[test_date, test_stock]
    end_price_val = end_price.loc[test_date, test_stock]

    print(f"  èµ·å§‹ä»·æ ¼ (T-1): {start_price_val:.4f}")
    print(f"  ç»“æŸä»·æ ¼ (T-{period}): {end_price_val:.4f}")

    # è®¡ç®—æ”¶ç›Šç‡
    if pd.notna(start_price_val) and pd.notna(end_price_val) and start_price_val != 0:
        raw_return = end_price_val / start_price_val - 1
        print(f"  åŸå§‹æ”¶ç›Šç‡: {raw_return:.6f}")
        print(f"  åŸå§‹æ”¶ç›Šç‡ (%): {raw_return * 100:.4f}%")

        # æ£€æŸ¥æ˜¯å¦å¿˜è®°å‡1
        ratio_only = end_price_val / start_price_val
        print(f"  ä»…æ¯”ç‡ (æœªå‡1): {ratio_only:.6f}")

        # æ£€æŸ¥æ–¹å‘æ˜¯å¦æ­£ç¡®
        if end_price_val > start_price_val:
            print(f"  âœ“ ä»·æ ¼ä¸Šæ¶¨ï¼Œæ”¶ç›Šç‡åº”ä¸ºæ­£")
        else:
            print(f"  âœ“ ä»·æ ¼ä¸‹è·Œï¼Œæ”¶ç›Šç‡åº”ä¸ºè´Ÿ")

    # æ£€æŸ¥æ•´ä½“æ”¶ç›Šç‡åˆ†å¸ƒ
    forward_returns_raw = end_price / start_price - 1
    survived_mask = start_price.notna() & end_price.notna()
    forward_returns = forward_returns_raw.where(survived_mask)

    # ç»Ÿè®¡ä¿¡æ¯
    returns_flat = forward_returns.values.flatten()
    returns_flat = returns_flat[~np.isnan(returns_flat)]

    print(f"\nğŸ“Š æ•´ä½“æ”¶ç›Šç‡ç»Ÿè®¡:")
    print(f"  æ•°æ®ç‚¹æ•°é‡: {len(returns_flat)}")
    print(f"  å‡å€¼: {np.mean(returns_flat):.6f}")
    print(f"  ä¸­ä½æ•°: {np.median(returns_flat):.6f}")
    print(f"  æ ‡å‡†å·®: {np.std(returns_flat):.6f}")
    print(f"  æœ€å°å€¼: {np.min(returns_flat):.6f}")
    print(f"  æœ€å¤§å€¼: {np.max(returns_flat):.6f}")
    print(f"  [1%, 99%] åˆ†ä½æ•°: [{np.percentile(returns_flat, 1):.6f}, {np.percentile(returns_flat, 99):.6f}]")

#
# # è°ƒç”¨è¿™ä¸ªå‡½æ•°
# def debug_grouping_data_transformation(factor_df,close_df,  period=20):
#     """è°ƒè¯•åˆ†ç»„è¿‡ç¨‹ä¸­çš„æ•°æ®å˜æ¢"""
#
#     print("ğŸ” è°ƒè¯•åˆ†ç»„è¿‡ç¨‹ä¸­çš„æ•°æ®å˜æ¢...")
#
#     # 1. è·å–åŸå§‹æ”¶ç›Šç‡
#     forward_returns = calcu_forward_returns_close_close( period,close_df)
#     print(f"ğŸ“Š åŸå§‹æ”¶ç›Šç‡ç»Ÿè®¡:")
#     returns_flat = forward_returns.values.flatten()
#     returns_flat = returns_flat[~np.isnan(returns_flat)]
#     print(f"  å‡å€¼: {np.mean(returns_flat):.6f}")
#     print(f"  æ ‡å‡†å·®: {np.std(returns_flat):.6f}")
#     print(f"  èŒƒå›´: [{np.min(returns_flat):.6f}, {np.max(returns_flat):.6f}]")
#
#     # 2. é€‰æ‹©æµ‹è¯•æ—¥æœŸ
#     test_date = factor_df.index[0]
#     factor_values = factor_df.loc[test_date].dropna()
#     return_values = forward_returns.loc[test_date].dropna()
#
#     print(f"\nğŸ” æµ‹è¯•æ—¥æœŸ {test_date}:")
#     print(f"  æ”¶ç›Šç‡æ•°æ®ç»Ÿè®¡:")
#     print(f"    å‡å€¼: {return_values.mean():.6f}")
#     print(f"    æ ‡å‡†å·®: {return_values.std():.6f}")
#     print(f"    èŒƒå›´: [{return_values.min():.6f}, {return_values.max():.6f}]")
#     print(f"    å‰10ä¸ªå€¼: {return_values.head(10).values}")
#
#     # 3. åˆå¹¶æ•°æ®
#     common_stocks = factor_values.index.intersection(return_values.index)
#     factor_common = factor_values[common_stocks]
#     return_common = return_values[common_stocks]
#
#     print(f"\nğŸ“Š åˆå¹¶åæ•°æ®:")
#     print(f"  æ”¶ç›Šç‡ç»Ÿè®¡:")
#     print(f"    å‡å€¼: {return_common.mean():.6f}")
#     print(f"    æ ‡å‡†å·®: {return_common.std():.6f}")
#     print(f"    èŒƒå›´: [{return_common.min():.6f}, {return_common.max():.6f}]")
#
#     # 4. åˆ†ç»„
#     quantiles = pd.qcut(factor_common, 5, labels=False, duplicates='drop') + 1
#     df_temp = pd.DataFrame({
#         'factor': factor_common,
#         'return': return_common,
#         'quantile': quantiles
#     })
#
#     # 5. æ£€æŸ¥åˆ†ç»„åçš„åŸå§‹æ•°æ®
#     print(f"\nğŸ” åˆ†ç»„åå„ç»„åŸå§‹æ”¶ç›Šç‡æ£€æŸ¥:")
#     for q in range(1, 6):
#         group_data = df_temp[df_temp['quantile'] == q]['return']
#         print(f"  Q{q}: æ•°é‡={len(group_data)}, å‡å€¼={group_data.mean():.6f}, æ ‡å‡†å·®={group_data.std():.6f}")
#         print(f"       å‰5ä¸ªå€¼: {group_data.head().values}")
#
#         # æ£€æŸ¥æ˜¯å¦æœ‰å¼‚å¸¸å¤§çš„å€¼
#         extreme_values = group_data[abs(group_data) > 1.0]  # æ”¶ç›Šç‡>100%
#         if len(extreme_values) > 0:
#             print(f"       ğŸš¨ æç«¯å€¼æ•°é‡: {len(extreme_values)}, æœ€å¤§å€¼: {extreme_values.max():.6f}")


def debug_monotonicity_skip_nan_dates(factor_df, returns_calculator, period=20):
    """è·³è¿‡NaNæ—¥æœŸï¼Œç›´æ¥è°ƒè¯•å•è°ƒæ€§é—®é¢˜"""

    print("ğŸ¯ è·³è¿‡NaNæ—¥æœŸï¼Œç›´æ¥è°ƒè¯•å•è°ƒæ€§...")

    # è·å–æ”¶ç›Šç‡
    forward_returns = returns_calculator(period=period)

    # æ‰¾åˆ°æœ‰æ•°æ®çš„æ—¥æœŸï¼Œè·³è¿‡å‰é¢çš„NaN
    common_dates = factor_df.index.intersection(forward_returns.index)

    print(f"ğŸ“… æ€»æ—¥æœŸæ•°é‡: {len(common_dates)}")

    # ä»ç¬¬21ä¸ªæ—¥æœŸå¼€å§‹æ£€æŸ¥ï¼ˆç¡®ä¿æœ‰è¶³å¤Ÿçš„å†å²æ•°æ®ï¼‰
    start_idx = max(21, period + 1)  # ç¡®ä¿è·³è¿‡NaNæœŸ

    for i, test_date in enumerate(common_dates[start_idx:start_idx + 5]):  # æ£€æŸ¥5ä¸ªæ—¥æœŸ
        factor_values = factor_df.loc[test_date].dropna()
        return_values = forward_returns.loc[test_date].dropna()

        common_stocks = factor_values.index.intersection(return_values.index)

        if len(common_stocks) > 100:  # ç¡®ä¿æœ‰è¶³å¤Ÿæ•°æ®
            print(f"\nğŸ” æ—¥æœŸ: {test_date} (ç¬¬{start_idx + i + 1}ä¸ªæ—¥æœŸ), è‚¡ç¥¨æ•°é‡: {len(common_stocks)}")

            factor_common = factor_values[common_stocks]
            return_common = return_values[common_stocks]

            # ç›´æ¥ç›¸å…³æ€§
            direct_corr = factor_common.corr(return_common, method='spearman')

            # åˆ†å±‚å•è°ƒæ€§
            try:
                quantiles = pd.qcut(factor_common, 5, labels=False, duplicates='drop') + 1
                df_temp = pd.DataFrame({
                    'factor': factor_common,
                    'return': return_common,
                    'quantile': quantiles
                })

                group_means = df_temp.groupby('quantile')['return'].mean()

                from scipy.stats import spearmanr
                layer_mono, _ = spearmanr(range(len(group_means)), group_means.values)

                print(f"  ğŸ“Š ç›´æ¥ç›¸å…³æ€§: {direct_corr:.4f}")
                print(f"  ğŸ“Š åˆ†å±‚å•è°ƒæ€§: {layer_mono:.4f}")
                print(f"  ğŸ“Š å·®å¼‚: {abs(direct_corr - layer_mono):.4f}")
                print(f"  ğŸ¯ å„ç»„å‡å€¼: {group_means.values}")

                # å¦‚æœå‘ç°å¼‚å¸¸å•è°ƒæ€§
                if abs(layer_mono) > 0.99:
                    print(f"  ğŸš¨ å‘ç°å¼‚å¸¸å•è°ƒæ€§ï¼")

                    # æ£€æŸ¥æ”¶ç›Šç‡åˆ†å¸ƒ
                    print(f"  ğŸ“ˆ æ”¶ç›Šç‡ç»Ÿè®¡: å‡å€¼={return_common.mean():.6f}, æ ‡å‡†å·®={return_common.std():.6f}")
                    print(f"  ğŸ“ˆ æ”¶ç›Šç‡èŒƒå›´: [{return_common.min():.6f}, {return_common.max():.6f}]")

                    # æ£€æŸ¥å„ç»„çš„è¯¦ç»†ç»Ÿè®¡
                    for q in range(1, 6):
                        group_data = df_temp[df_temp['quantile'] == q]['return']
                        print(
                            f"    Q{q}: å‡å€¼={group_data.mean():.6f}, ä¸­ä½æ•°={group_data.median():.6f}, æ•°é‡={len(group_data)}")

            except Exception as e:
                print(f"  âŒ è®¡ç®—å¤±è´¥: {e}")
        else:
            print(f"  âš ï¸ æ—¥æœŸ {test_date}: æ•°æ®ä¸è¶³ï¼Œè·³è¿‡")


# è°ƒç”¨ä¿®æ­£åçš„å‡½æ•°
def debug_spearman_calculation_detailed(factor_df, returns_calculator, period=20):
    """è¯¦ç»†è°ƒè¯•Spearmanè®¡ç®—è¿‡ç¨‹"""

    print("ğŸ” è¯¦ç»†è°ƒè¯•Spearmanè®¡ç®—...")

    forward_returns = returns_calculator(period=period)
    common_dates = factor_df.index.intersection(forward_returns.index)

    # é€‰æ‹©ä¸€ä¸ªæµ‹è¯•æ—¥æœŸ
    test_date = common_dates[22]  # å¯¹åº”2024-01-16

    factor_values = factor_df.loc[test_date].dropna()
    return_values = forward_returns.loc[test_date].dropna()
    common_stocks = factor_values.index.intersection(return_values.index)

    factor_common = factor_values[common_stocks]
    return_common = return_values[common_stocks]

    print(f"ğŸ” æµ‹è¯•æ—¥æœŸ: {test_date}")
    print(f"ğŸ“Š è‚¡ç¥¨æ•°é‡: {len(common_stocks)}")

    # åˆ†ç»„
    quantiles = pd.qcut(factor_common, 5, labels=False, duplicates='drop') + 1
    df_temp = pd.DataFrame({
        'factor': factor_common,
        'return': return_common,
        'quantile': quantiles
    })

    group_means = df_temp.groupby('quantile')['return'].mean()
    print(f"ğŸ¯ å„ç»„å‡å€¼: {group_means.values}")

    # æ‰‹åŠ¨è®¡ç®—Spearmanç›¸å…³æ€§
    print(f"\nğŸ” æ‰‹åŠ¨è®¡ç®—Spearmanè¿‡ç¨‹:")

    # æ–¹æ³•1ï¼šä½¿ç”¨scipy
    from scipy.stats import spearmanr
    x_values = list(range(len(group_means)))  # [0, 1, 2, 3, 4]
    y_values = group_means.values

    print(f"  Xå€¼ (ç»„åºå·): {x_values}")
    print(f"  Yå€¼ (ç»„å‡å€¼): {y_values}")

    spearman_corr, p_value = spearmanr(x_values, y_values)
    print(f"  Scipyç»“æœ: ç›¸å…³æ€§={spearman_corr:.6f}, på€¼={p_value:.6f}")

    # æ–¹æ³•2ï¼šæ‰‹åŠ¨è®¡ç®—æ’å
    import numpy as np
    from scipy.stats import rankdata

    x_ranks = rankdata(x_values)
    y_ranks = rankdata(y_values)

    print(f"  Xæ’å: {x_ranks}")
    print(f"  Yæ’å: {y_ranks}")

    # è®¡ç®—Pearsonç›¸å…³æ€§ï¼ˆå¯¹æ’åï¼‰
    manual_corr = np.corrcoef(x_ranks, y_ranks)[0, 1]
    print(f"  æ‰‹åŠ¨è®¡ç®—: {manual_corr:.6f}")

    # æ–¹æ³•3ï¼šæ£€æŸ¥æ˜¯å¦æœ‰é‡å¤å€¼å½±å“
    unique_y = len(np.unique(y_values))
    print(f"  Yå€¼å”¯ä¸€æ•°é‡: {unique_y} / {len(y_values)}")

    if unique_y < len(y_values):
        print(f"  ğŸš¨ å­˜åœ¨é‡å¤çš„ç»„å‡å€¼ï¼")
        for i, val in enumerate(y_values):
            print(f"    ç»„{i + 1}: {val:.8f}")

    # æ–¹æ³•4ï¼šæ£€æŸ¥æ˜¯å¦å› ä¸ºæ•°å€¼ç²¾åº¦é—®é¢˜
    print(f"\nğŸ” æ•°å€¼ç²¾åº¦æ£€æŸ¥:")
    for i in range(len(y_values) - 1):
        diff = y_values[i + 1] - y_values[i]
        print(f"  ç»„{i + 1} -> ç»„{i + 2}: å·®å¼‚ = {diff:.8f}")


def find_perfect_monotonicity_dates(factor_df, returns_calculator, period=20):
    """å¯»æ‰¾å‡ºç°å®Œç¾å•è°ƒæ€§(1.0)çš„æ—¥æœŸ"""

    print("ğŸ¯ å¯»æ‰¾å®Œç¾å•è°ƒæ€§å¼‚å¸¸æ—¥æœŸ...")

    forward_returns = returns_calculator(period=period)
    common_dates = factor_df.index.intersection(forward_returns.index)

    perfect_dates = []

    for i, test_date in enumerate(common_dates[21:]):  # è·³è¿‡NaNæœŸ
        try:
            factor_values = factor_df.loc[test_date].dropna()
            return_values = forward_returns.loc[test_date].dropna()
            common_stocks = factor_values.index.intersection(return_values.index)

            if len(common_stocks) > 100:
                factor_common = factor_values[common_stocks]
                return_common = return_values[common_stocks]

                # åˆ†ç»„è®¡ç®—å•è°ƒæ€§
                quantiles = pd.qcut(factor_common, 5, labels=False, duplicates='drop') + 1
                df_temp = pd.DataFrame({
                    'factor': factor_common,
                    'return': return_common,
                    'quantile': quantiles
                })

                group_means = df_temp.groupby('quantile')['return'].mean()

                from scipy.stats import spearmanr
                mono_corr, _ = spearmanr(range(len(group_means)), group_means.values)

                # å¯»æ‰¾å®Œç¾å•è°ƒæ€§
                if abs(mono_corr) > 0.99:
                    perfect_dates.append((test_date, mono_corr, group_means.values))
                    print(f"ğŸš¨ å‘ç°å®Œç¾å•è°ƒæ€§ï¼æ—¥æœŸ: {test_date}, ç›¸å…³æ€§: {mono_corr:.6f}")
                    print(f"   å„ç»„å‡å€¼: {group_means.values}")

        except Exception as e:
            continue

    if perfect_dates:
        print(f"\nğŸ¯ æ€»å…±å‘ç° {len(perfect_dates)} ä¸ªå®Œç¾å•è°ƒæ€§æ—¥æœŸ")

        # è¯¦ç»†åˆ†æç¬¬ä¸€ä¸ªå¼‚å¸¸æ—¥æœŸ
        test_date, mono_corr, group_means = perfect_dates[0]
        print(f"\nğŸ” è¯¦ç»†åˆ†æå¼‚å¸¸æ—¥æœŸ: {test_date}")

        factor_values = factor_df.loc[test_date].dropna()
        return_values = forward_returns.loc[test_date].dropna()
        common_stocks = factor_values.index.intersection(return_values.index)

        factor_common = factor_values[common_stocks]
        return_common = return_values[common_stocks]

        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨æ•°æ®é—®é¢˜
        print(f"ğŸ“Š æ”¶ç›Šç‡è¯¦ç»†ç»Ÿè®¡:")
        print(f"  å”¯ä¸€å€¼æ•°é‡: {return_common.nunique()}")
        print(f"  æ˜¯å¦æ‰€æœ‰å€¼ç›¸åŒ: {return_common.nunique() == 1}")
        print(f"  æœ€å°å€¼: {return_common.min():.8f}")
        print(f"  æœ€å¤§å€¼: {return_common.max():.8f}")
        print(f"  æ ‡å‡†å·®: {return_common.std():.8f}")

        # æ£€æŸ¥åˆ†ç»„åçš„è¯¦ç»†æƒ…å†µ
        quantiles = pd.qcut(factor_common, 5, labels=False, duplicates='drop') + 1
        df_temp = pd.DataFrame({
            'factor': factor_common,
            'return': return_common,
            'quantile': quantiles
        })

        print(f"ğŸ“Š å„ç»„è¯¦ç»†ç»Ÿè®¡:")
        for q in range(1, 6):
            group_data = df_temp[df_temp['quantile'] == q]['return']
            print(f"  Q{q}: æ•°é‡={len(group_data)}, å‡å€¼={group_data.mean():.8f}")
            print(f"      æ ‡å‡†å·®={group_data.std():.8f}, å”¯ä¸€å€¼={group_data.nunique()}")

    else:
        print("âœ… æ²¡æœ‰å‘ç°å®Œç¾å•è°ƒæ€§å¼‚å¸¸")


def check_lookahead_bias(factor_df, returns_calculator, period=20):
    """æ£€æŸ¥æ˜¯å¦å­˜åœ¨å‰ç»æ€§åå·®"""

    print("ğŸ” æ£€æŸ¥å‰ç»æ€§åå·®...")

    forward_returns = returns_calculator(period=period)

    # é€‰æ‹©ä¸€ä¸ªå¼‚å¸¸æ—¥æœŸ
    test_date = pd.Timestamp('2024-02-02')

    print(f"ğŸ” åˆ†ææ—¥æœŸ: {test_date}")

    # æ£€æŸ¥å› å­å€¼çš„æ—¶é—´æˆ³
    factor_values = factor_df.loc[test_date].dropna()
    return_values = forward_returns.loc[test_date].dropna()

    print(f"ğŸ“Š å› å­æ•°æ®ç‚¹æ•°: {len(factor_values)}")
    print(f"ğŸ“Š æ”¶ç›Šç‡æ•°æ®ç‚¹æ•°: {len(return_values)}")

    # æ£€æŸ¥å› å­å€¼çš„åˆ†å¸ƒ
    print(f"ğŸ“Š å› å­å€¼ç»Ÿè®¡:")
    print(f"  æœ€å°å€¼: {factor_values.min():.6f}")
    print(f"  æœ€å¤§å€¼: {factor_values.max():.6f}")
    print(f"  å‡å€¼: {factor_values.mean():.6f}")
    print(f"  æ ‡å‡†å·®: {factor_values.std():.6f}")

    # æ£€æŸ¥æ”¶ç›Šç‡çš„æ—¶é—´çª—å£
    print(f"ğŸ“Š æ”¶ç›Šç‡æ—¶é—´çª—å£æ£€æŸ¥:")
    print(f"  å½“å‰æ—¥æœŸ: {test_date}")
    print(f"  æ”¶ç›Šç‡åº”è¯¥æ˜¯ä» {test_date} åˆ° {test_date + pd.Timedelta(days=period)} çš„æ”¶ç›Š")

    # æ£€æŸ¥æ˜¯å¦å› å­å€¼ä¸æœªæ¥æ”¶ç›Šç‡æœ‰å¼‚å¸¸ç›¸å…³æ€§
    common_stocks = factor_values.index.intersection(return_values.index)
    factor_common = factor_values[common_stocks]
    return_common = return_values[common_stocks]

    direct_corr = factor_common.corr(return_common, method='spearman')
    print(f"ğŸ“Š ç›´æ¥Spearmanç›¸å…³æ€§: {direct_corr:.6f}")

    # å¦‚æœç›¸å…³æ€§å¼‚å¸¸é«˜ï¼Œæ£€æŸ¥å…·ä½“åŸå› 
    if abs(direct_corr) > 0.5:
        print(f"ğŸš¨ å‘ç°å¼‚å¸¸é«˜ç›¸å…³æ€§ï¼")

        # æ£€æŸ¥æç«¯å€¼
        factor_q99 = factor_common.quantile(0.99)
        factor_q01 = factor_common.quantile(0.01)

        high_factor_stocks = factor_common[factor_common > factor_q99].index
        low_factor_stocks = factor_common[factor_common < factor_q01].index

        high_factor_returns = return_common[high_factor_stocks]
        low_factor_returns = return_common[low_factor_stocks]

        print(f"  é«˜å› å­å€¼è‚¡ç¥¨(å‰1%)å¹³å‡æ”¶ç›Š: {high_factor_returns.mean():.6f}")
        print(f"  ä½å› å­å€¼è‚¡ç¥¨(å1%)å¹³å‡æ”¶ç›Š: {low_factor_returns.mean():.6f}")
        print(f"  æ”¶ç›Šå·®å¼‚: {high_factor_returns.mean() - low_factor_returns.mean():.6f}")


# æ£€æŸ¥å‰ç»æ€§åå·®

# å¯»æ‰¾å¼‚å¸¸æ—¥æœŸ

# è°ƒç”¨è¯¦ç»†è°ƒè¯•
# è°ƒç”¨è°ƒè¯•
# è°ƒç”¨è°ƒè¯•å‡½æ•°
# è°ƒç”¨è¿™ä¸ªå‡½æ•°
# if __name__ == '__main__':
    # factor_df = pd.read_parquet('D:\lqs\codeAbout\py\Quantitative\quant_research_portfolio\projects\_03_factor_selection\debug_snapshot/factor_to_test__prcessed.parquet')
    # price_df = pd.read_parquet('D:\lqs\codeAbout\py\Quantitative\quant_research_portfolio\projects\_03_factor_selection\debug_snapshot/price_for_returns.parquet')
    # # âœ… ä½¿ç”¨æ­£ç¡®çš„O2Cå‡½æ•°
    # returns_calculator = partial(calculate_forward_returns_o2o, close_df=price_df, open_df=price_df)
    #
    # check_lookahead_bias(factor_df, returns_calculator, period=20)
    #
    # # debug_spearman_calculation([-0.012930, -0.012934, -0.013231, -0.014663, -0.013641])
    # # debug_spearman_calculation([-12930, -12934, -13231, -14663, -13641])
    # # debug_spearman_calculation([-0.00012, -0.00013, -0.00014, -0.00015, -0.00010])
    # # debug_spearman_calculation([-0.00012, -0.00013, -0.00014, -0.00012, -0.00010])
    # final_audit('D:\lqs\codeAbout\py\Quantitative\quant_research_portfolio\projects\_03_factor_selection\debug_snapshot/factor_to_test__prcessed.parquet')
    # final_audit('D:\lqs\codeAbout\py\Quantitative\quant_research_portfolio\projects\_03_factor_selection\debug_snapshot/factor_to_test__raw.parquet')
