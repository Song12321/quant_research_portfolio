"""
å› å­é¢„å¤„ç†æµæ°´çº¿ - å•å› å­æµ‹è¯•ç»ˆæä½œæˆ˜æ‰‹å†Œ
ç¬¬ä¸‰é˜¶æ®µï¼šå› å­é¢„å¤„ç†

å®ç°å®Œæ•´çš„å› å­é¢„å¤„ç†æµæ°´çº¿ï¼š
1. å»æå€¼ (Winsorization)
2. ä¸­æ€§åŒ– (Neutralization) 
3. æ ‡å‡†åŒ– (Standardization)
"""
from projects._03_factor_selection.utils.IndustryMap import PointInTimeIndustryMap

try:
    import statsmodels.api as sm
    HAS_STATSMODELS = True
except ImportError:
    from sklearn.linear_model import LinearRegression
    HAS_STATSMODELS = False
import pandas as pd
import numpy as np
from typing import Dict
import warnings
import sys
from pathlib import Path

from projects._03_factor_selection.config_manager.base_config import FACTOR_STYLE_RISK_MODEL
from projects._03_factor_selection.factor_manager.classifier.factor_classifier import FactorClassifier
from projects._03_factor_selection.factor_manager.factor_manager import FactorManager
from quant_lib.config.constant_config import permanent__day

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root))

from quant_lib.config.logger_config import setup_logger, log_warning, log_flow_start
from projects._03_factor_selection.utils.data.residualization_rules import \
    need_residualization_in_neutral_processing as need_residualization_in_neutral_proceessing, \
    get_residualization_config

warnings.filterwarnings('ignore')

# é…ç½®æ—¥å¿—
logger = setup_logger(__name__)


class FactorProcessor:
    """
    å› å­é¢„å¤„ç†å™¨ - ä¸“ä¸šçº§å› å­é¢„å¤„ç†æµæ°´çº¿
    
    æŒ‰ç…§åæ³°è¯åˆ¸æ ‡å‡†å®ç°ï¼š
    1. å»æå€¼ï¼šä¸­ä½æ•°ç»å¯¹åå·®æ³•(MAD) / åˆ†ä½æ•°æ³•
    2. ä¸­æ€§åŒ–ï¼šè¡Œä¸šä¸­æ€§åŒ– + å¸‚å€¼ä¸­æ€§åŒ–
    3. æ ‡å‡†åŒ–ï¼šZ-Scoreæ ‡å‡†åŒ– / æ’åºæ ‡å‡†åŒ–
    """

    def __init__(self, config: Dict):
        """
        åˆå§‹åŒ–å› å­é¢„å¤„ç†å™¨
        
        Args:
            config: é…ç½®å­—å…¸
        """
        self.config = config
        self.preprocessing_config = config.get('preprocessing', {})

    # ok
    def process_factor(self,
                       factor_df_shifted: pd.DataFrame,
                       target_factor_name: str,
                       neutral_dfs,
                       style_category: str,
                       need_standardize: bool = True, #æ ‡å‡†åŒ–
                       pit_map:PointInTimeIndustryMap = None
                       ):
        """
        å®Œæ•´çš„å› å­é¢„å¤„ç†æµæ°´çº¿
        
        Args:
            factor_data: åŸå§‹å› å­æ•°æ®
            auxiliary_df_dict: è¾…åŠ©æ•°æ®ï¼ˆå¸‚å€¼ã€è¡Œä¸šç­‰ï¼‰
            
        Returns:
            é¢„å¤„ç†åçš„å› å­æ•°æ®
        """
        # print("\n" + "=" * 30 + " ã€è¿›å…¥ process_factor è°ƒè¯•æ¨¡å¼ã€‘ " + "=" * 30)
        # print("--- ç›®æ ‡å› å­ (target_factor_df) çš„æœ€å5è¡Œ ---")
        # print(factor_df_shifted.tail())
        #
        # print("\n--- ä¸­æ€§åŒ–é£æ ¼å› å­ (neutral_dfs) çš„æœ€å5è¡Œ ---")
        # for name, df in neutral_dfs.items():
        #     print(f"  > é£æ ¼å› å­: {name}")
        #     print(df.tail())
        #
        # print("=" * 80 + "\n")
        log_flow_start(f"{target_factor_name}å› å­è¿›å…¥å› å­é¢„å¤„ç†...")

        # ã€è°ƒè¯•è¾“å‡ºã€‘æ·»åŠ è¯¦ç»†çš„æ•°æ®ç»Ÿè®¡
        print(f"\n=== ğŸ” è°ƒè¯• {target_factor_name} é¢„å¤„ç†æµç¨‹ ===")
        print(f"è¾“å…¥æ•°æ®å½¢çŠ¶: {factor_df_shifted.shape}")
        print(f"è¾“å…¥éç©ºå€¼æ•°é‡: {factor_df_shifted.notna().sum().sum()}")
        print(f"è¾“å…¥éç©ºå€¼æ¯”ä¾‹: {factor_df_shifted.notna().sum().sum() / (factor_df_shifted.shape[0] * factor_df_shifted.shape[1]):.3f}")

        # æ£€æŸ¥æ¯æ—¥è‚¡ç¥¨æ•°é‡
        daily_counts = factor_df_shifted.notna().sum(axis=1)
        print(f"æ¯æ—¥æœ‰æ•ˆè‚¡ç¥¨æ•°ç»Ÿè®¡: å‡å€¼={daily_counts.mean():.1f}, æœ€å°={daily_counts.min()}, æœ€å¤§={daily_counts.max()}")

        processed_target_factor_df = factor_df_shifted.copy()

        if pit_map is None:
            pit_map = PointInTimeIndustryMap()
        # æ­¥éª¤1ï¼šå»æå€¼
        # print("2. å»æå€¼å¤„ç†...")
        processed_target_factor_df = self.winsorize_robust(processed_target_factor_df,pit_map)
        # æ­¥éª¤2ï¼šä¸­æ€§åŒ–
        if self.preprocessing_config.get('neutralization', {}).get('enable', False):
            processed_target_factor_df = self._neutralize(processed_target_factor_df, target_factor_name,
                                                          neutral_dfs, style_category)
            #ç”¨äºæµ‹è¯•éªŒè¯ä¸­æ€§åŒ–æœ‰æ— ä½œç”¨
            # verify_neutralization_effectiveness(processed_target_factor_df,processed_target_factor_df_E,neutral_dfs=neutral_dfs,test_dates=processed_target_factor_df.index[:100])
        else:
            logger.info("2. è·³è¿‡ä¸­æ€§åŒ–å¤„ç†...")
        if  need_standardize:
            # æ­¥éª¤3ï¼šæ ‡å‡†åŒ–
            processed_target_factor_df = self._standardize_robust(processed_target_factor_df,pit_map)
        else:
            logger.info("2. è·³è¿‡æ ‡å‡†åŒ–å¤„ç†...")

        # ç»Ÿè®¡å¤„ç†ç»“æœ
        FactorManager._validate_data_quality(processed_target_factor_df ,target_factor_name,'é¢„å¤„ç†å®Œä¹‹åï¼š')

        return processed_target_factor_df

    # # ok#ok
    # def winsorize(self, factor_data: pd.DataFrame) -> pd.DataFrame:
    #     """
    #     å»æå€¼å¤„ç†
    #
    #     Args:
    #         factor_data: å› å­æ•°æ®
    #
    #     Returns:
    #         å»æå€¼åçš„å› å­æ•°æ®
    #     """
    #     winsorization_config = self.preprocessing_config.get('winsorization', {})
    #     method = winsorization_config.get('method', 'mad')
    #
    #     processed_factor = factor_data.copy()
    #
    #     if method == 'mad':
    #         # ä¸­ä½æ•°ç»å¯¹åå·®æ³• (Median Absolute Deviation)
    #         threshold = winsorization_config.get('mad_threshold', 5)
    #         # print(f"  ä½¿ç”¨MADæ–¹æ³•ï¼Œé˜ˆå€¼å€æ•°: {threshold}")
    #
    #         # å‘é‡åŒ–è®¡ç®—æ¯æ—¥çš„ä¸­ä½æ•°å’ŒMAD
    #         median = factor_data.median(axis=1)
    #         mad = (factor_data.sub(median, axis=0)).abs().median(axis=1)
    #
    #         # å‘é‡åŒ–è®¡ç®—æ¯æ—¥çš„ä¸Šä¸‹è¾¹ç•Œ
    #         upper_bound = median + threshold * mad
    #         lower_bound = median - threshold * mad
    #
    #         # å‘é‡åŒ–clipï¼Œaxis=0ç¡®ä¿æŒ‰è¡Œå¹¿æ’­è¾¹ç•Œ
    #         return factor_data.clip(lower_bound, upper_bound, axis=0)
    #     elif method == 'quantile':
    #         # åˆ†ä½æ•°æ³•
    #         quantile_range = winsorization_config.get('quantile_range', [0.01, 0.99])
    #         print(f"  ä½¿ç”¨åˆ†ä½æ•°æ–¹æ³•ï¼ŒèŒƒå›´: {quantile_range}")
    #         # å‘é‡åŒ–è®¡ç®—æ¯æ—¥çš„åˆ†ä½æ•°è¾¹ç•Œ
    #         bounds = factor_data.quantile(q=quantile_range, axis=1).T  # .Tè½¬ç½®æ˜¯ä¸ºäº†æ–¹ä¾¿åç»­clip
    #         lower_bound = bounds.iloc[:, 0]
    #         upper_bound = bounds.iloc[:, 1]
    #         return factor_data.clip(lower_bound, upper_bound, axis=0)
    #
    #     return processed_factor

    def _winsorize_mad_series(self, series: pd.Series, threshold: float, min_samples: int = 10) -> pd.Series:
        """
        MADå»æå€¼
        - æ–°å¢ min_samples å‚æ•°ï¼Œç”¨äºå¤„ç†å°æ ·æœ¬ç»„
        - ã€ä¿®å¤ã€‘æ­£ç¡®å¤„ç†NaNå€¼å’Œæœ‰æ•ˆæ ·æœ¬æ•°æ£€æŸ¥
        """
        # 1. ã€ä¿®å¤ã€‘å…ˆå»é™¤NaNï¼Œç„¶åæ£€æŸ¥æœ‰æ•ˆæ ·æœ¬æ•°
        valid_series = series.dropna()
        if len(valid_series) < min_samples:
            return series  # æœ‰æ•ˆæ ·æœ¬å¤ªå°‘ï¼Œä¸å¤„ç†ï¼Œç›´æ¥è¿”å›åŸåºåˆ—

        # 2. è®¡ç®—ä¸­ä½æ•°å’ŒMAD (åŸºäºæœ‰æ•ˆå€¼)
        median = valid_series.median()
        mad = (valid_series - median).abs().median()

        # 3. å¤„ç†é›¶MADé—®é¢˜ï¼ˆæ‰€æœ‰å€¼éƒ½ç›¸åŒï¼‰
        if mad == 0 or pd.isna(mad):
            return series

        # 4. ã€æ–°å¢ã€‘å‚æ•°éªŒè¯
        if threshold <= 0:
            logger.warning(f"MADé˜ˆå€¼æ— æ•ˆ: {threshold}ï¼Œä½¿ç”¨é»˜è®¤å€¼5.0")
            threshold = 5.0

        # 5. è®¡ç®—è¾¹ç•Œå¹¶clip
        const = 1.4826  # æ­£æ€åˆ†å¸ƒä¸‹çš„MADè°ƒæ•´å¸¸æ•°
        upper_bound = median + threshold * const * mad
        lower_bound = median - threshold * const * mad

        # 6. ã€ä¿®å¤ã€‘åªå¯¹æœ‰æ•ˆå€¼è¿›è¡Œclipï¼Œä¿æŒNaNä¸å˜
        result = series.copy()
        mask = ~series.isna()
        result[mask] = series[mask].clip(lower_bound, upper_bound)

        return result

    def _winsorize_quantile_series(self, series: pd.Series, quantile_range: list, min_samples: int = 10) -> pd.Series:
        """
        ã€è¾…åŠ©å‡½æ•°ã€‘å¯¹å•ä¸ªSeriesè¿›è¡Œåˆ†ä½æ•°å»æå€¼ã€‚
        ã€ä¿®å¤ã€‘æ­£ç¡®å¤„ç†NaNå€¼å’Œå‚æ•°éªŒè¯
        """
        # 1. ã€ä¿®å¤ã€‘å…ˆå»é™¤NaNï¼Œç„¶åæ£€æŸ¥æœ‰æ•ˆæ ·æœ¬æ•°
        valid_series = series.dropna()
        if len(valid_series) < min_samples:
            return series

        # 2. ã€æ–°å¢ã€‘å‚æ•°éªŒè¯
        if not isinstance(quantile_range, (list, tuple)) or len(quantile_range) != 2:
            logger.warning(f"åˆ†ä½æ•°èŒƒå›´æ— æ•ˆ: {quantile_range}ï¼Œä½¿ç”¨é»˜è®¤å€¼[0.01, 0.99]")
            quantile_range = [0.01, 0.99]

        lower_q, upper_q = min(quantile_range), max(quantile_range)
        if not (0 <= lower_q < upper_q <= 1):
            logger.warning(f"åˆ†ä½æ•°èŒƒå›´è¶…å‡º[0,1]: {quantile_range}ï¼Œä½¿ç”¨é»˜è®¤å€¼[0.01, 0.99]")
            lower_q, upper_q = 0.01, 0.99

        # 3. è®¡ç®—åˆ†ä½æ•° (åŸºäºæœ‰æ•ˆå€¼)
        lower_bound = valid_series.quantile(lower_q)
        upper_bound = valid_series.quantile(upper_q)

        # 4. ã€ä¿®å¤ã€‘åªå¯¹æœ‰æ•ˆå€¼è¿›è¡Œclipï¼Œä¿æŒNaNä¸å˜
        result = series.copy()
        mask = ~series.isna()
        result[mask] = series[mask].clip(lower_bound, upper_bound)

        return result
        # =========================================================================
        # ã€æ ¸å¿ƒä¿®æ”¹ã€‘æ–°çš„è¾…åŠ©å‡½æ•°ï¼Œå¤„ç†å•ä¸ªæˆªé¢æ—¥çš„å›æº¯é€»è¾‘
        # =========================================================================
    #ok
    def _winsorize_cross_section_fallback(
            self,
            daily_factor_series: pd.Series,
            daily_industry_map: pd.DataFrame,
            config: dict
    ) -> pd.Series:
        """
        å¯¹å•ä¸ªæˆªé¢æ—¥çš„å› å­æ•°æ®æ‰§è¡Œâ€œå‘ä¸Šå›æº¯â€å»æå€¼ã€‚
        è¿™æ˜¯ä¹‹å‰æˆ‘ä»¬ç‹¬ç«‹è®¾è®¡çš„ winsorize_by_industry_fallback å‡½æ•°çš„ç±»æ–¹æ³•ç‰ˆæœ¬ã€‚
        """
        primary_col = config['primary_level']  # e.g., 'l2_code'
        fallback_col = config['fallback_level']  # e.g., 'l1_code'
        min_samples = config['min_samples']

        # 1. æ•°æ®æ•´åˆ
        df = daily_factor_series.to_frame(name='factor')
        merged_df = df.join(daily_industry_map, how='left')
        #   merge ä¹‹å‰ï¼Œå…ˆå°†ç´¢å¼•ts_codeé‡ç½®ä¸ºä¸€åˆ—ï¼Œä»¥é˜²åœ¨merge(merged_df.merge(primary_stats, on=primary_col, how='left'))ä¸­ä¸¢å¤±
        merged_df=merged_df.reset_index(inplace=False)

        # åˆ é™¤æ²¡æœ‰å› å­å€¼æˆ–è¡Œä¸šåˆ†ç±»çš„æ•°æ®
        merged_df=merged_df.dropna(subset=['factor', primary_col, fallback_col], inplace=False)
        if merged_df.empty:
            return pd.Series(index=daily_factor_series.index, dtype=float)

        # 2. è®¡ç®—å„çº§åˆ«è¡Œä¸šçš„ç»Ÿè®¡æ•°æ®
        def mad_func(s: pd.Series) -> float:
            return (s - s.median()).abs().median()

        primary_stats = merged_df.groupby(primary_col)['factor'].agg(['median', 'count', mad_func])
        primary_stats=primary_stats.rename(columns={'median': 'primary_median', 'count': 'primary_count', 'mad_func': 'primary_mad'},
                             inplace=False)

        fallback_stats = merged_df.groupby(fallback_col)['factor'].agg(['median', mad_func])
        fallback_stats=fallback_stats.rename(columns={'median': 'fallback_median', 'mad_func': 'fallback_mad'}, inplace=False)

        # 3. å°†ç»Ÿè®¡æ•°æ®æ˜ å°„å›æ¯åªè‚¡ç¥¨
        merged_df = merged_df.merge(primary_stats, on=primary_col, how='left')
        merged_df = merged_df.merge(fallback_stats, on=fallback_col, how='left')

        # 4. æ ¸å¿ƒå›æº¯é€»è¾‘ ä¸æ»¡è¶³å¿…é¡»æ ·æœ¬æ•°ç›®ï¼Œå°±ç”¨ä¸€çº§è¡Œä¸šçš„mad
        use_fallback = merged_df['primary_count'] < min_samples

        merged_df['final_median'] = np.where(use_fallback, merged_df['fallback_median'], merged_df['primary_median'])
        merged_df['final_mad'] = np.where(use_fallback, merged_df['fallback_mad'], merged_df['primary_mad'])

        merged_df['final_mad'].replace(0, 1e-9, inplace=True)  # #ç§’å•Šï¼Œå¦‚æœæ˜¯0çš„è¯ ä¸‹é¢upper loweræ˜¯ä¸€ä¸ªå€¼ï¼ å¯¼è‡´æœ€åæ‰€å› å­éƒ½æ˜¯ä¸€ä¸ªå€¼ï¼å¤§å¿Œï¼
        merged_df.set_index('ts_code', inplace=True)

        # 5. æ‰§è¡Œå»æå€¼
        method = config.get('method', 'mad')
        if method == 'mad':
            threshold = config.get('mad_threshold', 3)
            const = 1.4826
            upper = merged_df['final_median'] + threshold * const * merged_df['final_mad']
            lower = merged_df['final_median'] - threshold * const * merged_df['final_mad']
        elif method == 'quantile':
            # åˆ†ä½æ•°æ³•ä¹Ÿå¯ä»¥åº”ç”¨å›æº¯é€»è¾‘ï¼Œä½†è¾ƒä¸ºç½•è§ã€‚è¿™é‡Œæˆ‘ä»¬ä»¥MADä¸ºä¸»ï¼Œåˆ†ä½æ•°ä¿æŒç»„å†…å¤„ç†ã€‚
            # å¦‚éœ€åˆ†ä½æ•°å›æº¯ï¼Œé€»è¾‘ä¼šæ›´å¤æ‚ï¼Œæ­¤å¤„ä¸ºç®€åŒ–ã€‚
            return merged_df['factor']  # æš‚ä¸å¤„ç†quantileçš„å›æº¯
        else:
            return merged_df['factor']

        winsorized_factor = merged_df['factor'].clip(lower=lower, upper=upper)

        # è¿”å›ä¸€ä¸ªä¸è¾“å…¥Serieså¯¹é½çš„Series
        return winsorized_factor.reindex(daily_factor_series.index)

        # =========================================================================
        # ã€æ ¸å¿ƒä¿®æ”¹ã€‘é‡æ„åçš„ winsorize_robust å‡½æ•°
        # =========================================================================

    def winsorize_robust(self, factor_data: pd.DataFrame,pit_industry_map: PointInTimeIndustryMap = None) -> pd.DataFrame:
        """
        å»æå€¼å¤„ç†å‡½æ•°ã€‚
        æ”¯æŒå…¨å¸‚åœºæˆ–åˆ†è¡Œä¸šï¼ˆå¸¦å‘ä¸Šå›æº¯åŠŸèƒ½ï¼‰çš„MADå’Œåˆ†ä½æ•°æ³•ã€‚

        Args:
            factor_data (pd.DataFrame): å› å­æ•°æ® (index=date, columns=stock)ã€‚
            industry_map (pd.DataFrame, optional): è¡Œä¸šåˆ†ç±»æ•°æ® (index=stock, columns=['l1_code', 'l2_code',...])
                                                   å¦‚æœæä¾›æ­¤å‚æ•°ï¼Œåˆ™æ‰§è¡Œåˆ†è¡Œä¸šå»æå€¼ã€‚
        Returns:
            pd.DataFrame: å»æå€¼åçš„å› å­æ•°æ®ã€‚
        """
        winsorization_config = self.preprocessing_config.get('winsorization', {})
        industry_config = winsorization_config.get('by_industry')

        # --- è·¯å¾„ä¸€ï¼šå…¨å¸‚åœºå»æå€¼ (é€»è¾‘åŸºæœ¬ä¸å˜) ---
        if pit_industry_map is None or industry_config is None:
            logger.info("  æ‰§è¡Œå…¨å¸‚åœºå»æå€¼...")
            method = winsorization_config.get('method', 'mad')
            if method == 'mad':
                params = {'threshold': winsorization_config.get('mad_threshold', 5),
                          'min_samples': 1}  # å…¨å¸‚åœºä¸éœ€min_samples
                return factor_data.apply(self._winsorize_mad_series, axis=1, **params)
            elif method == 'quantile':
                params = {'quantile_range': winsorization_config.get('quantile_range', [0.01, 0.99]), 'min_samples': 1}
                return factor_data.apply(self._winsorize_quantile_series, axis=1, **params)
            return factor_data

        # --- è·¯å¾„äºŒï¼šåˆ†è¡Œä¸šå»æå€¼ (é‡‡ç”¨å›æº¯é€»è¾‘) ---
        else:
            logger.info(
                f"  æ‰§è¡Œåˆ†è¡Œä¸šå»æå€¼ (ä¸»è¡Œä¸š: {industry_config['primary_level']}, å½“æ ·æœ¬ä¸è¶³ä¼šè‡ªåŠ¨å›æº¯è‡³: {industry_config['fallback_level']})...")
            #  ä¸ºäº†é«˜æ•ˆè·å–å‰ä¸€äº¤æ˜“æ—¥ï¼Œæå‰åˆ›å»ºäº¤æ˜“æ—¥åºåˆ—
            trading_dates_series = pd.Series(factor_data.index, index=factor_data.index)

            # æŒ‰å¤©å¾ªç¯ï¼Œåœ¨æˆªé¢æ—¥ä¸Šæ‰§è¡ŒçŸ¢é‡åŒ–æ“ä½œ
            processed_data = {}
            for date in factor_data.index:
                # è·å–å½“å¤©çš„å› å­å’Œè¡Œä¸šæ•°æ®
                daily_factor_series = factor_data.loc[date].dropna()

                # å¦‚æœå½“å¤©æ²¡æœ‰æœ‰æ•ˆå› å­å€¼ï¼Œåˆ™è·³è¿‡
                if daily_factor_series.empty:
                    processed_data[date] = pd.Series(dtype=float)
                    log_warning(f"å»æå€¼è¿‡ç¨‹ä¸­ï¼Œå‘ç°å½“å¤©{date}æ‰€æœ‰è‚¡ç¥¨å› å­å€¼éƒ½ä¸ºç©º")
                    continue
                    # åœ¨å¾ªç¯å†…éƒ¨ï¼Œä¸ºæ¯ä¸€å¤©è·å–æ­£ç¡®çš„å†å²åœ°å›¾
                    #  è·å– T-1 çš„æ—¥æœŸ
                prev_trading_date = trading_dates_series.shift(1).loc[date]
                # å¤„ç†å›æµ‹ç¬¬ä¸€å¤©çš„è¾¹ç•Œæƒ…å†µ
                if pd.isna(prev_trading_date):
                    # log_warning(f"æ­£å¸¸ç°è±¡ï¼šæ—¥æœŸ {date} æ˜¯å›æµ‹é¦–æ—¥ï¼Œæ²¡æœ‰å‰ä¸€å¤©çš„è¡Œä¸šæ•°æ®ï¼Œè·³è¿‡åˆ†è¡Œä¸šå¤„ç†ã€‚")
                    processed_data[date] = daily_factor_series  # å½“å¤©ä¸åšå¤„ç†æˆ–æ‰§è¡Œå…¨å¸‚åœºå¤„ç†
                    continue

                # ä½¿ç”¨ T-1 çš„æ—¥æœŸæŸ¥è¯¢è¡Œä¸šåœ°å›¾
                daily_industry_map = pit_industry_map.get_map_for_date(prev_trading_date)

                processed_data[date] = self._winsorize_cross_section_fallback(
                    daily_factor_series=daily_factor_series,
                    daily_industry_map=daily_industry_map,
                    config=industry_config
                )

            # å°†å¤„ç†åçš„æ•°æ®åˆå¹¶å›DataFrame
            result_df = pd.DataFrame.from_dict(processed_data, orient='index')
            # ä¿æŒåŸå§‹çš„ç´¢å¼•å’Œåˆ—é¡ºåº
            return result_df.reindex(index=factor_data.index, columns=factor_data.columns)

    # ok
    #è€ƒè™‘ ä¼ å…¥çš„è¡Œä¸šå¦‚æœæ˜¯äºŒçº§è¡Œä¸šé‚£ä¹ˆè¡Œä¸šå˜é‡å¤šè¾¾130ä¸ªï¼ï¼Œæˆ‘åˆä¸åšå…¨Aï¼Œä¸­è¯800æ‰800ï¼Œå¹³å‡ä¸€ä¸ªè¡Œä¸šæ‰5åªè‚¡ç¥¨ æ¥è¿›è¡Œä¸­æ€§åŒ–ï¼Œæœ‰ç‚¹ä¸å…·å‚ç…§ï¼ï¼Œå¿…é¡»ç”¨ä¸€çº§è¡Œä¸š
    ##
    # å•æµ‹é€šè¿‡ï¼š
    # === ğŸ”¬ ä¸­æ€§åŒ–æ•ˆæœéªŒè¯æŠ¥å‘Š ===
    # 1ï¸âƒ£ åŸºç¡€ç»Ÿè®¡æ£€æŸ¥
    #    åŸå§‹å› å­: å‡å€¼=0.005922, æ ‡å‡†å·®=0.112081
    #    ä¸­æ€§åŒ–å: å‡å€¼=0.000000, æ ‡å‡†å·®=0.082274
    #    æ•°æ®è¦†ç›–: åŸå§‹76.26% â†’ ä¸­æ€§åŒ–å76.26%
    # 2ï¸âƒ£ ä¸é£æ ¼å› å­ç›¸å…³æ€§æ£€æŸ¥
    #    vs circ_mv     :  0.1737 â†’  0.1499 (é™ä½äº† 0.0239)
    #    vs pct_chg_beta:  0.0266 â†’  0.0368 (é™ä½äº† 0.0102)
    # 3ï¸âƒ£ è¡Œä¸šä¸­æ€§åŒ–æ£€æŸ¥
    #    2019-07-11: è¡Œä¸šæ•ˆåº”é™ä½ 0.0305
    #    2019-07-12: è¡Œä¸šæ•ˆåº”é™ä½ 0.0308
    # 4ï¸âƒ£ æˆªé¢ç›¸å…³æ€§ä¿æŒæ£€æŸ¥
    #    æˆªé¢ç›¸å…³æ€§ä¿æŒåº¦: 0.8222 (>0.5ä¸ºè‰¯å¥½)
    # 5ï¸âƒ£ å…·ä½“æ—¥æœŸéªŒè¯
    #    ğŸ“… 2019-07-10 è¯¦ç»†æ£€æŸ¥:
    #       æœ‰æ•ˆè‚¡ç¥¨æ•°: 799 â†’ 799
    #       å› å­å‡å€¼: 0.004928 â†’ 0.000000
    #       å› å­æ ‡å‡†å·®: 0.069131 â†’ 0.068919
    #       æç«¯å€¼æ•°é‡: 40 â†’ 40
    #    ğŸ“… 2019-07-11 è¯¦ç»†æ£€æŸ¥:
    #       æœ‰æ•ˆè‚¡ç¥¨æ•°: 788 â†’ 788
    #       å› å­å‡å€¼: 0.005155 â†’ 0.000000
    #       å› å­æ ‡å‡†å·®: 0.068800 â†’ 0.061785
    #       æç«¯å€¼æ•°é‡: 40 â†’ 40
    #    ğŸ“… 2019-07-12 è¯¦ç»†æ£€æŸ¥:
    #       æœ‰æ•ˆè‚¡ç¥¨æ•°: 787 â†’ 787
    #       å› å­å‡å€¼: 0.002713 â†’ 0.000000
    #       å› å­æ ‡å‡†å·®: 0.072949 â†’ 0.065334
    #       æç«¯å€¼æ•°é‡: 40 â†’ 40#
    ##
    #  1. ä¸­æ€§åŒ–å®Œå…¨ç”Ÿæ•ˆ âœ…
    #     - å› å­å‡å€¼ç²¾ç¡®å½’é›¶
    #     - é£æ ¼æš´éœ²æœ‰æ•ˆé™ä½
    #     - æˆªé¢ä¿¡æ¯è‰¯å¥½ä¿æŒ
    #   2. å”¯ä¸€çš„æ”¹è¿›ç©ºé—´ï¼š
    #     - å¸‚å€¼ä¸­æ€§åŒ–è¿˜å¯ä»¥æ›´å½»åº•ï¼ˆ0.1499è¿˜èƒ½å†é™ä½ï¼‰
    #     - å¯èƒ½æ˜¯å› ä¸ºæ ·æœ¬ä¸è¶³å¯¼è‡´çš„ï¼Œå¯ä»¥è€ƒè™‘ï¼š
    #         - é™ä½æœ€å°æ ·æœ¬æ•°è¦æ±‚ï¼ˆä»30â†’20ï¼‰
    #       - æˆ–ä½¿ç”¨ä¸€çº§è¡Œä¸šè€ŒéäºŒçº§è¡Œä¸š
    #   3. å¯¹å› å­é€‰æ‹©çš„æ„ä¹‰ï¼š
    #     - è¿™ä¸ªä¸­æ€§åŒ–åçš„å› å­æ˜¯çº¯å‡€çš„alphaä¿¡å·
    #     - å¯ä»¥å®‰å…¨ç”¨äºICæµ‹è¯•å’Œåˆ†å±‚å›æµ‹
    #     - é£é™©è°ƒæ•´åçš„è¡¨ç°ä¼šæ›´ç¨³å®š#


    ##
    # å¤§ç™½è¯ï¼Œä¸­æ€§åŒ–å°±æ˜¯å¸®æˆ‘ä»¬ï¼šå‰”é™¤å¤šä½™å˜é‡ï¼Œå‰©ä¸‹çš„å€¼ ï¼ˆæš´éœ²çš„å€¼ï¼šéƒ½æ˜¯å„å‡­æœ¬äº‹å‡ºæ¥çš„ï¼‰#
    def _neutralize(self,
                    factor_data: pd.DataFrame,
                    target_factor_name: str,
                    # auxiliary_dfs: Dict[str, pd.DataFrame], # åœ¨æ–°æ¶æ„ä¸‹ï¼Œbetaä¹Ÿåº”åœ¨neutral_dfsä¸­
                    neutral_dfs: Dict[str, pd.DataFrame],
                    style_category: str
                    ) -> pd.DataFrame:
        """
        ã€V2.0-é‡æ„ç‰ˆã€‘æ ¹æ®å› å­æ‰€å±çš„â€œé—¨æ´¾â€ï¼Œè‡ªåŠ¨é€‰æ‹©æœ€åˆé€‚çš„ä¸­æ€§åŒ–æ–¹æ¡ˆã€‚
        æ­¤ç‰ˆæœ¬ä¿®å¤äº†æ½œåœ¨bugï¼Œå¹¶ä¼˜åŒ–äº†æ•°æ®æ„å»ºæµç¨‹ï¼Œæå‡äº†è¿è¡Œæ•ˆç‡å’Œä»£ç æ¸…æ™°åº¦ã€‚
        """
        neutralization_config = self.preprocessing_config.get('neutralization', {})
        if not neutralization_config.get('enable', False):
            return factor_data

        logger.info(f"  > æ­£åœ¨å¯¹ '{target_factor_name}' å› å­ '{target_factor_name}' è¿›è¡Œä¸­æ€§åŒ–å¤„ç†...")
        processed_factor = factor_data.copy()

        # --- é˜¶æ®µä¸€ï¼šå› å­æ®‹å·®åŒ– ---
        if need_residualization_in_neutral_proceessing(target_factor_name, style_category):
            # è·å–è¯¥å› å­çš„å®šåˆ¶åŒ–æ®‹å·®é…ç½®
            resid_config = get_residualization_config(target_factor_name)
            window = resid_config.get('window', 20)
            min_periods = resid_config.get('min_periods', max(1, int(window * 0.5)))
            factor_mean = processed_factor.rolling(window=window, min_periods=min_periods).mean()
            processed_factor = processed_factor - factor_mean
            logger.info(f"{target_factor_name} è¿›è¡Œå› å­æ®‹å·®åŒ– for ä¸­æ€§åŒ–")


        # --- é˜¶æ®µäºŒï¼šç¡®å®šä¸­æ€§åŒ–å› å­åˆ—è¡¨ ---
        factors_to_neutralize = self.get_regression_need_neutral_factor_list(style_category, target_factor_name)
        if not factors_to_neutralize:
            logger.info(f"    > '{target_factor_name}' å› å­æ— éœ€ä¸­æ€§åŒ–ã€‚")
            return processed_factor

        # logger.info(f"    > {target_factor_name} å°†å¯¹ä»¥ä¸‹é£æ ¼è¿›è¡Œä¸­æ€§åŒ–: {factors_to_neutralize}")

        skipped_days_count = 0
        total_days = len(processed_factor.index)

        # --- é˜¶æ®µä¸‰ï¼šé€æ—¥æˆªé¢å›å½’ä¸­æ€§åŒ– ---
        for date in processed_factor.index:
            y_series = processed_factor.loc[date].dropna()
            if y_series.empty:
                skipped_days_count += 1
                log_warning(f"{target_factor_name}ä¸­æ€§åŒ–è·³è¿‡è¿™ä¸€å¤©{date} y_series.empty ")
                continue

            # --- a) ã€æ•ˆç‡ä¼˜åŒ–ã€‘æ„å»ºå›å½’è‡ªå˜é‡çŸ©é˜µ X ---
            X_df_parts = []  # ä½¿ç”¨ä¸€ä¸ªåˆ—è¡¨æ¥æ”¶é›†æ‰€æœ‰è‡ªå˜é‡ Series

            # --- å¸‚å€¼å› å­ ---
            if 'market_cap' in factors_to_neutralize:
                # ã€å‘½åç»Ÿä¸€ã€‘ä» neutral_dfs ä¸­å¯»æ‰¾è§„æ¨¡å› å­ï¼Œåå­—å¯ä»¥æ˜¯ 'log_circ_mv', 'log_circ_mv' ç­‰
                market_cap_key = 'log_circ_mv'
                if market_cap_key not in neutral_dfs:
                    raise ValueError(f"neutral_dfs ä¸­ç¼ºå°‘å¸‚å€¼å› å­ '{market_cap_key}'ã€‚")
                mv_series = neutral_dfs[market_cap_key].loc[date].rename('log_circ_mv')
                X_df_parts.append(mv_series)

            # --- è¡Œä¸šå› å­ ---
            if 'industry' in factors_to_neutralize:
                industry_dummy_keys = [k for k in neutral_dfs.keys() if k.startswith('industry_')]
                if not industry_dummy_keys:
                    raise ValueError("neutral_dfs ä¸­æœªå‘ç°è¡Œä¸šå“‘å˜é‡ã€‚")

                # ã€æ•ˆç‡ä¼˜åŒ–ã€‘ä¸€æ¬¡æ€§ä» neutral_dfs ä¸­æå–å½“å¤©çš„æ‰€æœ‰è¡Œä¸šå“‘å˜é‡
                daily_dummies_df = pd.concat(
                    [neutral_dfs[key].loc[date].rename(key) for key in industry_dummy_keys],
                    axis=1
                )
                X_df_parts.append(daily_dummies_df)

            # --- Beta å› å­ ---
            if 'pct_chg_beta' in factors_to_neutralize:
                if 'pct_chg_beta' not in neutral_dfs:  # å»ºè®®å°†betaä¹Ÿç»Ÿä¸€æ”¾å…¥neutral_dfs
                    raise ValueError("neutral_dfs ä¸­ç¼ºå°‘ 'pct_chg_beta' æ•°æ®ã€‚")

                beta_series = neutral_dfs['pct_chg_beta'].loc[date].rename('pct_chg_beta')
                X_df_parts.append(beta_series)

            if not X_df_parts:
                log_warning(f"{target_factor_name}ä¸­æ€§åŒ–è·³è¿‡è¿™ä¸€å¤©{date} :not X_df_parts")
                continue

            # --- b) ã€æµç¨‹ä¼˜åŒ–ã€‘å°†æ‰€æœ‰éƒ¨åˆ†ä¸€æ¬¡æ€§åˆå¹¶ï¼Œç„¶åä¸ y å¯¹é½ ---
            X_df = pd.concat(X_df_parts, axis=1)
            # ä½¿ç”¨ join='inner' å¯ä»¥ä¸€æ­¥åˆ°ä½åœ°å®Œæˆå¯¹é½å’Œç­›é€‰
            combined_df = pd.concat([y_series.rename('factor'), X_df], axis=1, join='inner').dropna()

            # --- c) æ ·æœ¬é‡æ£€æŸ¥ (é€»è¾‘ä¸å˜ï¼Œä½†æ›´å¥å£®) ---
            num_predictors = X_df.shape[1]
            MIN_SAMPLES_ABSOLUTE = 30   # ç»å¯¹æœ€å°æ ·æœ¬æ•°ï¼ˆä»150é™åˆ°30ï¼‰
            MIN_SAMPLES_RELATIVE_FACTOR = 1.5  # ç›¸å¯¹æœ€å°æ ·æœ¬å€æ•°ï¼ˆä»3é™åˆ°1.5ï¼‰

            # åŒæ—¶æ»¡è¶³ç›¸å¯¹å’Œç»å¯¹ä¸¤ä¸ªæ¡ä»¶
            is_sample_insufficient = (len(combined_df) < MIN_SAMPLES_ABSOLUTE) or \
                                     (len(combined_df) < MIN_SAMPLES_RELATIVE_FACTOR * num_predictors)

            if is_sample_insufficient:
                log_warning(
                    f"  è­¦å‘Š: æ—¥æœŸ {date.date()} æ¸…ç†åæ ·æœ¬æ•°ä¸º: "
                    f"({len(combined_df)}), æœªæ»¡è¶³ æ ·æœ¬æ•° >ç»å¯¹æœ€å°æ ·æœ¬æ•°:( {MIN_SAMPLES_ABSOLUTE}) æˆ–æœªæ»¡è¶³-- æ ·æœ¬æ•° > ç›¸å¯¹æœ€å°æ ·æœ¬å€æ•°:({MIN_SAMPLES_RELATIVE_FACTOR})*è‡ªå˜é‡ä¸ªæ•°:({num_predictors}) çš„æ¡ä»¶ï¼Œ"
                    f"è·³è¿‡ä¸­æ€§åŒ–ã€‚"
                )
                # ã€é‡è¦å†³ç­–ã€‘å½“æ ·æœ¬ä¸è¶³æ—¶ï¼Œæ˜¯ä¿ç•™åŸå§‹å› å­å€¼ï¼Œè¿˜æ˜¯è®¾ä¸ºç©ºå€¼ï¼Ÿ
                # è®¾ä¸ºç©ºå€¼(NaN)æ˜¯æ›´ä¿å®ˆã€æ›´è¯šå®çš„é€‰æ‹©ã€‚å®ƒæ‰¿è®¤äº†å½“å¤©æ— æ³•ç”Ÿæˆä¸€ä¸ªå¯é çš„ä¸­æ€§åŒ–ä¿¡å·ã€‚
                # å¦‚æœä¿ç•™åŸå§‹å› å­å€¼ï¼ˆå³ï¼šç›´æ¥continueï¼‰ï¼Œæ„å‘³ç€åœ¨è¿™ä¸€å¤©ä½ äº¤æ˜“çš„æ˜¯ä¸€ä¸ªæœªè¢«ä¸­æ€§åŒ–çš„ã€æœ‰é£é™©æš´éœ²çš„å› å­ã€‚

                ##
                # å¦‚æœç›´æ¥continue ()ï¼šè¿™æ„å‘³ç€åœ¨æ ·æœ¬ä¸è¶³çš„æ—¥æœŸï¼Œprocessed_factor ä¸­ä¿ç•™çš„æ˜¯åŸå§‹å› å­å€¼ã€‚è¿™ä¼šä½¿ä½ çš„â€œçº¯å‡€å› å­â€åœ¨æŸäº›å¤©çªç„¶å˜å›â€œåŸå§‹å› å­â€ï¼Œå¯¼è‡´é£é™©æš´éœ²ä¸ä¸€è‡´ã€‚#
                processed_factor.loc[date] = np.nan  #
                skipped_days_count += 1
                log_warning(f"{target_factor_name}ä¸­æ€§åŒ–è·³è¿‡è¿™ä¸€å¤©{date} å½“å¤©æ ·æœ¬ä¸å¤Ÿï¼ˆç»è¿‡æ®‹å·®åŒ–çš„å‰å‡ å¤©éƒ½æ˜¯nanå¾ˆæ­£å¸¸ï¼ä½†æ˜¯ä¸ä¼šè¶…è¿‡10å¤©")
                continue
            # --- d) æ‰§è¡Œå›å½’å¹¶è®¡ç®—æ®‹å·® ---
            y_clean = combined_df['factor']
            # ä½¿ç”¨ sm.add_constant æ·»åŠ æˆªè·é¡¹ï¼Œæ˜¯ statsmodels çš„æ ‡å‡†åšæ³•
            X_clean = sm.add_constant(combined_df.drop(columns=['factor']))
            # print(f"{date}:ç›¸å…³æ€§ï¼š{pd.concat([y_clean, X_clean], axis=1).corr()}") æ­£å¸¸
            try:
                model = sm.OLS(y_clean, X_clean).fit()
                residuals = model.resid
                # self.neutral_gression_diagnostics(model,date,y_clean,X_clean,residuals)

                # å°†ä¸­æ€§åŒ–åçš„æ®‹å·®æ›´æ–°å› processed_factor
                processed_factor.loc[date, residuals.index] = residuals

            except Exception as e:
                logger.error(f"  é”™è¯¯: æ—¥æœŸ {date.date()} ä¸­æ€§åŒ–å›å½’å¤±è´¥: {e}ã€‚è¯¥æ—¥å› å­æ•°æ®å°†æ ‡è®°ä¸ºNaNã€‚")
                processed_factor.loc[date] = np.nan
                skipped_days_count += 1
        # å¾ªç¯ç»“æŸåï¼Œæ‰§è¡Œâ€œç†”æ–­æ£€æŸ¥â€ ===
        # ä»é…ç½®ä¸­è·å–æœ€å¤§è·³è¿‡æ¯”ä¾‹ï¼Œå¦‚æœæœªé…ç½®ï¼Œåˆ™é»˜è®¤ä¸º10%
        max_skip_ratio = neutralization_config.get('max_skip_ratio', 0.20)  # ä»0.10æ”¾å®½åˆ°0.20

        actual_skip_ratio = skipped_days_count / total_days

        if actual_skip_ratio > max_skip_ratio:
            # å½“å®é™…è·³è¿‡æ¯”ä¾‹è¶…è¿‡é˜ˆå€¼æ—¶ï¼Œç›´æ¥æŠ›å‡ºå¼‚å¸¸ï¼Œä¸­æ–­ç¨‹åº
            raise ValueError(
                f"å› å­ '{target_factor_name}' ä¸­æ€§åŒ–å¤±è´¥ï¼šå¤„ç†çš„ {total_days} å¤©ä¸­ï¼Œ"
                f"æœ‰ {skipped_days_count} å¤© ({actual_skip_ratio:.2%}) å› æ ·æœ¬ä¸è¶³è¢«è·³è¿‡ï¼Œ"
                f"è¶…è¿‡äº† {max_skip_ratio:.0%} çš„å®¹å¿ä¸Šé™ã€‚"
                f"è¯·æ£€æŸ¥ä¸Šæ¸¸å› å­æ•°æ®è´¨é‡æˆ–è‚¡ç¥¨æ± è®¾ç½®ã€‚"
            )

        logger.info(f"  > ä¸­æ€§åŒ–å®Œæˆã€‚åœ¨ {total_days} å¤©ä¸­ï¼Œå…±è·³è¿‡äº† {skipped_days_count} å¤©ã€‚")

        # ğŸš¨ æ·±åº¦è¯Šæ–­ï¼šæ£€æŸ¥ä¸­æ€§åŒ–å‰åçš„å› å­åˆ†å¸ƒå˜åŒ–
        # self.show_neutral_result(factor_data,processed_factor)
        return processed_factor
    # # ok
    # def _standardize(self, factor_data: pd.DataFrame) -> pd.DataFrame:
    #     """
    #     æ ‡å‡†åŒ–å¤„ç†
    #
    #     Args:
    #         factor_data: å› å­æ•°æ®
    #
    #     Returns:
    #         æ ‡å‡†åŒ–åçš„å› å­æ•°æ®
    #     """
    #     standardization_config = self.preprocessing_config.get('standardization', {})
    #     method = standardization_config.get('method', 'zscore')
    #
    #     processed_factor = factor_data.copy()
    #
    #     if method == 'zscore':
    #         # print("  ä½¿ç”¨Z-Scoreæ ‡å‡†åŒ– (å¥å£®ç‰ˆ)")
    #         mean = processed_factor.mean(axis=1)
    #         std = processed_factor.std(axis=1)
    #
    #         # è¯†åˆ«std=0çš„å®‰å…¨éšæ‚£
    #         std_is_zero_mask = (std == 0)
    #
    #         # å…ˆè¿›è¡Œæ ‡å‡†åŒ–ï¼ˆä¼šäº§ç”Ÿinfï¼‰
    #         processed_factor = processed_factor.sub(mean, axis=0).div(std, axis=0)
    #
    #         # å°†std=0çš„è¡Œï¼Œç»“æœå®‰å…¨åœ°è®¾ä¸º0
    #         processed_factor[std_is_zero_mask] = 0.0
    #
    #         return processed_factor
    #
    #     elif method == 'rank':
    #         print("  ä½¿ç”¨æ’åºæ ‡å‡†åŒ– (å¥å£®ç‰ˆ)")
    #
    #         # è¯†åˆ«åªæœ‰ä¸€ä¸ªæœ‰æ•ˆå€¼çš„è¾¹ç•Œæƒ…å†µ
    #         valid_counts = processed_factor.notna().sum(axis=1)
    #         single_value_mask = (valid_counts == 1)
    #
    #         # æ­£å¸¸è®¡ç®—æ’å
    #         ranks = processed_factor.rank(axis=1, pct=True)
    #         processed_factor = 2 * ranks - 1
    #
    #         # å°†åªæœ‰ä¸€ä¸ªæœ‰æ•ˆå€¼çš„è¡Œï¼Œç»“æœå®‰å…¨åœ°è®¾ä¸º0
    #         processed_factor[single_value_mask] = 0.0
    #         return processed_factor
    #
    #     raise RuntimeError("è¯·æŒ‡å®šæ ‡å‡†åŒ–æ–¹å¼")

    # ä½ çš„è¾…åŠ©å‡½æ•°ç¨ä½œè°ƒæ•´ï¼Œä¸“æ³¨äºè®¡ç®—æœ¬èº«
    def _zscore_series(self, s: pd.Series) -> pd.Series:
        """ã€è¾…åŠ©å‡½æ•°ã€‘å¯¹å•ä¸ªSeriesè¿›è¡ŒZ-Scoreæ ‡å‡†åŒ–"""
        if s.count() < 2: return pd.Series(0, index=s.index)
        std_val = s.std()
        if std_val == 0: return pd.Series(0, index=s.index)
        mean_val = s.mean()
        return (s - mean_val) / std_val

    def _rank_series(self, s: pd.Series) -> pd.Series:
        """ã€è¾…åŠ©å‡½æ•°ã€‘å¯¹å•ä¸ªSeriesè¿›è¡Œæ’åºæ ‡å‡†åŒ– (è½¬æ¢ä¸º[-1, 1]åŒºé—´)"""
        return s.rank(pct=True, na_option='keep') * 2 - 1

        # =========================================================================
        # ã€æ–°å¢æ ¸å¿ƒã€‘å¤„ç†æˆªé¢æ ‡å‡†åŒ–å›æº¯çš„è¾…åŠ©å‡½æ•°
        # =========================================================================

    def _standardize_cross_section_fallback(
            self,
            daily_factor_series: pd.Series,
            daily_industry_map: pd.DataFrame,
            config: dict
    ) -> pd.Series:
        """å¯¹å•ä¸ªæˆªé¢æ—¥çš„å› å­æ•°æ®æ‰§è¡Œâ€œå‘ä¸Šå›æº¯â€Z-Scoreæ ‡å‡†åŒ–ã€‚"""
        primary_col = config['primary_level']
        fallback_col = config['fallback_level']
        min_samples = config.get('min_samples', 3)  # æ ‡å‡†åŒ–è‡³å°‘éœ€è¦2ä¸ªç‚¹ï¼Œè®¾ä¸º3æ›´ç¨³å¥

        # 1. æ•°æ®æ•´åˆ
        df = daily_factor_series.to_frame(name='factor')
        merged_df = df.join(daily_industry_map, how='left')

        # èµ¶ç´§ å…ˆå°†ç´¢å¼•ts_codeé‡ç½®ä¸ºä¸€åˆ—ï¼Œä»¥é˜²åœ¨merge(merged_df.merge(primary_stats, on=primary_col, how='left'))ä¸­ä¸¢å¤±
        merged_df=merged_df.reset_index(inplace=False)

        merged_df=merged_df.dropna(subset=['factor', primary_col, fallback_col], inplace=False)
        if merged_df.empty:
            return pd.Series(index=daily_factor_series.index, dtype=float)

        # 2. è®¡ç®—å„çº§åˆ«è¡Œä¸šçš„ç»Ÿè®¡æ•°æ® (mean, std, count)
        primary_stats = merged_df.groupby(primary_col)['factor'].agg(['mean', 'std', 'count'])
        primary_stats=primary_stats.rename(columns={'mean': 'primary_mean', 'std': 'primary_std', 'count': 'primary_count'},
                             inplace=False)

        fallback_stats = merged_df.groupby(fallback_col)['factor'].agg(['mean', 'std'])
        fallback_stats=fallback_stats.rename(columns={'mean': 'fallback_mean', 'std': 'fallback_std'}, inplace=False)

        # 3. å°†ç»Ÿè®¡æ•°æ®æ˜ å°„å›æ¯åªè‚¡ç¥¨
        merged_df = merged_df.merge(primary_stats, on=primary_col, how='left')
        merged_df = merged_df.merge(fallback_stats, on=fallback_col, how='left')

        # 4. æ ¸å¿ƒå›æº¯é€»è¾‘
        use_fallback = merged_df['primary_count'] < min_samples
        merged_df['final_mean'] = np.where(use_fallback, merged_df['fallback_mean'], merged_df['primary_mean'])
        merged_df['final_std'] = np.where(use_fallback, merged_df['fallback_std'], merged_df['primary_std'])

        # ä¿®å¤æ ‡å‡†åŒ–å¤„ç†ï¼šæ›´åˆç†åœ°å¤„ç†æ ‡å‡†å·®ä¸º0çš„æƒ…å†µ
        merged_df['final_std'] = merged_df['final_std'].fillna(0)
        merged_df=merged_df.set_index('ts_code', inplace=False)

        # å¯¹äºæ ‡å‡†å·®ä¸º0çš„æƒ…å†µï¼Œç›´æ¥è®¾ä¸º0ï¼Œä¸è¿›è¡Œæ ‡å‡†åŒ–ï¼ˆåœ¨è®¾ç½®ç´¢å¼•ååˆ›å»ºmaskï¼‰
        zero_std_mask = merged_df['final_std'] < 1e-6

        # 5. åŸæ¥æ–¹æ¡ˆ Z-Scoreæ ‡å‡†åŒ– ï¼šå¼Šç«¯ ä¸‡ä¸€å°±æ˜¯æœ‰ å¤©ç”Ÿå°±æ˜¯std =1çš„å‘¢
        # standardized_factor = (merged_df['factor'] - merged_df['final_mean']) / merged_df['final_std']

        # å¯¹äºæ ‡å‡†å·®ä¸º0å¯¼è‡´stdè¢«è®¾ä¸º1çš„ç»„ï¼Œå…¶(factor-mean)å¯èƒ½ä¸ä¸º0ï¼Œéœ€è¦æ‰‹åŠ¨è®¾ä¸º0
        # standardized_factor.loc[merged_df['final_std'] == 1.0] = 0
        # 5. æ‰§è¡ŒZ-Scoreæ ‡å‡†åŒ–
        standardized_factor = pd.Series(index=merged_df.index, dtype=float)

        # å¯¹äºæœ‰æ•ˆæ ‡å‡†å·®çš„è‚¡ç¥¨è¿›è¡Œæ ‡å‡†åŒ–
        valid_std_mask = ~zero_std_mask
        if valid_std_mask.any():
            # ç¡®ä¿ç´¢å¼•æ“ä½œè¿”å›Seriesè€Œä¸æ˜¯scalar
            factor_values = merged_df.loc[valid_std_mask, 'factor']
            mean_values = merged_df.loc[valid_std_mask, 'final_mean']
            std_values = merged_df.loc[valid_std_mask, 'final_std']

            # è®¡ç®—æ ‡å‡†åŒ–å€¼
            standardized_values = (factor_values - mean_values) / std_values
            standardized_factor.loc[valid_std_mask] = standardized_values

        # å¯¹äºæ ‡å‡†å·®ä¸º0çš„è‚¡ç¥¨ï¼Œè®¾ä¸º0
        if zero_std_mask.any():
            standardized_factor.loc[zero_std_mask] = 0

        return standardized_factor.reindex(daily_factor_series.index)

        # =========================================================================
        # ã€æ ¸å¿ƒå‡çº§ã€‘é‡æ„åçš„ standardiize_robust å‡½æ•°
        # =========================================================================

    def _standardize_robust(self, factor_data: pd.DataFrame,
                           pit_industry_map: PointInTimeIndustryMap = None) -> pd.DataFrame:
        """
        å› å­æ ‡å‡†åŒ–å‡½æ•°ã€‚
        æ”¯æŒå…¨å¸‚åœºæˆ–åˆ†è¡Œä¸šï¼ˆå¸¦å‘ä¸Šå›æº¯åŠŸèƒ½ï¼‰çš„Z-Scoreå’Œæ’åºæ ‡å‡†åŒ–ã€‚
        """
        config = self.preprocessing_config.get('standardization', {})
        method = config.get('method', 'zscore')
        industry_config = config.get('by_industry')

        # --- è·¯å¾„ä¸€ï¼šå…¨å¸‚åœºæ ‡å‡†åŒ– ---
        if pit_industry_map is None or industry_config is None:
            print("  æ‰§è¡Œå…¨å¸‚åœºæ ‡å‡†åŒ–...")
            if method == 'zscore':
                return factor_data.apply(self._zscore_series, axis=1)
            elif method == 'rank':
                return factor_data.apply(self._rank_series, axis=1)
            return factor_data

        # --- è·¯å¾„äºŒï¼šåˆ†è¡Œä¸šæ ‡å‡†åŒ– ---
        else:
            logger.info(
                f"  æ‰§è¡Œåˆ†è¡Œä¸šæ ‡å‡†åŒ– (ä¸»è¡Œä¸š: {industry_config['primary_level']}, å›æº¯è‡³: {industry_config['fallback_level']})...")
            trading_dates_series = pd.Series(factor_data.index, index=factor_data.index)

            # Rankæ³•é€šå¸¸åœ¨å…¨å¸‚åœºè¿›è¡Œæ‰æœ‰æ„ä¹‰ï¼Œåˆ†è¡Œä¸šRankåä¸åŒè¡Œä¸šçš„åºæ— æ³•ç›´æ¥æ¯”è¾ƒã€‚
            # è¿™é‡Œæˆ‘ä»¬çº¦å®šï¼Œåˆ†è¡Œä¸šæ ‡å‡†åŒ–ä¸»è¦é’ˆå¯¹Z-Scoreã€‚
            if method == 'rank':
                print("    è­¦å‘Šï¼šåˆ†è¡Œä¸šRankæ ‡å‡†åŒ–é€»è¾‘å¤æ‚ä¸”ä¸å¸¸ç”¨ï¼Œå°†æ‰§è¡Œå…¨å¸‚åœºRankæ ‡å‡†åŒ–ã€‚")
                return factor_data.apply(self._rank_series, axis=1)

            processed_data = {}
            for date in factor_data.index:
                daily_factor_series = factor_data.loc[date].dropna()
                if daily_factor_series.empty:
                    processed_data[date] = pd.Series(dtype=float)
                    log_warning(f"æ ‡å‡†åŒ–è¿‡ç¨‹ä¸­ï¼Œå‘ç°å½“å¤©{date}æ‰€æœ‰è‚¡ç¥¨å› å­å€¼éƒ½ä¸ºç©º( å¦‚æœæ˜¯ å¾®è§‚ç»“æ„å› å­&è¿ç»­&10æ¡ä»¥å†… é‚£:æ­£å¸¸ å› ä¸ºå¾®è§‚ç»“æ„å› å­ æ ¹æ®æ—¶é—´åºåˆ—æ®‹å·®åŒ–(æ»‘åŠ¨å–çš„å‡å€¼)")
                    continue

                # åœ¨å¾ªç¯å†…éƒ¨ï¼Œä¸ºæ¯ä¸€å¤©è·å–æ­£ç¡®çš„å†å²åœ°å›¾
                #  è·å– T-1 çš„æ—¥æœŸ
                prev_trading_date = trading_dates_series.shift(1).loc[date]
                # å¤„ç†å›æµ‹ç¬¬ä¸€å¤©çš„è¾¹ç•Œæƒ…å†µ
                if pd.isna(prev_trading_date):
                    # log_warning(f"æ­£å¸¸ç°è±¡ï¼šæ—¥æœŸ {date} æ˜¯å›æµ‹é¦–æ—¥ï¼Œæ²¡æœ‰å‰ä¸€å¤©çš„è¡Œä¸šæ•°æ®ï¼Œè·³è¿‡åˆ†è¡Œä¸šå¤„ç†ã€‚")
                    processed_data[date] = daily_factor_series  # å½“å¤©ä¸åšå¤„ç†æˆ–æ‰§è¡Œå…¨å¸‚åœºå¤„ç†
                    continue

                # ä½¿ç”¨ T-1 çš„æ—¥æœŸæŸ¥è¯¢è¡Œä¸šåœ°å›¾
                daily_industry_map = pit_industry_map.get_map_for_date(prev_trading_date)
                processed_data[date] = self._standardize_cross_section_fallback(
                    daily_factor_series=daily_factor_series,
                    daily_industry_map=daily_industry_map,
                    config=industry_config
                )

            result_df = pd.DataFrame.from_dict(processed_data, orient='index')
            return result_df.reindex(index=factor_data.index, columns=factor_data.columns)

    def _print_processing_stats(self,
                                original_factor: pd.DataFrame,
                                processed_factor: pd.DataFrame
                                ):
        """æ‰“å°å¤„ç†ç»Ÿè®¡ä¿¡æ¯"""
        logger.info("å› å­é¢„å¤„ç†ç»Ÿè®¡:")

        # åŸå§‹å› å­ç»Ÿè®¡
        orig_valid = original_factor.notna().sum().sum()
        orig_total = original_factor.shape[0] * original_factor.shape[1]

        # å¤„ç†åå› å­ç»Ÿè®¡
        proc_valid = processed_factor.notna().sum().sum()

        # åˆ†å¸ƒç»Ÿè®¡
        all_values = processed_factor.values.flatten()
        all_values = all_values[~np.isnan(all_values)]

        if len(all_values) > 0:
            logger.info(
                f"  å¤„ç†ååˆ†å¸ƒ: å‡å€¼={all_values.mean():.3f}, æ ‡å‡†å·®={all_values.std():.3f} ï¼ˆzæ ‡å‡†åŒ–ï¼Œå‡å€¼ä¸€å®šæ˜¯0ï¼‰")
            logger.info(f"  åˆ†ä½æ•°: 1%={np.percentile(all_values, 1):.3f}, "
                        f"99%={np.percentile(all_values, 99):.3f}")



    def get_regression_need_neutral_factor_list(self, style_category,target_factor_name):
        """
           ã€V2ä¸“ä¸šç‰ˆã€‘æ ¹æ®å› å­é—¨æ´¾å’Œç›®æ ‡å› å­åç§°ï¼ŒåŠ¨æ€è·å–éœ€è¦ç”¨äºä¸­æ€§åŒ–çš„å› å­åˆ—è¡¨ã€‚

           æ­¤ç‰ˆæœ¬ä¿®å¤äº†æ—§ç‰ˆæœ¬çš„æ‰€æœ‰é—®é¢˜ï¼š
           1. é‡‡ç”¨é…ç½®å­—å…¸ï¼Œæ˜“äºæ‰©å±•ã€‚
           2. ç§»é™¤äº†æ‰€æœ‰ç¡¬ç¼–ç çš„ç‰¹ä¾‹ï¼Œé‡‡ç”¨é€šç”¨é€»è¾‘ã€‚
           3. ä½¿ç”¨å¥å£®çš„æ–¹å¼ç§»é™¤å…ƒç´ ï¼Œé¿å…ç¨‹åºå´©æºƒã€‚
           """
        # 1. æ ¹æ®å› å­é—¨æ´¾ï¼Œä»é…ç½®ä¸­è·å–åŸºç¡€çš„ä¸­æ€§åŒ–åˆ—è¡¨
        base_neutralization_list = FACTOR_STYLE_RISK_MODEL.get(style_category, FACTOR_STYLE_RISK_MODEL['default'])
        #
        # logger.info(
        #     f"å› å­ '{target_factor_name}' (styleåˆ—åˆ«: {style_category}) çš„åˆå§‹ä¸­æ€§åŒ–åˆ—è¡¨ä¸º: {base_neutralization_list}")

        # 2. ã€æ ¸å¿ƒé€»è¾‘ã€‘: åŠ¨æ€æ’é™¤ - é˜²æ­¢å› å­å¯¹è‡ªå·±è¿›è¡Œä¸­æ€§åŒ–
        # ä½¿ç”¨åˆ—è¡¨æ¨å¯¼å¼ï¼Œè¿™æ˜¯ä¸€ç§æ›´Pythonicã€æ›´å¥å£®çš„æ–¹å¼
        final_list = []
        for risk_factor in base_neutralization_list:
            # æ£€æŸ¥å¸‚å€¼
            if risk_factor == 'market_cap' and FactorClassifier.is_size_factor(target_factor_name):
                logger.info(f"  - ç›®æ ‡æ˜¯å¸‚å€¼å› å­ï¼Œå·²ä»ä¸­æ€§åŒ–åˆ—è¡¨ä¸­ç§»é™¤ 'market_cap'")
                continue  # è·³è¿‡ï¼Œä¸åŠ å…¥final_list

            # æ£€æŸ¥è¡Œä¸š
            if risk_factor == 'industry' and FactorClassifier.is_industry_factor(target_factor_name):
                logger.info(f"  - ç›®æ ‡æ˜¯è¡Œä¸šå› å­ï¼Œå·²ä»ä¸­æ€§åŒ–åˆ—è¡¨ä¸­ç§»é™¤ 'industry'")
                continue

            # æ£€æŸ¥Beta
            if risk_factor == 'pct_chg_beta' and FactorClassifier.is_beta_factor(target_factor_name):
                logger.info(f"  - ç›®æ ‡æ˜¯Betaå› å­ï¼Œå·²ä»ä¸­æ€§åŒ–åˆ—è¡¨ä¸­ç§»é™¤ 'pct_chg_beta'")
                continue

            final_list.append(risk_factor)

        # #ä¸´æ—¶çš„ è®°å¾—åˆ é™¤
        # if target_factor_name in ['bm_ratio', 'ep_ratio', 'sp_ratio','beta']:
        #     if 'market_cap' in final_list:
        #         final_list.remove('market_cap')
        logger.info(f"æœ€ç»ˆç”¨äºå›å½’çš„ä¸­æ€§åŒ–ç›®æ ‡å› å­ä¸º: {final_list}\n")
        return final_list

    def neutral_gression_diagnostics(self,model,date,y_clean,X_clean,residuals):
        # ğŸš¨ è¯¦ç»†çš„å›å½’è¯Šæ–­
        r_squared = model.rsquared

        # ğŸ” ç‰¹åˆ«æ£€æŸ¥æ³¢åŠ¨ç‡å› å­çš„å›å½’æƒ…å†µ
        logger.info(f"  ğŸ“Š æ³¢åŠ¨ç‡å› å­å›å½’è¯¦æƒ… - æ—¥æœŸ {date.date()}:")
        logger.info(f"    RÂ² = {r_squared:.4f}")
        logger.info(f"    æ ·æœ¬æ•° = {len(y_clean)}")
        logger.info(f"    è‡ªå˜é‡æ•° = {X_clean.shape[1]}")
        logger.info(f"    å› å­å€¼ç»Ÿè®¡: å‡å€¼={y_clean.mean():.6f}, æ ‡å‡†å·®={y_clean.std():.6f}")
        logger.info(f"    æ®‹å·®ç»Ÿè®¡: å‡å€¼={residuals.mean():.6f}, æ ‡å‡†å·®={residuals.std():.6f}")

        # æ£€æŸ¥å›å½’ç³»æ•°
        coefficients = model.params
        logger.info(f"    å›å½’ç³»æ•°: {dict(zip(X_clean.columns, coefficients))}")

        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨å¼‚å¸¸å¤§çš„ç³»æ•°
        max_coef = coefficients.abs().max()
        if max_coef > 10:
            logger.warning(f"    ğŸš¨ å‘ç°å¼‚å¸¸å¤§çš„å›å½’ç³»æ•°: {max_coef:.4f}")

        # æ£€æŸ¥æ®‹å·®ä¸åŸå› å­å€¼çš„å…³ç³»
        residual_factor_corr = np.corrcoef(y_clean, residuals)[0, 1]
        logger.info(f"    æ®‹å·®ä¸åŸå› å­å€¼çš„ç›¸å…³æ€§: {residual_factor_corr:.6f}")

        # å¦‚æœRÂ²è¿‡é«˜ï¼Œå‘å‡ºè­¦å‘Š
        if r_squared > 0.8:
            logger.warning(f"    âš ï¸ RÂ²={r_squared:.4f} è¿‡é«˜ï¼Œå¯èƒ½å­˜åœ¨è¿‡åº¦æ‹Ÿåˆï¼")
        pass

    def show_neutral_result(self,factor_data,processed_factor):
        logger.info("ğŸ”¬ æ³¢åŠ¨ç‡å› å­æ·±åº¦è¯Šæ–­ - ä¸­æ€§åŒ–å‰åå¯¹æ¯”:")

        # åŸå§‹å› å­ç»Ÿè®¡
        orig_flat = factor_data.stack().dropna()
        proc_flat = processed_factor.stack().dropna()

        logger.info(f"  åŸå§‹å› å­: æ ·æœ¬æ•°={len(orig_flat)}, å‡å€¼={orig_flat.mean():.6f}, æ ‡å‡†å·®={orig_flat.std():.6f}")
        logger.info(f"  åŸå§‹å› å­: æœ€å°å€¼={orig_flat.min():.6f}, æœ€å¤§å€¼={orig_flat.max():.6f}")
        logger.info(f"  åŸå§‹å› å­: 25%åˆ†ä½={orig_flat.quantile(0.25):.6f}, 75%åˆ†ä½={orig_flat.quantile(0.75):.6f}")

        logger.info(f"  ä¸­æ€§åŒ–å: æ ·æœ¬æ•°={len(proc_flat)}, å‡å€¼={proc_flat.mean():.6f}, æ ‡å‡†å·®={proc_flat.std():.6f}")
        logger.info(f"  ä¸­æ€§åŒ–å: æœ€å°å€¼={proc_flat.min():.6f}, æœ€å¤§å€¼={proc_flat.max():.6f}")
        logger.info(f"  ä¸­æ€§åŒ–å: 25%åˆ†ä½={proc_flat.quantile(0.25):.6f}, 75%åˆ†ä½={proc_flat.quantile(0.75):.6f}")

        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨å¤§é‡ç›¸åŒå€¼
        unique_orig = len(orig_flat.unique())
        unique_proc = len(proc_flat.unique())
        logger.info(f"  å”¯ä¸€å€¼æ•°é‡: åŸå§‹={unique_orig}, ä¸­æ€§åŒ–å={unique_proc}")

        # æ£€æŸ¥æœ€å¸¸è§çš„å€¼
        most_common_orig = orig_flat.value_counts().head(3)
        most_common_proc = proc_flat.value_counts().head(3)
        logger.info(f"  åŸå§‹å› å­æœ€å¸¸è§å€¼: {most_common_orig.to_dict()}")
        logger.info(f"  ä¸­æ€§åŒ–åæœ€å¸¸è§å€¼: {most_common_proc.to_dict()}")


# æ¨¡æ‹Ÿä¸€ä¸ªæ›´çœŸå®çš„ã€åŒ…å«å†å²å˜æ›´çš„è¡Œä¸šéš¶å±å…³ç³»æ•°æ®
def mock_full_historical_industry_data():
    """
    æ¨¡æ‹Ÿ index_member_all çš„å…¨é‡å†å²è¿”å›
    - S3 åœ¨ 2023-02-01 ä» L2_A1 å˜æ›´åˆ° L2_A2
    - S6 æ—©æœŸå­˜åœ¨ï¼Œä½†åœ¨ 2023-01-15 è¢«å‰”é™¤
    """
    data = [
        # S1, S2, S4, S5 ä¿æŒä¸å˜
        ['L1_A', 'L2_A1', 'S1', '20200101', None],
        ['L1_A', 'L2_A1', 'S2', '20200101', None],
        ['L1_B', 'L2_B1', 'S4', '20200101', None],
        ['L1_B', 'L2_B1', 'S5', '20200101', None],
        # S3 çš„å˜æ›´å†å²
        ['L1_A', 'L2_A1', 'S3', '20200101', '20230131'], # æ—§çš„éš¶å±å…³ç³»ï¼Œåœ¨31æ—¥ç»“æŸ
        ['L1_A', 'L2_A2', 'S3', '20230201', None],       # æ–°çš„éš¶å±å…³ç³»ï¼Œä»2æœˆ1æ—¥å¼€å§‹
        # S6 çš„å†å²
        ['L1_C', 'L2_C1', 'S6', '20200101', '20230115'],
    ]
    columns = ['l1_code', 'l2_code', 'ts_code', 'in_date', 'out_date']
    df = pd.DataFrame(data, columns=columns)
    df['in_date'] = pd.to_datetime(df['in_date'])
    # out_date ä¸º None çš„è¡¨ç¤ºè‡³ä»Šæœ‰æ•ˆï¼Œä¸ºäº†ä¾¿äºæ¯”è¾ƒï¼Œæˆ‘ä»¬ç”¨ä¸€ä¸ªæœªæ¥çš„æ—¥æœŸä»£æ›¿
    df['out_date'] = pd.to_datetime(df['out_date']).fillna(pd.Timestamp(permanent__day))
    return df




if __name__ == '__main__':
    # 1. è·å–å…¨é‡å†å²è¡Œä¸šæ•°æ®
    raw_industry_df = mock_full_historical_industry_data()

    # 2. ä¸€æ¬¡æ€§æ„å»ºPITæŸ¥è¯¢å¼•æ“
    pit_map_engine = PointInTimeIndustryMap(raw_industry_df)

    # 3. å‡†å¤‡å› å­æ•°æ®ï¼Œæ—¥æœŸè·¨è¶ŠS3çš„è¡Œä¸šå˜æ›´æ—¥
    factor_data_df = pd.DataFrame({
        'S1': [0.01, 0.02, 0.03],
        'S2': [0.03, 0.04, 0.05],
        'S3': [5.00, 0.01, 6.00],  # S3åœ¨ 2023-01-31 å’Œ 2023-02-01 çš„æç«¯å€¼
        'S4': [-4.0, 0.05, 0.06],
        'S5': [0.06, 0.07, 0.08]
    }, index=pd.to_datetime(['2023-01-31', '2023-02-01', '2023-02-02']))

    # 4. å‡†å¤‡é…ç½®å’ŒQuantDeveloperå®ä¾‹
    app_config = {
        'preprocessing': {'winsorization': {
            'method': 'mad', 'mad_threshold': 3.0,
            'by_industry': {'primary_level': 'l2_code', 'fallback_level': 'l1_code', 'min_samples': 2}
        }}}
    developer = FactorProcessor(config=app_config)

    # 5. æ‰§è¡Œå»æå€¼
    winsorized_df = developer.winsorize_robust(factor_data = factor_data_df, pit_industry_map=pit_map_engine)

    print("\n--- åŸå§‹å› å­æ•°æ® ---\n", factor_data_df)
    print("\n--- å»æå€¼åå› å­æ•°æ® ---\n", winsorized_df)

    # 6. éªŒè¯ S3 åœ¨ä¸åŒæ—¥æœŸçš„å¤„ç†é€»è¾‘
    print("\n--- éªŒè¯S3çš„è¡Œä¸šå½’å±å’Œå¤„ç†é€»è¾‘ ---")
    s3_val_before = winsorized_df.loc['2023-01-31', 'S3']
    s3_val_after = winsorized_df.loc['2023-02-01', 'S3']

    # åœ¨2023-01-31ï¼ŒS3å±äºL2_A1ï¼Œè¯¥ç»„æœ‰S1,S2,S3ä¸‰åªè‚¡ç¥¨ï¼Œæ ·æœ¬è¶³å¤Ÿï¼Œä½¿ç”¨ç»„å†…æ•°æ®
    print(f"2023-01-31, S3(5.0) å±äº L2_A1, ç»„å‘˜[S1,S2,S3], å› å­[0.01,0.03,5.0], å¤„ç†åå€¼ä¸º: {s3_val_before:.4f}")

    # åœ¨2023-02-01ï¼ŒS3å˜æ›´åˆ°L2_A2ï¼Œè¯¥ç»„åªæœ‰å®ƒè‡ªå·±ï¼Œæ ·æœ¬ä¸è¶³ï¼Œå›æº¯åˆ°L1_A
    # L1_A å½“å¤©æœ‰ S1,S2,S3ï¼Œå› å­å€¼ä¸º [0.02, 0.04, 0.01]ï¼Œç”¨è¿™ç»„çš„ç»Ÿè®¡é‡æ¥å¤„ç†S3
    print(
        f"2023-02-01, S3(0.01) å±äº L2_A2(å°æ ·æœ¬), å›æº¯è‡³ L1_A, ç»„å‘˜[S1,S2,S3], å› å­[0.02,0.04,0.01], å¤„ç†åå€¼ä¸º: {s3_val_after:.4f}")
